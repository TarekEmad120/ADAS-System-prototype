{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils  # If you are unable to install this library, ask the TA; we only need this in extract_hsv_histogram.\n",
    "import cv2  # cv2 \n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the image \n",
    "\n",
    "1- aplly gaussian blur to remove noise\n",
    "\n",
    "\n",
    "2- apply median blur to remove salt and pepper noise\n",
    "\n",
    "\n",
    "3- resize image\n",
    "\n",
    "\n",
    "4- mormalize pixels values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_fromPath(image_path, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Preprocess the image by resizing, grayscaling, applying noise reduction, and normalizing.\n",
    "    Args:\n",
    "        image_path: Path to the image.\n",
    "        target_size: Target size for resizing.\n",
    "    Returns:\n",
    "        Preprocessed image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image not found at {image_path}\")\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian Blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Apply a Median filter for further noise reduction (optional)\n",
    "    filtered = cv2.medianBlur(blurred, 3)\n",
    "    \n",
    "    # Resize to the target size\n",
    "    resized = cv2.resize(filtered, target_size)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    normalized = resized / 255.0\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def preprocess_image(image, target_size):\n",
    "    \"\"\"\n",
    "    Preprocess the image by resizing, grayscaling, applying noise reduction, and normalizing.\n",
    "    Args:\n",
    "        image: Input image.\n",
    "        target_size: Target size for resizing.\n",
    "    Returns:\n",
    "        Preprocessed image.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian Blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Apply a Median filter for further noise reduction (optional)\n",
    "    filtered = cv2.medianBlur(blurred, 3)\n",
    "    \n",
    "    # Resize to the target size\n",
    "    resized = cv2.resize(filtered, target_size)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    normalized = resized / 255.0\n",
    "    \n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract HOG features \n",
    "\n",
    "\n",
    "def extract_hog_features(...): This function is defined to compute Histogram of Oriented Gradients (HOG) features from a given grayscale image. HOG features describe the distribution of gradient orientations in an image, useful for object detection and recognition.\n",
    "\n",
    "\n",
    "image: Input parameter for the preprocessed image, expected to be in grayscale format.\n",
    "\n",
    "\n",
    "orientations: Number of orientation bins used to compute the gradient histogram. Default is 9, meaning the gradient direction is divided into 9 bins (e.g., 0°–20°, 20°–40°, etc.).\n",
    "\n",
    "\n",
    "pixels_per_cell: Defines the size of each cell in pixels. The gradient histogram is computed for each cell. Default size is 8x8 pixels.\n",
    "\n",
    "\n",
    "cells_per_block: Specifies how many cells are grouped into a block for normalization. Default is a 2x2 block of cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    \"\"\"\n",
    "    Extract HOG features from the image.\n",
    "    Args:\n",
    "        image: Preprocessed image (grayscale).\n",
    "        orientations: Number of orientation bins.\n",
    "        pixels_per_cell: Size of the cell in pixels.\n",
    "        cells_per_block: Number of cells in each block.\n",
    "    Returns:\n",
    "        HOG feature vector.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    hog_features = hog(image, orientations=orientations,\n",
    "                       pixels_per_cell=pixels_per_cell,\n",
    "                       cells_per_block=cells_per_block,\n",
    "                       block_norm='L2-Hys', visualize=False)\n",
    "    '''\n",
    "hog(image, ...): This function, imported from skimage.feature, computes the HOG features for the input image based on the specified parameters.\n",
    "\n",
    "orientations=orientations: Specifies the number of bins for the gradient histogram.\n",
    "\n",
    "pixels_per_cell=pixels_per_cell: Defines the dimensions of the cells for which the gradient histogram is computed.\n",
    "\n",
    "cells_per_block=cells_per_block: Specifies the size of the block used for local contrast normalization.\n",
    "\n",
    "block_norm='L2-Hys': Indicates the normalization method to use for the blocks. L2-Hys is a widely used normalization technique that improves robustness to lighting and contrast changes.\n",
    "\n",
    "visualize=False: Disables the visualization of the HOG image, which is useful for debugging but not required for feature extraction.\n",
    "'''\n",
    "    return hog_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build list of features and labels for training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def load_dataset(dataset_path, target_size=(64, 64)):\n",
    "#     \"\"\"\n",
    "#     Load images, preprocess, and extract HOG features.\n",
    "#     Args:\n",
    "#         dataset_path: Root directory of the dataset.\n",
    "#         target_size: Target size for resizing.\n",
    "#     Returns:\n",
    "#         Features, labels, and class mapping.\n",
    "#     \"\"\"\n",
    "#     features = []\n",
    "#     labels = []\n",
    "#     class_mapping = {}\n",
    "\n",
    "#     # Generate class mapping dynamically\n",
    "#     class_names = sorted(os.listdir(dataset_path))  # Sort for consistent label assignment\n",
    "#     for idx, class_name in enumerate(class_names):\n",
    "#         class_mapping[class_name] = idx  # Assign an index to each class\n",
    "\n",
    "#     for class_name, label in class_mapping.items():\n",
    "#         class_path = os.path.join(dataset_path, class_name)\n",
    "#         for file_name in os.listdir(class_path):\n",
    "#             image_path = os.path.join(class_path, file_name)\n",
    "#             # Preprocess the image\n",
    "#             preprocessed_image = preprocess_image_fromPath(image_path, target_size)\n",
    "#             # Extract HOG features\n",
    "#             feature_vector = extract_hog_features(preprocessed_image)\n",
    "#             features.append(feature_vector)\n",
    "#             labels.append(label)\n",
    "\n",
    "#     return np.array(features), np.array(labels), class_mapping\n",
    "\n",
    "# # Define dataset path\n",
    "# dataset_path = r\"D:\\CUFE\\GP\\Sleep_detection\\archive\\mrleyedataset\"\n",
    "\n",
    "# # Load dataset\n",
    "# X, y, class_mapping = load_dataset(dataset_path)\n",
    "\n",
    "# # Split into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Print the class mapping\n",
    "# print(\"Class Mapping:\", class_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# pickle_out = open(\"class_mapping.pickle\",\"wb\")\n",
    "# pickle.dump(class_mapping, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# pickle_out = open(\"X_train.pickle\",\"wb\")\n",
    "# pickle.dump(X_train, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# pickle_out = open(\"X_test.pickle\",\"wb\")\n",
    "# pickle.dump(X_test, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# pickle_out = open(\"y_train.pickle\",\"wb\")\n",
    "# pickle.dump(y_train, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# pickle_out = open(\"y_test.pickle\",\"wb\")\n",
    "# pickle.dump(y_test, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "pickle_in = open(\"class_mapping.pickle\",\"rb\")\n",
    "class_mapping = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(\"X_train.pickle\",\"rb\")\n",
    "# X_train = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(\"X_test.pickle\",\"rb\")\n",
    "# X_test = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(\"y_train.pickle\",\"rb\")\n",
    "# y_train = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(\"y_test.pickle\",\"rb\")\n",
    "# y_test = pickle.load(pickle_in)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Train the model\n",
    "\n",
    "2- Test the model\n",
    "\n",
    "3- Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Train the SVM\n",
    "# svm_model = SVC(kernel='linear', random_state=42)\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the SVM\n",
    "# y_pred = svm_model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"SVM Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'SVM_model.sav'\n",
    "# pickle.dump(svm_model, open(filename, 'wb'))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, y_test)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_fromPath(image_path, model, class_mapping, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Predict the class of a new image.\n",
    "    Args:\n",
    "        image_path: Path to the image.\n",
    "        model: Trained SVM model.\n",
    "        class_mapping: Mapping of class names to labels.\n",
    "        target_size: Target size for resizing.\n",
    "    Returns:\n",
    "        Predicted class name.\n",
    "    \"\"\"\n",
    "    # Preprocess and extract features\n",
    "    preprocessed_image = preprocess_image_fromPath(image_path, target_size)\n",
    "    feature_vector = extract_hog_features(preprocessed_image).reshape(1, -1)\n",
    "    # Predict\n",
    "    prediction = model.predict(feature_vector)[0]\n",
    "    # Map label back to class name\n",
    "    for class_name, label in class_mapping.items():\n",
    "        if label == prediction:\n",
    "            return class_name\n",
    "\n",
    "def predict_image(image, model, class_mapping, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Predict the class of a new image.\n",
    "    Args:\n",
    "        image: Input image.\n",
    "        model: Trained model.\n",
    "        class_mapping: Dictionary mapping class indices to class names.\n",
    "        target_size: Target size for resizing.\n",
    "    Returns:\n",
    "        Predicted class name.\n",
    "    \"\"\"\n",
    "    # Preprocess and extract features\n",
    "    preprocessed_image = preprocess_image(image, target_size)\n",
    "    feature_vector = extract_hog_features(preprocessed_image).reshape(1, -1)\n",
    "    # Predict\n",
    "    prediction = model.predict(feature_vector)[0]\n",
    "    # Map label back to class name\n",
    "    for class_name, label in class_mapping.items():\n",
    "        if label == prediction:\n",
    "            return class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Open-Eyes\n"
     ]
    }
   ],
   "source": [
    "# prdicting the image\n",
    "image_path = r\"D:\\CUFE\\GP\\Sleep_detection\\archive\\open_eye.jpg\"\n",
    "predicted_class = predict_image_fromPath(image_path, loaded_model, class_mapping)\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23596\\809540654.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"archive/man_open.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# plt.imshow(img)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mface_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhaarcascades\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'haarcascade_frontalface_default.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0meye_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhaarcascades\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'haarcascade_eye.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(\"archive/man_open.png\")\n",
    "# plt.imshow(img)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "i = 0\n",
    "for (x, y, w, h) in eyes:\n",
    "    i += 1\n",
    "    print(f\"Eye {i}: ({x}, {y}, {w}, {h})\")\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)  \n",
    "    \n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23596\\2930372155.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#CROP THE EYES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0meye_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhaarcascades\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'haarcascade_eye.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0meyes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meye_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "#CROP THE EYES\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "# eyes_roi = []\n",
    "for x, y, w, h in eyes:\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    eyess = eye_cascade.detectMultiScale(roi_gray)\n",
    "    if len(eyess) == 0:\n",
    "        print(\"eyes are not detected\")\n",
    "    else:\n",
    "        for (ex, ey, ew, eh) in eyess:\n",
    "            eyes_roi = roi_color[ey: ey+eh, ex: ex+ew]\n",
    "\n",
    "plt.imshow(cv2.cvtColor(eyes_roi, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eyes_roi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23596\\758556163.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meyes_roi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'eyes_roi' is not defined"
     ]
    }
   ],
   "source": [
    "eyes_roi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the image\n",
    "cv2.imwrite(\"archive/test.jpg\", eyes_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Open-Eyes\n"
     ]
    }
   ],
   "source": [
    "test = predict_image_fromPath(\"archive/test.jpg\", loaded_model, class_mapping)\n",
    "print(f\"Predicted Class: {test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 26244 features, but SVC is expecting 1764 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23060\\4224360723.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0meyes_roi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroi_color\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mey\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0meh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mew\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;31m#USING THE MODEL TO PREDICT THE EYE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meyes_roi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Predicted Class: {test}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23060\\2906226348.py\u001b[0m in \u001b[0;36mpredict_image\u001b[1;34m(image, model, class_mapping, target_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mfeature_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_hog_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessed_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;31m# Map label back to class name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclass_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \"\"\"\n\u001b[1;32m--> 414\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m                 \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                 \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             )\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m             raise ValueError(\n\u001b[1;32m--> 401\u001b[1;33m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m                 \u001b[1;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 26244 features, but SVC is expecting 1764 features as input."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import time\n",
    "# import winsound  # For beep sound (Windows)\n",
    "\n",
    "\n",
    "# # Load Haar cascade classifiers\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "# eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# if not cap.isOpened():\n",
    "#     cap = cv2.VideoCapture(1)\n",
    "# if not cap.isOpened():\n",
    "#     raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# eyes_closed_time = None  # Track when eyes were first closed\n",
    "# ALERT_THRESHOLD = 3  # Seconds before triggering an alert\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "#     eyes_detected = False  # Default: No eyes detected\n",
    "\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "#         roi_gray = gray[y:y + h, x:x + w]\n",
    "#         roi_color = frame[y:y + h, x:x + w]\n",
    "\n",
    "#         eyes = eye_cascade.detectMultiScale(roi_gray, 1.1, 4)\n",
    "\n",
    "#         if len(eyes) > 0:\n",
    "#             eyes_detected = True  # Eyes detected, reset timer\n",
    "#             for (ex, ey, ew, eh) in eyes:\n",
    "#                 cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (255, 0,\n",
    "#                                                                             0), 2)\n",
    "#                 #use model to predict the eye\n",
    "#                 eyes_roi = roi_color[ey: ey+eh, ex: ex+ew]\n",
    "#                 #USING THE MODEL TO PREDICT THE EYE\n",
    "#                 test = predict_image(eyes_roi, loaded_model, class_mapping)\n",
    "#                 print(f\"Predicted Class: {test}\")\n",
    "    \n",
    "\n",
    "\n",
    "#     # Check if eyes are closed or not detected\n",
    "#     if not eyes_detected:\n",
    "#         if eyes_closed_time is None:  # Start timing when eyes close/disappear\n",
    "#             eyes_closed_time = time.time()\n",
    "#         elif time.time() - eyes_closed_time >= ALERT_THRESHOLD:\n",
    "#             print(\"ALERT! Eyes closed/not detected for 3 seconds!\")\n",
    "#             winsound.Beep(2500, 1000)  # Beep sound (2500 Hz, 1 sec)\n",
    "#             eyes_closed_time = None  # Reset timer after alert\n",
    "#     else:\n",
    "#         eyes_closed_time = None  # Reset if eyes open\n",
    "\n",
    "#     cv2.imshow('Drowsiness Detection', frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tarek\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py:338: UserWarning: Trying to unpickle estimator SVC from version 1.5.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State changed: Open-Eyes (via Regular-Cascade, confidence: 1.00)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Ensure archive directory exists\n",
    "if not os.path.exists(\"archive\"):\n",
    "    os.makedirs(\"archive\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = pickle.load(open(\"SVM_model.sav\", 'rb'))\n",
    "class_mapping = pickle.load(open(\"class_mapping.pickle\", \"rb\"))\n",
    "\n",
    "# Eye Tracker with improved glasses handling\n",
    "class ImprovedEyeTracker:\n",
    "    def __init__(self):\n",
    "        # State tracking\n",
    "        self.state = \"Unknown\"\n",
    "        self.confidence = 0.0\n",
    "        self.closed_time_start = None\n",
    "        self.alert_active = False\n",
    "        self.alert_threshold = 3.0  # Seconds before alerting\n",
    "        \n",
    "        # Enhanced history management\n",
    "        self.history = []\n",
    "        self.history_size = 30  # Longer history for stability\n",
    "        self.state_stability_threshold = 0.65  # Higher threshold for state changes\n",
    "        \n",
    "        # Glasses-specific settings\n",
    "        self.has_glasses = False\n",
    "        self.glasses_detection_count = 0\n",
    "        self.glasses_threshold = 5  # Need this many consecutive detections to confirm glasses\n",
    "        \n",
    "        # Detection info\n",
    "        self.detection_method = None\n",
    "        self.last_successful_detection = time.time()\n",
    "        self.detection_timeout = 1.0  # Seconds before considering detection lost\n",
    "        \n",
    "        # Last known good regions\n",
    "        self.last_eye_roi = None\n",
    "        \n",
    "    def update(self, eye_state, detection_confidence, detection_method, has_glasses_frame=False):\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Update glasses detection\n",
    "        if has_glasses_frame:\n",
    "            self.glasses_detection_count += 1\n",
    "        else:\n",
    "            self.glasses_detection_count = max(0, self.glasses_detection_count - 1)\n",
    "        \n",
    "        # Determine if wearing glasses (needs multiple consistent detections)\n",
    "        self.has_glasses = self.glasses_detection_count >= self.glasses_threshold\n",
    "        \n",
    "        # Update detection method\n",
    "        self.detection_method = detection_method\n",
    "        \n",
    "        # Add to history for smoothing\n",
    "        self.history.append(eye_state)\n",
    "        if len(self.history) > self.history_size:\n",
    "            self.history.pop(0)\n",
    "        \n",
    "        # Count state occurrences\n",
    "        state_counts = {}\n",
    "        for state in self.history:\n",
    "            if state in state_counts:\n",
    "                state_counts[state] += 1\n",
    "            else:\n",
    "                state_counts[state] = 1\n",
    "        \n",
    "        # Get most common state with sufficient history\n",
    "        if len(self.history) >= 5:  # Need at least 5 detections to make a decision\n",
    "            most_common = max(state_counts.items(), key=lambda x: x[1])\n",
    "            smoothed_state = most_common[0]\n",
    "            self.confidence = most_common[1] / len(self.history)\n",
    "            \n",
    "            # Update state if confident enough\n",
    "            if self.confidence > self.state_stability_threshold:\n",
    "                if self.state != smoothed_state:\n",
    "                    print(f\"State changed: {smoothed_state} (via {detection_method}, confidence: {self.confidence:.2f})\")\n",
    "                    self.state = smoothed_state\n",
    "        \n",
    "        # Track closed eyes for drowsiness detection\n",
    "        if self.state == \"Close-Eyes\":\n",
    "            if self.closed_time_start is None:\n",
    "                self.closed_time_start = current_time\n",
    "            # Do nothing else - just keep the timer running\n",
    "        else:\n",
    "            self.closed_time_start = None\n",
    "            self.alert_active = False\n",
    "\n",
    "    def check_drowsiness(self):\n",
    "        if self.closed_time_start is not None:\n",
    "            elapsed = time.time() - self.closed_time_start\n",
    "            if elapsed >= self.alert_threshold and not self.alert_active:\n",
    "                self.alert_active = True\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_closed_duration(self):\n",
    "        if self.closed_time_start is None:\n",
    "            return 0\n",
    "        return time.time() - self.closed_time_start\n",
    "\n",
    "# Specialized preprocessing for different lighting conditions\n",
    "def enhance_for_glasses(image):\n",
    "    if image is None or image.size == 0:\n",
    "        return None\n",
    "        \n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image.copy()\n",
    "    \n",
    "    # Apply advanced preprocessing specifically for glasses\n",
    "    # 1. CLAHE for better contrast around eyes with glasses\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "    \n",
    "    # 2. Bilateral filter to preserve edges while reducing noise\n",
    "    enhanced = cv2.bilateralFilter(enhanced, 9, 25, 25)\n",
    "    \n",
    "    # 3. Gamma correction to better handle reflections on glasses\n",
    "    gamma = 1.2\n",
    "    lookUpTable = np.empty((1,256), np.uint8)\n",
    "    for i in range(256):\n",
    "        lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n",
    "    enhanced = cv2.LUT(enhanced, lookUpTable)\n",
    "    \n",
    "    return enhanced\n",
    "\n",
    "# Main application\n",
    "def run_eye_detection():\n",
    "    # Initialize cascades with optimized parameters for glasses\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "    eyeglasses_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye_tree_eyeglasses.xml')\n",
    "    \n",
    "    # Verify cascades loaded properly\n",
    "    if face_cascade.empty() or eye_cascade.empty() or eyeglasses_cascade.empty():\n",
    "        print(\"Error: Couldn't load cascade classifiers!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(1)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "    \n",
    "    # Create eye tracker\n",
    "    tracker = ImprovedEyeTracker()\n",
    "    \n",
    "    # Main loop\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Create a copy for display\n",
    "        display = frame.copy()\n",
    "        \n",
    "        # Basic grayscale conversion\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply CLAHE for better contrast globally\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        enhanced_gray = clahe.apply(gray)\n",
    "        \n",
    "        # State variables for this frame\n",
    "        eye_state = \"Unknown\"\n",
    "        detection_confidence = 0.0\n",
    "        detection_method = \"None\"\n",
    "        has_glasses_detected = False\n",
    "        \n",
    "        # Detect faces first\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            enhanced_gray, \n",
    "            scaleFactor=1.1, \n",
    "            minNeighbors=5, \n",
    "            minSize=(80, 80),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            # Process the largest face\n",
    "            x, y, w, h = max(faces, key=lambda face: face[2] * face[3])\n",
    "            \n",
    "            # Draw face rectangle\n",
    "            cv2.rectangle(display, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            # Extract face region with some margin\n",
    "            margin = int(h * 0.1)  # 10% margin\n",
    "            face_y1 = max(0, y-margin)\n",
    "            face_y2 = min(frame.shape[0], y+h+margin)\n",
    "            face_x1 = max(0, x-margin)\n",
    "            face_x2 = min(frame.shape[1], x+w+margin)\n",
    "            \n",
    "            face_gray = enhanced_gray[face_y1:face_y2, face_x1:face_x2]\n",
    "            face_color = frame[face_y1:face_y2, face_x1:face_x2]\n",
    "            \n",
    "            # Specially enhanced version for glasses detection\n",
    "            glasses_enhanced = enhance_for_glasses(face_gray)\n",
    "            \n",
    "            # First try glasses-specific cascade with relaxed parameters\n",
    "            eyes_with_glasses = eyeglasses_cascade.detectMultiScale(\n",
    "                glasses_enhanced, \n",
    "                scaleFactor=1.05,  # Smaller scale factor for better detection\n",
    "                minNeighbors=3,    # Fewer neighbors required for glasses\n",
    "                minSize=(25, 25),  # Smaller minimum size\n",
    "                flags=cv2.CASCADE_SCALE_IMAGE\n",
    "            )\n",
    "            \n",
    "            if len(eyes_with_glasses) > 0:\n",
    "                # Glasses detected\n",
    "                eyes = eyes_with_glasses\n",
    "                has_glasses_detected = True\n",
    "                detection_method = \"Glasses-Cascade\"\n",
    "                \n",
    "                # Draw all detected glasses eyes\n",
    "                for (ex, ey, ew, eh) in eyes:\n",
    "                    cv2.rectangle(face_color, (ex, ey), (ex+ew, ey+eh), (255, 0, 0), 2)\n",
    "                \n",
    "                # Use the most likely to be an eye (typically higher in the face)\n",
    "                # Sort by y-coordinate (smaller = higher in face)\n",
    "                eyes_sorted = sorted(eyes, key=lambda e: e[1])\n",
    "                \n",
    "                # Take the top eye if we found multiple\n",
    "                ex, ey, ew, eh = eyes_sorted[0] if len(eyes_sorted) > 0 else eyes[0]\n",
    "                \n",
    "            else:\n",
    "                # Try regular eye detector with optimized parameters\n",
    "                eyes = eye_cascade.detectMultiScale(\n",
    "                    face_gray, \n",
    "                    scaleFactor=1.05, \n",
    "                    minNeighbors=6,     # More neighbors for better accuracy\n",
    "                    minSize=(25, 25),\n",
    "                    flags=cv2.CASCADE_SCALE_IMAGE\n",
    "                )\n",
    "                \n",
    "                has_glasses_detected = False\n",
    "                detection_method = \"Regular-Cascade\"\n",
    "                \n",
    "                if len(eyes) > 0:\n",
    "                    # Draw detected eyes\n",
    "                    for (ex, ey, ew, eh) in eyes:\n",
    "                        cv2.rectangle(face_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 255), 2)\n",
    "                    \n",
    "                    # If multiple eyes detected, use the one closer to expected position\n",
    "                    if len(eyes) >= 2:\n",
    "                        # Sort by y-coordinate to get upper eyes\n",
    "                        eyes_sorted = sorted(eyes, key=lambda e: e[1])\n",
    "                        # Take the first eye\n",
    "                        ex, ey, ew, eh = eyes_sorted[0]\n",
    "                    else:\n",
    "                        ex, ey, ew, eh = eyes[0]\n",
    "            \n",
    "            # If eyes were detected with either method\n",
    "            if len(eyes) > 0:\n",
    "                # Extract the eye region\n",
    "                eye_roi = face_color[ey:ey+eh, ex:ex+ew]\n",
    "                \n",
    "                # Save the eye ROI for later use\n",
    "                if eye_roi.size > 0:\n",
    "                    tracker.last_eye_roi = eye_roi\n",
    "                    \n",
    "                    # Save and predict\n",
    "                    cv2.imwrite(\"archive/test.jpg\", eye_roi)\n",
    "                    try:\n",
    "                        eye_state = predict_image_fromPath(\"archive/test.jpg\", loaded_model, class_mapping)\n",
    "                        detection_confidence = 0.7\n",
    "                        \n",
    "                        # Draw prediction on frame\n",
    "                        label = f\"{eye_state} ({detection_confidence:.2f})\"\n",
    "                        cv2.putText(face_color, label, (ex, ey-5), \n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Prediction error: {e}\")\n",
    "                        eye_state = \"Error\"\n",
    "                        detection_confidence = 0.3\n",
    "            else:\n",
    "                # No eyes detected in this frame\n",
    "                eye_state = \"Eyes Not Detected\"\n",
    "                detection_confidence = 0.4\n",
    "                \n",
    "                # If we have a saved eye ROI, try using that\n",
    "                if tracker.last_eye_roi is not None:\n",
    "                    try:\n",
    "                        # Save the last known eye and classify\n",
    "                        cv2.imwrite(\"archive/last_eye.jpg\", tracker.last_eye_roi)\n",
    "                        fallback_state = predict_image_fromPath(\"archive/last_eye.jpg\", loaded_model, class_mapping)\n",
    "                        \n",
    "                        # Only use this if we're fairly sure eyes are still there but detection failed\n",
    "                        if time.time() - tracker.last_successful_detection < tracker.detection_timeout:\n",
    "                            eye_state = fallback_state\n",
    "                            detection_confidence = 0.5\n",
    "                            detection_method = \"Last-ROI-Fallback\"\n",
    "                    except:\n",
    "                        pass\n",
    "        else:\n",
    "            # No face detected\n",
    "            eye_state = \"No Face\"\n",
    "            detection_confidence = 0.2\n",
    "            \n",
    "        # Update tracker with detected eye state\n",
    "        tracker.update(eye_state, detection_confidence, detection_method, has_glasses_detected)\n",
    "        \n",
    "        # If we detected eyes successfully, update the timestamp\n",
    "        if eye_state not in [\"No Face\", \"Eyes Not Detected\", \"Error\", \"Unknown\"]:\n",
    "            tracker.last_successful_detection = time.time()\n",
    "        \n",
    "        # Check for drowsiness\n",
    "        if tracker.check_drowsiness():\n",
    "            # Alert for drowsiness\n",
    "            try:\n",
    "                import winsound\n",
    "                winsound.Beep(1000, 500)  # Beep at 1000 Hz for 500 ms\n",
    "            except:\n",
    "                print(\"ALERT: Drowsiness detected!\")\n",
    "        \n",
    "        # Create info panel\n",
    "        info_panel = np.zeros((200, frame.shape[1], 3), dtype=np.uint8)\n",
    "        \n",
    "        # Add information to the panel\n",
    "        cv2.putText(info_panel, f\"Eye State: {tracker.state}\", (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.putText(info_panel, f\"Detection: {tracker.detection_method}\", (10, 60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "        \n",
    "        cv2.putText(info_panel, f\"Confidence: {tracker.confidence:.2f}\", (10, 90), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "        \n",
    "        # Use more visible indication for glasses detection\n",
    "        glasses_color = (0, 255, 255) if tracker.has_glasses else (100, 100, 100)\n",
    "        cv2.putText(info_panel, f\"Glasses: {'YES' if tracker.has_glasses else 'NO'}\", (10, 120), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, glasses_color, 2)\n",
    "        \n",
    "        # Show drowsiness warning if necessary\n",
    "        if tracker.closed_time_start is not None:\n",
    "            elapsed = tracker.get_closed_duration()\n",
    "            cv2.putText(info_panel, f\"Eyes closed for: {elapsed:.1f}s\", (10, 150), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n",
    "            \n",
    "            # Flash warning if alert is active\n",
    "            if elapsed >= tracker.alert_threshold:\n",
    "                if int(time.time() * 2) % 2 == 0:  # Flashing effect\n",
    "                    cv2.putText(info_panel, \"DROWSINESS ALERT!\", (10, 190), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)\n",
    "        \n",
    "        # Combine display and info panel\n",
    "        combined = np.vstack((display, info_panel))\n",
    "        \n",
    "        # Show the result\n",
    "        cv2.imshow(\"Drowsiness Detection\", combined)\n",
    "        \n",
    "        # Exit on 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start the application\n",
    "run_eye_detection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

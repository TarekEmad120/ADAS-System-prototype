{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils  # If you are unable to install this library, ask the TA; we only need this in extract_hsv_histogram.\n",
    "import cv2  # cv2 \n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the image \n",
    "\n",
    "1- aplly gaussian blur to remove noise\n",
    "\n",
    "\n",
    "2- apply median blur to remove salt and pepper noise\n",
    "\n",
    "\n",
    "3- resize image\n",
    "\n",
    "\n",
    "4- mormalize pixels values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_fromPath(image_path, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Preprocess the image by resizing, grayscaling, applying noise reduction, and normalizing.\n",
    "    Args:\n",
    "        image_path: Path to the image.\n",
    "        target_size: Target size for resizing.\n",
    "    Returns:\n",
    "        Preprocessed image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image not found at {image_path}\")\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian Blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Apply a Median filter for further noise reduction (optional)\n",
    "    filtered = cv2.medianBlur(blurred, 3)\n",
    "    \n",
    "    # Resize to the target size\n",
    "    resized = cv2.resize(filtered, target_size)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    normalized = resized / 255.0\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def preprocess_image(image, target_size):\n",
    "    \"\"\"\n",
    "    Preprocess the image by resizing, grayscaling, applying noise reduction, and normalizing.\n",
    "    Args:\n",
    "        image: Input image.\n",
    "        target_size: Target size for resizing.\n",
    "    Returns:\n",
    "        Preprocessed image.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian Blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Apply a Median filter for further noise reduction (optional)\n",
    "    filtered = cv2.medianBlur(blurred, 3)\n",
    "    \n",
    "    # Resize to the target size\n",
    "    resized = cv2.resize(filtered, target_size)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    normalized = resized / 255.0\n",
    "    \n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract HOG features \n",
    "\n",
    "\n",
    "def extract_hog_features(...): This function is defined to compute Histogram of Oriented Gradients (HOG) features from a given grayscale image. HOG features describe the distribution of gradient orientations in an image, useful for object detection and recognition.\n",
    "\n",
    "\n",
    "image: Input parameter for the preprocessed image, expected to be in grayscale format.\n",
    "\n",
    "\n",
    "orientations: Number of orientation bins used to compute the gradient histogram. Default is 9, meaning the gradient direction is divided into 9 bins (e.g., 0°–20°, 20°–40°, etc.).\n",
    "\n",
    "\n",
    "pixels_per_cell: Defines the size of each cell in pixels. The gradient histogram is computed for each cell. Default size is 8x8 pixels.\n",
    "\n",
    "\n",
    "cells_per_block: Specifies how many cells are grouped into a block for normalization. Default is a 2x2 block of cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    \"\"\"\n",
    "    Extract HOG features from the image.\n",
    "    Args:\n",
    "        image: Preprocessed image (grayscale).\n",
    "        orientations: Number of orientation bins.\n",
    "        pixels_per_cell: Size of the cell in pixels.\n",
    "        cells_per_block: Number of cells in each block.\n",
    "    Returns:\n",
    "        HOG feature vector.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    hog_features = hog(image, orientations=orientations,\n",
    "                       pixels_per_cell=pixels_per_cell,\n",
    "                       cells_per_block=cells_per_block,\n",
    "                       block_norm='L2-Hys', visualize=False)\n",
    "    '''\n",
    "hog(image, ...): This function, imported from skimage.feature, computes the HOG features for the input image based on the specified parameters.\n",
    "\n",
    "orientations=orientations: Specifies the number of bins for the gradient histogram.\n",
    "\n",
    "pixels_per_cell=pixels_per_cell: Defines the dimensions of the cells for which the gradient histogram is computed.\n",
    "\n",
    "cells_per_block=cells_per_block: Specifies the size of the block used for local contrast normalization.\n",
    "\n",
    "block_norm='L2-Hys': Indicates the normalization method to use for the blocks. L2-Hys is a widely used normalization technique that improves robustness to lighting and contrast changes.\n",
    "\n",
    "visualize=False: Disables the visualization of the HOG image, which is useful for debugging but not required for feature extraction.\n",
    "'''\n",
    "    return hog_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build list of features and labels for training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def load_dataset(dataset_path, target_size=(64, 64)):\n",
    "#     \"\"\"\n",
    "#     Load images, preprocess, and extract HOG features.\n",
    "#     Args:\n",
    "#         dataset_path: Root directory of the dataset.\n",
    "#         target_size: Target size for resizing.\n",
    "#     Returns:\n",
    "#         Features, labels, and class mapping.\n",
    "#     \"\"\"\n",
    "#     features = []\n",
    "#     labels = []\n",
    "#     class_mapping = {}\n",
    "\n",
    "#     # Generate class mapping dynamically\n",
    "#     class_names = sorted(os.listdir(dataset_path))  # Sort for consistent label assignment\n",
    "#     for idx, class_name in enumerate(class_names):\n",
    "#         class_mapping[class_name] = idx  # Assign an index to each class\n",
    "\n",
    "#     for class_name, label in class_mapping.items():\n",
    "#         class_path = os.path.join(dataset_path, class_name)\n",
    "#         for file_name in os.listdir(class_path):\n",
    "#             image_path = os.path.join(class_path, file_name)\n",
    "#             # Preprocess the image\n",
    "#             preprocessed_image = preprocess_image_fromPath(image_path, target_size)\n",
    "#             # Extract HOG features\n",
    "#             feature_vector = extract_hog_features(preprocessed_image)\n",
    "#             features.append(feature_vector)\n",
    "#             labels.append(label)\n",
    "\n",
    "#     return np.array(features), np.array(labels), class_mapping\n",
    "\n",
    "# # Define dataset path\n",
    "# dataset_path = r\"D:\\CUFE\\GP\\Sleep_detection\\archive\\mrleyedataset\"\n",
    "\n",
    "# # Load dataset\n",
    "# X, y, class_mapping = load_dataset(dataset_path)\n",
    "\n",
    "# # Split into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Print the class mapping\n",
    "# print(\"Class Mapping:\", class_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# pickle_out = open(\"class_mapping.pickle\",\"wb\")\n",
    "# pickle.dump(class_mapping, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# pickle_out = open(\"X_train.pickle\",\"wb\")\n",
    "# pickle.dump(X_train, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# pickle_out = open(\"X_test.pickle\",\"wb\")\n",
    "# pickle.dump(X_test, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# pickle_out = open(\"y_train.pickle\",\"wb\")\n",
    "# pickle.dump(y_train, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# pickle_out = open(\"y_test.pickle\",\"wb\")\n",
    "# pickle.dump(y_test, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "pickle_in = open(\"class_mapping.pickle\",\"rb\")\n",
    "class_mapping = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(\"X_train.pickle\",\"rb\")\n",
    "# X_train = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(\"X_test.pickle\",\"rb\")\n",
    "# X_test = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(\"y_train.pickle\",\"rb\")\n",
    "# y_train = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(\"y_test.pickle\",\"rb\")\n",
    "# y_test = pickle.load(pickle_in)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Train the model\n",
    "\n",
    "2- Test the model\n",
    "\n",
    "3- Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Train the SVM\n",
    "# svm_model = SVC(kernel='linear', random_state=42)\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the SVM\n",
    "# y_pred = svm_model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"SVM Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'SVM_model.sav'\n",
    "# pickle.dump(svm_model, open(filename, 'wb'))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, y_test)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_fromPath(image_path, model, class_mapping, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Predict the class of a new image.\n",
    "    Args:\n",
    "        image_path: Path to the image.\n",
    "        model: Trained SVM model.\n",
    "        class_mapping: Mapping of class names to labels.\n",
    "        target_size: Target size for resizing.\n",
    "    Returns:\n",
    "        Predicted class name.\n",
    "    \"\"\"\n",
    "    # Preprocess and extract features\n",
    "    preprocessed_image = preprocess_image_fromPath(image_path, target_size)\n",
    "    feature_vector = extract_hog_features(preprocessed_image).reshape(1, -1)\n",
    "    # Predict\n",
    "    prediction = model.predict(feature_vector)[0]\n",
    "    # Map label back to class name\n",
    "    for class_name, label in class_mapping.items():\n",
    "        if label == prediction:\n",
    "            return class_name\n",
    "\n",
    "def predict_image(image, model, class_mapping, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Predict the class of a new image.\n",
    "    Args:\n",
    "        image: Input image.\n",
    "        model: Trained model.\n",
    "        class_mapping: Dictionary mapping class indices to class names.\n",
    "        target_size: Target size for resizing.\n",
    "    Returns:\n",
    "        Predicted class name.\n",
    "    \"\"\"\n",
    "    # Preprocess and extract features\n",
    "    preprocessed_image = preprocess_image(image, target_size)\n",
    "    feature_vector = extract_hog_features(preprocessed_image).reshape(1, -1)\n",
    "    # Predict\n",
    "    prediction = model.predict(feature_vector)[0]\n",
    "    # Map label back to class name\n",
    "    for class_name, label in class_mapping.items():\n",
    "        if label == prediction:\n",
    "            return class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Open-Eyes\n"
     ]
    }
   ],
   "source": [
    "# prdicting the image\n",
    "image_path = r\"D:\\CUFE\\GP\\Sleep_detection\\archive\\open_eye.jpg\"\n",
    "predicted_class = predict_image_fromPath(image_path, loaded_model, class_mapping)\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23596\\809540654.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"archive/man_open.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# plt.imshow(img)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mface_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhaarcascades\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'haarcascade_frontalface_default.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0meye_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhaarcascades\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'haarcascade_eye.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(\"archive/man_open.png\")\n",
    "# plt.imshow(img)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "i = 0\n",
    "for (x, y, w, h) in eyes:\n",
    "    i += 1\n",
    "    print(f\"Eye {i}: ({x}, {y}, {w}, {h})\")\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)  \n",
    "    \n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23596\\2930372155.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#CROP THE EYES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0meye_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhaarcascades\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'haarcascade_eye.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0meyes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meye_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "#CROP THE EYES\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "# eyes_roi = []\n",
    "for x, y, w, h in eyes:\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    eyess = eye_cascade.detectMultiScale(roi_gray)\n",
    "    if len(eyess) == 0:\n",
    "        print(\"eyes are not detected\")\n",
    "    else:\n",
    "        for (ex, ey, ew, eh) in eyess:\n",
    "            eyes_roi = roi_color[ey: ey+eh, ex: ex+ew]\n",
    "\n",
    "plt.imshow(cv2.cvtColor(eyes_roi, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eyes_roi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23596\\758556163.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meyes_roi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'eyes_roi' is not defined"
     ]
    }
   ],
   "source": [
    "eyes_roi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the image\n",
    "cv2.imwrite(\"archive/test.jpg\", eyes_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Open-Eyes\n"
     ]
    }
   ],
   "source": [
    "test = predict_image_fromPath(\"archive/test.jpg\", loaded_model, class_mapping)\n",
    "print(f\"Predicted Class: {test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 26244 features, but SVC is expecting 1764 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23060\\4224360723.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0meyes_roi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroi_color\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mey\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0meh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mew\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;31m#USING THE MODEL TO PREDICT THE EYE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meyes_roi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Predicted Class: {test}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23060\\2906226348.py\u001b[0m in \u001b[0;36mpredict_image\u001b[1;34m(image, model, class_mapping, target_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mfeature_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_hog_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessed_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;31m# Map label back to class name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclass_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \"\"\"\n\u001b[1;32m--> 414\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m                 \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                 \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             )\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m             raise ValueError(\n\u001b[1;32m--> 401\u001b[1;33m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m                 \u001b[1;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 26244 features, but SVC is expecting 1764 features as input."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import time\n",
    "# import winsound  # For beep sound (Windows)\n",
    "\n",
    "\n",
    "# # Load Haar cascade classifiers\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "# eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# if not cap.isOpened():\n",
    "#     cap = cv2.VideoCapture(1)\n",
    "# if not cap.isOpened():\n",
    "#     raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# eyes_closed_time = None  # Track when eyes were first closed\n",
    "# ALERT_THRESHOLD = 3  # Seconds before triggering an alert\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "#     eyes_detected = False  # Default: No eyes detected\n",
    "\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "#         roi_gray = gray[y:y + h, x:x + w]\n",
    "#         roi_color = frame[y:y + h, x:x + w]\n",
    "\n",
    "#         eyes = eye_cascade.detectMultiScale(roi_gray, 1.1, 4)\n",
    "\n",
    "#         if len(eyes) > 0:\n",
    "#             eyes_detected = True  # Eyes detected, reset timer\n",
    "#             for (ex, ey, ew, eh) in eyes:\n",
    "#                 cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (255, 0,\n",
    "#                                                                             0), 2)\n",
    "#                 #use model to predict the eye\n",
    "#                 eyes_roi = roi_color[ey: ey+eh, ex: ex+ew]\n",
    "#                 #USING THE MODEL TO PREDICT THE EYE\n",
    "#                 test = predict_image(eyes_roi, loaded_model, class_mapping)\n",
    "#                 print(f\"Predicted Class: {test}\")\n",
    "    \n",
    "\n",
    "\n",
    "#     # Check if eyes are closed or not detected\n",
    "#     if not eyes_detected:\n",
    "#         if eyes_closed_time is None:  # Start timing when eyes close/disappear\n",
    "#             eyes_closed_time = time.time()\n",
    "#         elif time.time() - eyes_closed_time >= ALERT_THRESHOLD:\n",
    "#             print(\"ALERT! Eyes closed/not detected for 3 seconds!\")\n",
    "#             winsound.Beep(2500, 1000)  # Beep sound (2500 Hz, 1 sec)\n",
    "#             eyes_closed_time = None  # Reset timer after alert\n",
    "#     else:\n",
    "#         eyes_closed_time = None  # Reset if eyes open\n",
    "\n",
    "#     cv2.imshow('Drowsiness Detection', frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tarek\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py:338: UserWarning: Trying to unpickle estimator SVC from version 1.5.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State changed: Eyes Not Detected (via Regular-Cascade, confidence: 1.00)\n",
      "State changed: Open-Eyes (via Glasses-Cascade, confidence: 0.67)\n",
      "State changed: No Face (via None, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Glasses-Cascade, confidence: 0.67)\n",
      "State changed: No Face (via None, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Last-ROI-Fallback, confidence: 0.67)\n",
      "State changed: Close-Eyes (via Last-ROI-Fallback, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Glasses-Cascade, confidence: 0.67)\n",
      "State changed: No Face (via None, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Glasses-Cascade, confidence: 0.67)\n",
      "State changed: No Face (via None, confidence: 0.67)\n",
      "State changed: Close-Eyes (via Last-ROI-Fallback, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Glasses-Cascade, confidence: 0.67)\n",
      "State changed: No Face (via None, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Regular-Cascade, confidence: 0.67)\n",
      "State changed: Close-Eyes (via Last-ROI-Fallback, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Glasses-Cascade, confidence: 0.67)\n",
      "State changed: Close-Eyes (via Glasses-Cascade, confidence: 0.67)\n",
      "State changed: No Face (via None, confidence: 0.67)\n",
      "State changed: Close-Eyes (via Glasses-Cascade, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Last-ROI-Fallback, confidence: 0.67)\n",
      "State changed: No Face (via None, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Glasses-Cascade, confidence: 0.67)\n",
      "State changed: Close-Eyes (via Last-ROI-Fallback, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Glasses-Cascade, confidence: 0.67)\n",
      "State changed: Close-Eyes (via Last-ROI-Fallback, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Last-ROI-Fallback, confidence: 0.67)\n",
      "State changed: No Face (via None, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Last-ROI-Fallback, confidence: 0.67)\n",
      "State changed: No Face (via None, confidence: 0.67)\n",
      "State changed: Eyes Not Detected (via Regular-Cascade, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Last-ROI-Fallback, confidence: 0.67)\n",
      "State changed: Close-Eyes (via Glasses-Cascade, confidence: 0.67)\n",
      "State changed: Open-Eyes (via Glasses-Cascade, confidence: 0.67)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import dlib  # For more robust facial landmark detection\n",
    "\n",
    "# Create directories if they don't exist\n",
    "if not os.path.exists(\"archive\"):\n",
    "    os.makedirs(\"archive\")\n",
    "\n",
    "# Load the HOG-based SVM model for eye state classification\n",
    "loaded_model = pickle.load(open(\"SVM_model.sav\", 'rb'))\n",
    "class_mapping = pickle.load(open(\"class_mapping.pickle\", \"rb\"))\n",
    "\n",
    "# Initialize face detector and landmark predictor from dlib for better accuracy\n",
    "try:\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    # You'll need to download this file: http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "    # If not available, we'll fall back to Haar cascades\n",
    "    landmark_predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    using_dlib = True\n",
    "    print(\"Using dlib for enhanced face and eye detection\")\n",
    "except:\n",
    "    print(\"Dlib not available or shape predictor file missing, falling back to Haar cascades\")\n",
    "    using_dlib = False\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    # Load both regular eye detector and eye glasses detector\n",
    "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "    eyeglasses_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye_tree_eyeglasses.xml')\n",
    "\n",
    "# Constants for eye detection\n",
    "EYE_AR_THRESH = 0.2       # Eye aspect ratio threshold for closed eyes\n",
    "EYE_AR_CONSEC_FRAMES = 3  # Number of consecutive frames the eye must be below threshold\n",
    "\n",
    "# Eye tracking class with enhanced features\n",
    "class EyeTracker:\n",
    "    def __init__(self):\n",
    "        self.state = \"Unknown\"\n",
    "        self.confidence = 0.0\n",
    "        self.closed_time_start = None\n",
    "        self.alert_active = False\n",
    "        self.alert_threshold = 3.0  # Seconds\n",
    "        self.history = []\n",
    "        self.history_size = 20\n",
    "        self.consecutive_closed = 0\n",
    "        self.has_glasses = False\n",
    "        self.detection_method = None\n",
    "        self.last_successful_eye_regions = None\n",
    "        \n",
    "    def update(self, eye_state, detection_confidence, detection_method):\n",
    "        # Store the detection method\n",
    "        self.detection_method = detection_method\n",
    "        \n",
    "        # Add to history for smoothing\n",
    "        self.history.append(eye_state)\n",
    "        if len(self.history) > self.history_size:\n",
    "            self.history.pop(0)\n",
    "        \n",
    "        # Count state occurrences\n",
    "        state_counts = {}\n",
    "        for state in self.history:\n",
    "            if state in state_counts:\n",
    "                state_counts[state] += 1\n",
    "            else:\n",
    "                state_counts[state] = 1\n",
    "        \n",
    "        # Get most common state\n",
    "        if self.history:\n",
    "            most_common = max(state_counts.items(), key=lambda x: x[1])\n",
    "            smoothed_state = most_common[0]\n",
    "            self.confidence = most_common[1] / len(self.history)\n",
    "            \n",
    "            # Update state if confident enough\n",
    "            if self.confidence > 0.6:  # 60% confidence threshold\n",
    "                if self.state != smoothed_state:\n",
    "                    print(f\"State changed: {smoothed_state} (via {detection_method})\")\n",
    "                    self.state = smoothed_state\n",
    "        \n",
    "        # Track closed eyes for drowsiness detection\n",
    "        if self.state == \"Close-Eyes\":\n",
    "            if self.closed_time_start is None:\n",
    "                self.closed_time_start = time.time()\n",
    "                self.consecutive_closed += 1\n",
    "            else:\n",
    "                self.consecutive_closed += 1\n",
    "        else:\n",
    "            self.closed_time_start = None\n",
    "            self.consecutive_closed = 0\n",
    "            self.alert_active = False\n",
    "\n",
    "    def check_drowsiness(self):\n",
    "        if self.closed_time_start is not None:\n",
    "            elapsed = time.time() - self.closed_time_start\n",
    "            if elapsed >= self.alert_threshold and not self.alert_active:\n",
    "                self.alert_active = True\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def set_glasses_status(self, has_glasses):\n",
    "        self.has_glasses = has_glasses\n",
    "    \n",
    "    def get_closed_duration(self):\n",
    "        if self.closed_time_start is None:\n",
    "            return 0\n",
    "        return time.time() - self.closed_time_start\n",
    "\n",
    "# Function to extract eye regions using dlib landmarks\n",
    "def get_eye_regions_dlib(frame, gray, rect):\n",
    "    shape = landmark_predictor(gray, rect)\n",
    "    landmarks = np.array([(shape.part(i).x, shape.part(i).y) for i in range(68)])\n",
    "    \n",
    "    # Extract eye landmarks\n",
    "    left_eye = landmarks[36:42]  # Left eye landmarks\n",
    "    right_eye = landmarks[42:48]  # Right eye landmarks\n",
    "    \n",
    "    # Create eye region masks\n",
    "    left_eye_hull = cv2.convexHull(left_eye)\n",
    "    right_eye_hull = cv2.convexHull(right_eye)\n",
    "    \n",
    "    # Draw eye regions on the frame for visualization\n",
    "    cv2.drawContours(frame, [left_eye_hull], -1, (0, 255, 0), 1)\n",
    "    cv2.drawContours(frame, [right_eye_hull], -1, (0, 255, 0), 1)\n",
    "    \n",
    "    # Create mask for the eye regions\n",
    "    mask = np.zeros(gray.shape, dtype=np.uint8)\n",
    "    cv2.drawContours(mask, [left_eye_hull], -1, 255, -1)\n",
    "    cv2.drawContours(mask, [right_eye_hull], -1, 255, -1)\n",
    "    \n",
    "    # Apply mask to get only the eye regions\n",
    "    eyes_gray = cv2.bitwise_and(gray, gray, mask=mask)\n",
    "    \n",
    "    # Find eye region bounding boxes\n",
    "    x_left, y_left, w_left, h_left = cv2.boundingRect(left_eye)\n",
    "    x_right, y_right, w_right, h_right = cv2.boundingRect(right_eye)\n",
    "    \n",
    "    # Extract eye images with some padding\n",
    "    padding = 5\n",
    "    left_eye_img = frame[max(0, y_left-padding):min(gray.shape[0], y_left+h_left+padding), \n",
    "                          max(0, x_left-padding):min(gray.shape[1], x_left+w_left+padding)]\n",
    "    right_eye_img = frame[max(0, y_right-padding):min(gray.shape[0], y_right+h_right+padding), \n",
    "                           max(0, x_right-padding):min(gray.shape[1], x_right+w_right+padding)]\n",
    "    \n",
    "    # Calculate eye aspect ratio to detect eyeglasses\n",
    "    ear_left = eye_aspect_ratio(left_eye)\n",
    "    ear_right = eye_aspect_ratio(right_eye)\n",
    "    avg_ear = (ear_left + ear_right) / 2.0\n",
    "    \n",
    "    # Glasses detection based on edge detection\n",
    "    edges = cv2.Canny(eyes_gray, 30, 100)\n",
    "    glasses_score = np.sum(edges) / (left_eye_hull.shape[0] + right_eye_hull.shape[0])\n",
    "    has_glasses = glasses_score > 20  # Threshold for glasses detection\n",
    "    \n",
    "    return left_eye_img, right_eye_img, has_glasses, avg_ear\n",
    "\n",
    "# Function to calculate eye aspect ratio\n",
    "def eye_aspect_ratio(eye):\n",
    "    # Compute the euclidean distances between the vertical eye landmarks\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    \n",
    "    # Compute the euclidean distance between the horizontal eye landmarks\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    \n",
    "    # Compute the eye aspect ratio\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to enhance eye region for better feature extraction\n",
    "def enhance_eye_region(eye_img):\n",
    "    if eye_img is None or eye_img.size == 0:\n",
    "        return None\n",
    "        \n",
    "    # Convert to grayscale if it's a color image\n",
    "    if len(eye_img.shape) == 3:\n",
    "        eye_gray = cv2.cvtColor(eye_img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        eye_gray = eye_img.copy()\n",
    "    \n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    eye_gray = clahe.apply(eye_gray)\n",
    "    \n",
    "    # Bilateral filter for noise reduction while preserving edges\n",
    "    eye_gray = cv2.bilateralFilter(eye_gray, 9, 75, 75)\n",
    "    \n",
    "    return eye_gray\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Create tracker\n",
    "tracker = EyeTracker()\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Create a copy for display\n",
    "    display = frame.copy()\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply CLAHE for better contrast\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced_gray = clahe.apply(gray)\n",
    "    \n",
    "    eye_state = \"Unknown\"\n",
    "    detection_confidence = 0.0\n",
    "    detection_method = \"None\"\n",
    "    has_glasses = False\n",
    "    \n",
    "    # Try to detect faces and eyes using available methods\n",
    "    if using_dlib:\n",
    "        # Detect faces using dlib\n",
    "        faces = face_detector(enhanced_gray)\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            # Process the largest face\n",
    "            face = max(faces, key=lambda rect: rect.width() * rect.height())\n",
    "            \n",
    "            # Get eye regions using facial landmarks\n",
    "            left_eye, right_eye, has_glasses, eye_ar = get_eye_regions_dlib(display, enhanced_gray, face)\n",
    "            \n",
    "            # Save the eye regions for future reference\n",
    "            if left_eye is not None and left_eye.size > 0 and right_eye is not None and right_eye.size > 0:\n",
    "                tracker.last_successful_eye_regions = (left_eye, right_eye)\n",
    "                \n",
    "                # Determine eye state based on aspect ratio and model prediction\n",
    "                if eye_ar < EYE_AR_THRESH:\n",
    "                    eye_state = \"Close-Eyes\"\n",
    "                    detection_confidence = 1.0 - (eye_ar / EYE_AR_THRESH)\n",
    "                    detection_method = \"EAR\"\n",
    "                else:\n",
    "                    # Try to predict using the model\n",
    "                    try:\n",
    "                        # Save and predict with the larger eye\n",
    "                        if left_eye.size > right_eye.size:\n",
    "                            eye_img = left_eye\n",
    "                        else:\n",
    "                            eye_img = right_eye\n",
    "                            \n",
    "                        if eye_img.size > 0:\n",
    "                            cv2.imwrite(\"archive/test.jpg\", eye_img)\n",
    "                            eye_state = predict_image_fromPath(\"archive/test.jpg\", loaded_model, class_mapping)\n",
    "                            detection_confidence = 0.8\n",
    "                            detection_method = \"SVM-Model\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"Prediction error: {e}\")\n",
    "                        # Use aspect ratio as fallback\n",
    "                        eye_state = \"Open-Eyes\" if eye_ar >= EYE_AR_THRESH else \"Close-Eyes\"\n",
    "                        detection_confidence = 0.7\n",
    "                        detection_method = \"EAR-Fallback\"\n",
    "            \n",
    "            # Set glasses status\n",
    "            tracker.set_glasses_status(has_glasses)\n",
    "            \n",
    "            # Draw face rectangle\n",
    "            x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "            cv2.rectangle(display, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "    else:\n",
    "        # Fallback to Haar cascades\n",
    "        faces = face_cascade.detectMultiScale(enhanced_gray, 1.1, 5, minSize=(100, 100))\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            # Process the largest face\n",
    "            x, y, w, h = max(faces, key=lambda face: face[2] * face[3])\n",
    "            \n",
    "            # Draw face rectangle\n",
    "            cv2.rectangle(display, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            # Extract face region\n",
    "            face_gray = enhanced_gray[y:y+h, x:x+w]\n",
    "            face_color = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Try to detect eyes with glasses first\n",
    "            eyes_with_glasses = eyeglasses_cascade.detectMultiScale(\n",
    "                face_gray, \n",
    "                scaleFactor=1.1, \n",
    "                minNeighbors=5, \n",
    "                minSize=(30, 30)\n",
    "            )\n",
    "            \n",
    "            # If found eyes with glasses detector, use those and mark as having glasses\n",
    "            if len(eyes_with_glasses) > 0:\n",
    "                eyes = eyes_with_glasses\n",
    "                has_glasses = True\n",
    "                detection_method = \"Glasses-Cascade\"\n",
    "            else:\n",
    "                # Try regular eye detector\n",
    "                eyes = eye_cascade.detectMultiScale(\n",
    "                    face_gray, \n",
    "                    scaleFactor=1.1, \n",
    "                    minNeighbors=5, \n",
    "                    minSize=(30, 30)\n",
    "                )\n",
    "                has_glasses = False\n",
    "                detection_method = \"Regular-Cascade\"\n",
    "            \n",
    "            # Process detected eyes\n",
    "            if len(eyes) > 0:\n",
    "                # Find the largest eye\n",
    "                ex, ey, ew, eh = max(eyes, key=lambda eye: eye[2] * eye[3])\n",
    "                \n",
    "                # Draw eye rectangle\n",
    "                cv2.rectangle(face_color, (ex, ey), (ex+ew, ey+eh), (255, 0, 0), 2)\n",
    "                \n",
    "                # Extract and enhance eye region\n",
    "                eye_roi = face_color[ey:ey+eh, ex:ex+ew]\n",
    "                \n",
    "                if eye_roi.size > 0:\n",
    "                    # Save last successful detection\n",
    "                    tracker.last_successful_eye_regions = (eye_roi, None)\n",
    "                    \n",
    "                    # Save and predict\n",
    "                    cv2.imwrite(\"archive/test.jpg\", eye_roi)\n",
    "                    try:\n",
    "                        eye_state = predict_image_fromPath(\"archive/test.jpg\", loaded_model, class_mapping)\n",
    "                        detection_confidence = 0.8\n",
    "                    except:\n",
    "                        eye_state = \"Unknown\"\n",
    "                        detection_confidence = 0.5\n",
    "            else:\n",
    "                # No eyes detected, try to use previously detected eye regions if available\n",
    "                if tracker.last_successful_eye_regions is not None:\n",
    "                    eye_state = \"Eyes Not Detected\"\n",
    "                    detection_confidence = 0.4\n",
    "                    detection_method = \"Last-Known\"\n",
    "    \n",
    "    # Update tracker with detected eye state\n",
    "    tracker.update(eye_state, detection_confidence, detection_method)\n",
    "    \n",
    "    # Check for drowsiness\n",
    "    if tracker.check_drowsiness():\n",
    "        # Alert for drowsiness\n",
    "        try:\n",
    "            import winsound\n",
    "            winsound.Beep(1000, 200)  # Beep at 1000 Hz for 200 ms\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Create info panel\n",
    "    info_panel = np.zeros((200, frame.shape[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Add information to the panel\n",
    "    cv2.putText(info_panel, f\"Eye State: {tracker.state}\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.putText(info_panel, f\"Detection Method: {tracker.detection_method}\", (10, 60), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "    \n",
    "    cv2.putText(info_panel, f\"Confidence: {tracker.confidence:.2f}\", (10, 90), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "    \n",
    "    cv2.putText(info_panel, f\"Glasses: {'Yes' if tracker.has_glasses else 'No'}\", (10, 120), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "    \n",
    "    # Show drowsiness warning if necessary\n",
    "    if tracker.closed_time_start is not None:\n",
    "        elapsed = tracker.get_closed_duration()\n",
    "        cv2.putText(info_panel, f\"Eyes closed for: {elapsed:.1f}s\", (10, 150), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n",
    "        \n",
    "        if elapsed >= tracker.alert_threshold:\n",
    "            cv2.putText(info_panel, \"DROWSINESS ALERT!\", (10, 190), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)\n",
    "    \n",
    "    # Combine display and info panel\n",
    "    combined = np.vstack((display, info_panel))\n",
    "    \n",
    "    # Show the result\n",
    "    cv2.imshow(\"Drowsiness Detection\", combined)\n",
    "    \n",
    "    # Exit on 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "\n",
    "#new code\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IntialiseEnv():\n",
    "    nb_dir = os.path.split(os.getcwd())[0]\n",
    "    if nb_dir not in sys.path:\n",
    "        sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import sys\n",
    "IntialiseEnv()\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory exists: True\n",
      "Test directory exists: True\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_dir = r\"D:\\GP_datasets\\archive(1)\"\n",
    "train_path = os.path.join(data_dir, \"Train\")\n",
    "test_path = os.path.join(data_dir, \"Test\")\n",
    "\n",
    "# Check if the paths exist\n",
    "print(f\"Train directory exists: {os.path.exists(train_path)}\")\n",
    "print(f\"Test directory exists: {os.path.exists(test_path)}\")\n",
    "\n",
    "# Define image dimensions for LeNet5 (traditionally 32x32)\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "def build_lenet5_model(input_shape=(32, 32, 3), num_classes=43):\n",
    "    \"\"\"\n",
    "    Build LeNet5 model for traffic sign classification\n",
    "    \n",
    "    Parameters:\n",
    "    - input_shape: shape of input images (height, width, channels)\n",
    "    - num_classes: number of traffic sign classes\n",
    "    \n",
    "    Returns:\n",
    "    - model: compiled LeNet5 model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Layer 1: Convolutional + ReLU + Pooling\n",
    "        layers.Conv2D(6, kernel_size=(5, 5), padding='valid', activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "        \n",
    "        # Layer 2: Convolutional + ReLU + Pooling\n",
    "        layers.Conv2D(16, kernel_size=(5, 5), padding='valid', activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "        \n",
    "        # Flatten\n",
    "        layers.Flatten(),\n",
    "        \n",
    "        # Layer 3: Fully Connected + ReLU\n",
    "        layers.Dense(120, activation='relu'),\n",
    "        \n",
    "        # Layer 4: Fully Connected + ReLU\n",
    "        layers.Dense(84, activation='relu'),\n",
    "        \n",
    "        # Output Layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Preprocess data for LeNet5 model\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: training images\n",
    "    - X_test: testing images\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_normalized: normalized training images\n",
    "    - X_test_normalized: normalized testing images\n",
    "    \"\"\"\n",
    "    # Resize images to 32x32 if they aren't already\n",
    "    # (LeNet5 was designed for 32x32 images)\n",
    "    if X_train.shape[1:3] != (32, 32):\n",
    "        # Implementation of resizing would go here\n",
    "        pass\n",
    "        \n",
    "    # Normalize pixel values to [0,1]\n",
    "    X_train_normalized = X_train.astype('float32') / 255.0\n",
    "    X_test_normalized = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    return X_train_normalized, X_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, batch_size=32, epochs=20):\n",
    "    \"\"\"\n",
    "    Train the LeNet5 model\n",
    "    \n",
    "    Parameters:\n",
    "    - model: compiled LeNet5 model\n",
    "    - X_train, y_train: training data\n",
    "    - X_test, y_test: test data\n",
    "    - batch_size: batch size for training\n",
    "    - epochs: number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "    - history: training history\n",
    "    - model: trained model\n",
    "    \"\"\"\n",
    "    # Data augmentation to help with generalization\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        shear_range=0.1\n",
    "    )\n",
    "    \n",
    "    # Convert labels to categorical if they aren't already\n",
    "    if len(y_train.shape) == 1:\n",
    "        y_train = tf.keras.utils.to_categorical(y_train)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folders(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load data from folder structure if pickle files don't exist\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading training data from folders...\")\n",
    "    for class_id in range(43):  # 43 classes in GTSRB dataset\n",
    "        class_path = os.path.join(train_path, str(class_id))\n",
    "        if not os.path.exists(class_path):\n",
    "            continue\n",
    "            \n",
    "        for image_file in os.listdir(class_path):\n",
    "            if not image_file.endswith('.png') and not image_file.endswith('.jpg') and not image_file.endswith('.ppm'):\n",
    "                continue\n",
    "                \n",
    "            img_path = os.path.join(class_path, image_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "                \n",
    "            img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            train_data.append(img)\n",
    "            train_labels.append(class_id)\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"Loading test data from folders...\")\n",
    "    for class_id in range(43):\n",
    "        class_path = os.path.join(test_path, str(class_id))\n",
    "        if not os.path.exists(class_path):\n",
    "            continue\n",
    "            \n",
    "        for image_file in os.listdir(class_path):\n",
    "            if not image_file.endswith('.png') and not image_file.endswith('.jpg') and not image_file.endswith('.ppm'):\n",
    "                continue\n",
    "                \n",
    "            img_path = os.path.join(class_path, image_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "                \n",
    "            img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            test_data.append(img)\n",
    "            test_labels.append(class_id)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    train_data = np.array(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_data = np.array(test_data)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    print(f\"Training data loaded: {train_data.shape}, {train_labels.shape}\")\n",
    "    print(f\"Test data loaded: {test_data.shape}, {test_labels.shape}\")\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle():\n",
    "    \"\"\"\n",
    "    Load data from pickle files if they exist\n",
    "    \"\"\"\n",
    "    train_file = os.path.join(data_dir, \"Processed_DataSet\", \"GermanTrainDataSet.pkl\")\n",
    "    test_file = os.path.join(data_dir, \"Processed_DataSet\", \"GermanTestDataSet.pkl\")\n",
    "    \n",
    "    train_data, train_labels = None, None\n",
    "    test_data, test_labels = None, None\n",
    "    \n",
    "    if os.path.exists(train_file):\n",
    "        print(\"Loading training data from pickle file...\")\n",
    "        with open(train_file, 'rb') as f:\n",
    "            train_data, train_labels = pickle.load(f)\n",
    "        print(f\"Training data loaded: {train_data.shape}, {train_labels.shape}\")\n",
    "    \n",
    "    if os.path.exists(test_file):\n",
    "        print(\"Loading test data from pickle file...\")\n",
    "        with open(test_file, 'rb') as f:\n",
    "            test_data, test_labels = pickle.load(f)\n",
    "        print(f\"Test data loaded: {test_data.shape}, {test_labels.shape}\")\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data_from_pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16088\\1866158418.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data_from_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# If pickle files don't exist, load from folders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data_from_pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data, train_labels, test_data, test_labels = load_data_from_pickle()\n",
    "\n",
    "# If pickle files don't exist, load from folders\n",
    "if train_data is None or test_data is None:\n",
    "    train_data, train_labels, test_data, test_labels = load_data_from_folders(train_path, test_path)\n",
    "\n",
    "# Check data shapes and types\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "# Check unique classes\n",
    "num_classes = len(np.unique(train_labels))\n",
    "print(f\"Number of unique classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your data loaded as X_train, y_train, X_test, y_test\n",
    "\n",
    "x\n",
    "\n",
    "# 1. Preprocess the data\n",
    "X_train_preprocessed, X_test_preprocessed = preprocess_data(X_train, X_test)\n",
    "\n",
    "# 2. Build the LeNet5 model\n",
    "model = build_lenet5_model(input_shape=(32, 32, 3), num_classes=len(np.unique(y_train)))\n",
    "\n",
    "# 3. Train the model\n",
    "history, trained_model = train_model(\n",
    "    model,\n",
    "    X_train_preprocessed, y_train,\n",
    "    X_test_preprocessed, y_test,\n",
    "    batch_size=64,\n",
    "    epochs=30\n",
    ")\n",
    "\n",
    "# 4. Save the trained model\n",
    "trained_model.save(\"traffic_sign_lenet5_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

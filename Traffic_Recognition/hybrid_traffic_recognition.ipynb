{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf1d3d1",
   "metadata": {},
   "source": [
    "# Hybrid Approach\n",
    "- Deep learning to localize the sign using YOLO\n",
    "- classical ML to recognize the traffic sign using SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e096851",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa4db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88970573",
   "metadata": {},
   "source": [
    "# Dataset Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3185652",
   "metadata": {},
   "source": [
    "### dataset to YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996b440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset prepared at: YOLO_dataset\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "root_dir = \"YOLODataset\"\n",
    "ts_dir = os.path.join(root_dir, \"ts\")\n",
    "train_txt = os.path.join(root_dir, \"train.txt\")\n",
    "val_txt = os.path.join(root_dir, \"test.txt\")\n",
    "\n",
    "# Output YOLO structure\n",
    "output_dir = \"YOLO_dataset\"\n",
    "images_train = os.path.join(output_dir, \"images/train\")\n",
    "images_val = os.path.join(output_dir, \"images/val\")\n",
    "labels_train = os.path.join(output_dir, \"labels/train\")\n",
    "labels_val = os.path.join(output_dir, \"labels/val\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(images_train, exist_ok=True)\n",
    "os.makedirs(images_val, exist_ok=True)\n",
    "os.makedirs(labels_train, exist_ok=True)\n",
    "os.makedirs(labels_val, exist_ok=True)\n",
    "\n",
    "def process_split(txt_path, image_dst, label_dst):\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        image_path = line.strip()\n",
    "        image_name = os.path.basename(image_path)\n",
    "        label_name = os.path.splitext(image_name)[0] + \".txt\"\n",
    "\n",
    "        full_image_path = os.path.join(ts_dir, image_name)\n",
    "        full_label_path = os.path.join(ts_dir, label_name)\n",
    "\n",
    "        # Copy image\n",
    "        shutil.copy(full_image_path, os.path.join(image_dst, image_name))\n",
    "\n",
    "        # Copy label\n",
    "        if os.path.exists(full_label_path):\n",
    "            shutil.copy(full_label_path, os.path.join(label_dst, label_name))\n",
    "        else:\n",
    "            print(f\"âš ï¸ Label not found for {image_name}\")\n",
    "\n",
    "# Process train and val splits\n",
    "process_split(train_txt, images_train, labels_train)\n",
    "process_split(val_txt, images_val, labels_val)\n",
    "\n",
    "# Create data.yaml\n",
    "data_yaml_path = os.path.join(output_dir, \"data.yaml\")\n",
    "with open(data_yaml_path, \"w\") as f:\n",
    "    f.write(f\"\"\"train: {os.path.abspath(images_train)}\n",
    "val: {os.path.abspath(images_val)}\n",
    "\n",
    "nc: 4\n",
    "names: ['prohibitory', 'danger', 'mandatory', 'other']\n",
    "\"\"\")\n",
    "\n",
    "print(\" Dataset prepared at:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56ff69",
   "metadata": {},
   "source": [
    "### filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb243f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New YOLO dataset created with only selected classes at: car_yolo_filtered\n"
     ]
    }
   ],
   "source": [
    "# Original dataset class names\n",
    "ORIGINAL_NAMES = [\n",
    "    'Green Light', 'Red Light', 'Speed Limit 10', 'Speed Limit 100', 'Speed Limit 110',\n",
    "    'Speed Limit 120', 'Speed Limit 20', 'Speed Limit 30', 'Speed Limit 40', 'Speed Limit 50',\n",
    "    'Speed Limit 60', 'Speed Limit 70', 'Speed Limit 80', 'Speed Limit 90', 'Stop'\n",
    "]\n",
    "\n",
    "# Mapping original class indices â†’ new class indices\n",
    "CLASS_MAP = {\n",
    "    0: 4,  # Green Light â†’ traffic light\n",
    "    1: 4,  # Red Light â†’ traffic light\n",
    "    2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0  # Speed Limits + Stop â†’ prohibitory\n",
    "    # Other classes (danger, mandatory, etc.) can be added similarly if needed\n",
    "}\n",
    "\n",
    "# Output dataset structure\n",
    "original_root = Path(\"car_yolo_format\")   # original dataset\n",
    "new_root = Path(\"car_yolo_filtered\")      # new filtered dataset\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = original_root / split / \"images\"\n",
    "    lbl_dir = original_root / split / \"labels\"\n",
    "\n",
    "    new_img_dir = new_root / split / \"images\"\n",
    "    new_lbl_dir = new_root / split / \"labels\"\n",
    "    new_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    new_lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for lbl_file in lbl_dir.glob(\"*.txt\"):\n",
    "        new_lines = []\n",
    "        with open(lbl_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                original_cls = int(parts[0])\n",
    "                bbox = parts[1:]\n",
    "\n",
    "                # Filter and remap class\n",
    "                if original_cls in CLASS_MAP:\n",
    "                    new_cls = CLASS_MAP[original_cls]\n",
    "                    new_lines.append(f\"{new_cls} {' '.join(bbox)}\\n\")\n",
    "\n",
    "        if new_lines:\n",
    "            # Save filtered label\n",
    "            new_lbl_path = new_lbl_dir / lbl_file.name\n",
    "            with open(new_lbl_path, \"w\") as f:\n",
    "                f.writelines(new_lines)\n",
    "\n",
    "            # Copy image\n",
    "            img_path = img_dir / (lbl_file.stem + \".jpg\")\n",
    "            if img_path.exists():\n",
    "                shutil.copy(img_path, new_img_dir / img_path.name)\n",
    "\n",
    "# Save new data.yaml\n",
    "with open(new_root / \"data.yaml\", \"w\") as f:\n",
    "    f.write(\"\"\"train: ../train/images\n",
    "val: ../val/images\n",
    "test: ../test/images\n",
    "\n",
    "nc: 5\n",
    "names: ['prohibitory', 'danger', 'mandatory', 'other', 'traffic light']\n",
    "\"\"\")\n",
    "\n",
    "print(\" New YOLO dataset created with only selected classes at:\", new_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fea6a1",
   "metadata": {},
   "source": [
    "### Traffic light dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ca255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New traffic light-only dataset created at: traffic_light_dataset\n"
     ]
    }
   ],
   "source": [
    "# Traffic light class indices in original dataset\n",
    "TRAFFIC_LIGHT_CLASSES = [0, 1]  # Green Light and Red Light in original data\n",
    "\n",
    "# Dataset paths\n",
    "original_root = Path(\"car_yolo_format\")  # original dataset\n",
    "new_root = Path(\"traffic_light_dataset\")  # new dataset only with traffic lights\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = original_root / split / \"images\"\n",
    "    lbl_dir = original_root / split / \"labels\"\n",
    "\n",
    "    new_img_dir = new_root / split / \"images\"\n",
    "    new_lbl_dir = new_root / split / \"labels\"\n",
    "    new_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    new_lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for lbl_file in lbl_dir.glob(\"*.txt\"):\n",
    "        new_lines = []\n",
    "        with open(lbl_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                cls = int(parts[0])\n",
    "                bbox = parts[1:]\n",
    "\n",
    "                if cls in TRAFFIC_LIGHT_CLASSES:\n",
    "                    new_lines.append(f\"0 {' '.join(bbox)}\\n\")  # remap to class 0\n",
    "\n",
    "        if new_lines:\n",
    "            # Save label\n",
    "            new_lbl_path = new_lbl_dir / lbl_file.name\n",
    "            with open(new_lbl_path, \"w\") as f:\n",
    "                f.writelines(new_lines)\n",
    "\n",
    "            # Copy image\n",
    "            img_path = img_dir / (lbl_file.stem + \".jpg\")\n",
    "            if img_path.exists():\n",
    "                shutil.copy(img_path, new_img_dir / img_path.name)\n",
    "\n",
    "# Save new data.yaml\n",
    "with open(new_root / \"data.yaml\", \"w\") as f:\n",
    "    f.write(\"\"\"train: ../train/images\n",
    "val: ../val/images\n",
    "test: ../test/images\n",
    "\n",
    "nc: 1\n",
    "names: ['traffic light']\n",
    "\"\"\")\n",
    "\n",
    "print(\" New traffic light-only dataset created at:\", new_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb918b",
   "metadata": {},
   "source": [
    "### change the traffic light class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f74e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All class 0 labels changed to class 4.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ðŸ” Path to your YOLO dataset labels\n",
    "label_dir = Path(\"C:/Users/OSAMA/Downloads/erp_traffic_light.v1i.yolov8/train/labels\")\n",
    "\n",
    "for txt_file in label_dir.glob(\"*.txt\"):\n",
    "    updated_lines = []\n",
    "\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            cls = int(parts[0])\n",
    "            # Change class 0 to class 4\n",
    "            if cls == 0:\n",
    "                parts[0] = \"4\"\n",
    "            updated_lines.append(\" \".join(parts) + \"\\n\")\n",
    "\n",
    "    # Overwrite with updated content\n",
    "    with open(txt_file, \"w\") as f:\n",
    "        f.writelines(updated_lines)\n",
    "\n",
    "print(\"All class 0 labels changed to class 4.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130e006",
   "metadata": {},
   "source": [
    "# Train YOLO detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678bfbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n.yaml\").load(\"yolov8n.pt\")  # build from YAML and transfer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0467b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.133 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.9 torch-1.13.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=YOLO_dataset/data.yaml, epochs=5, patience=50, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train8\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3011628 parameters, 3011612 gradients\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train8', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\labels\\train.cache... 630 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 630/630 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\images\\train\\00340.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\labels\\val.cache... 111 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train8\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 960 train, 960 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train8\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/5         0G      1.057      3.641     0.8755         12        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [08:27<00:00, 12.68s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:24<00:00,  6.02s/it]\n",
      "                   all        111        179    0.00197      0.318      0.231      0.196\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/5         0G     0.9083      2.398     0.8652          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [08:11<00:00, 12.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:23<00:00,  5.75s/it]\n",
      "                   all        111        179          1      0.155      0.717       0.57\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/5         0G     0.8593      1.977     0.8532         16        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [08:09<00:00, 12.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:22<00:00,  5.72s/it]\n",
      "                   all        111        179      0.929      0.817      0.903      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        4/5         0G     0.8247      1.748      0.844         16        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [08:09<00:00, 12.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:22<00:00,  5.74s/it]\n",
      "                   all        111        179      0.915      0.897       0.94      0.769\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        5/5         0G     0.8066      1.548     0.8434          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [08:07<00:00, 12.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:23<00:00,  5.79s/it]\n",
      "                   all        111        179      0.903      0.896      0.959      0.794\n",
      "\n",
      "5 epochs completed in 0.718 hours.\n",
      "Optimizer stripped from runs\\detect\\train8\\weights\\last.pt, 6.3MB\n",
      "Optimizer stripped from runs\\detect\\train8\\weights\\best.pt, 6.3MB\n",
      "\n",
      "Validating runs\\detect\\train8\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.9 torch-1.13.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "YOLOv8n summary (fused): 168 layers, 3006428 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:20<00:00,  5.02s/it]\n",
      "                   all        111        179      0.936      0.872      0.959      0.794\n",
      "           prohibitory        111         71      0.958      0.972      0.989      0.809\n",
      "                danger        111         36      0.928          1      0.995      0.857\n",
      "             mandatory        111         23      0.957      0.739       0.94      0.795\n",
      "                 other        111         49      0.901      0.776      0.912      0.715\n",
      "Speed: 2.5ms preprocess, 140.5ms inference, 0.0ms loss, 10.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model.train(data=\"YOLO_dataset/data.yaml\", epochs=5, imgsz=960, batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.133 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.9 torch-1.13.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=runs/detect/train8/weights/best.pt, data=YOLO_dataset/data.yaml, epochs=2, patience=50, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=True, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=5, translate=0.1, scale=0.5, shear=2.0, perspective=0.0, flipud=0.5, fliplr=0.5, mosaic=1.0, mixup=0.1, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train12\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3011628 parameters, 3011612 gradients\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train12', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\labels\\train.cache... 630 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 630/630 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\images\\train\\00340.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\labels\\val.cache... 111 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train12\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 960 train, 960 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train12\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"runs/detect/train8/weights/best.pt\")\n",
    "model.train(data=\"YOLO_dataset/data.yaml\", epochs=2, imgsz=960,visualize=True , batch=16, degrees=5, translate=0.1, scale=0.5, shear=2.0, flipud=0.5, fliplr=0.5, mosaic=1.0, mixup=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce537a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.135 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.9 torch-1.13.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=runs/detect/train9/weights/best.pt, data=car_yolo_filtered/data.yaml, epochs=2, patience=50, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=True, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train15\n",
      "Overriding model.yaml nc=4 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3011823 parameters, 3011807 gradients\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train15', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\train\\labels... 6081 images, 0 backgrounds, 1 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6082/6082 [00:09<00:00, 635.80it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\train\\images\\outdoor-vertical-traffic-light-green-color-outdoor-vertical-traffic-light-blue-sky-trees-around-traffic-control-concept-221609452_jpg.rf.6a211b5c631bf58021d4596ca8c6a26c.jpg: ignoring corrupt image/label: [Errno 2] No such file or directory: 'C:\\\\osama\\\\CUFE\\\\5-Senior2\\\\GP\\\\ADAS-System-prototype\\\\Traffic_Recognition\\\\car_yolo_filtered\\\\train\\\\images\\\\outdoor-vertical-traffic-light-green-color-outdoor-vertical-traffic-light-blue-sky-trees-around-traffic-control-concept-221609452_jpg.rf.6a211b5c631bf58021d4596ca8c6a26c.jpg'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\test\\labels... 1220 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1220/1220 [00:01<00:00, 691.10it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\test\\labels.cache\n",
      "Plotting labels to runs\\detect\\train15\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 960 train, 960 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train15\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/2         0G       1.42      3.287      1.767         29        960:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 319/381 [1:02:37<12:42, 12.29s/it]"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"runs/detect/train9/weights/best.pt\")\n",
    "model.train(data=\"car_yolo_filtered/data.yaml\", epochs=2, imgsz=960,visualize=True , batch=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3298b0",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\OSAMA\\Desktop\\download_resized.jpg: 736x960 1 prohibitory, 217.4ms\n",
      "Speed: 7.2ms preprocess, 217.4ms inference, 5.6ms postprocess per image at shape (1, 3, 736, 960)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: prohibitory 0.78, BBox: (0, 120, 1147, 849)\n"
     ]
    }
   ],
   "source": [
    "# Class names for your model (match your `data.yaml`)\n",
    "CLASS_NAMES = ['prohibitory', 'danger', 'mandatory', 'other']\n",
    "\n",
    "# Load trained model\n",
    "model = YOLO(\"runs/detect/train13/weights/best.pt\")\n",
    "\n",
    "# Load image\n",
    "img_path = \"Dataset/Train/1/00001_00000_00001.png\"\n",
    "\n",
    "# resize image\n",
    "image = cv2.imread(img_path)\n",
    "image = cv2.resize(image, (1300, 960))\n",
    "# Save resized image\n",
    "resized_img_path = \"C:/Users/OSAMA/Desktop/download_resized.jpg\"\n",
    "cv2.imwrite(resized_img_path, image)\n",
    "# Load resized image\n",
    "img_path = resized_img_path\n",
    "\n",
    "# Run prediction\n",
    "results = model.predict(source=img_path, conf=0.5)\n",
    "\n",
    "# Get first result\n",
    "boxes = results[0].boxes\n",
    "\n",
    "# Draw bounding boxes\n",
    "for box in boxes:\n",
    "    # Get box coordinates (xyxy format: [x1, y1, x2, y2])\n",
    "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "    cls_id = int(box.cls[0])\n",
    "    conf = float(box.conf[0])\n",
    "    label = f\"{CLASS_NAMES[cls_id]} {conf:.2f}\"\n",
    "\n",
    "    print(f\"Class: {label}, BBox: {x1, y1, x2, y2}\")\n",
    "\n",
    "    # Draw box\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow(\"Detected Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f97d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 804, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 804, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.50, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.42, BBox: (1099, 521, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.51, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.43, BBox: (1098, 525, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 394, 803, 474)\n",
      "Class: prohibitory 0.51, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.43, BBox: (1098, 525, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 394, 803, 473)\n",
      "Class: prohibitory 0.51, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.43, BBox: (1098, 525, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 394, 803, 474)\n",
      "Class: prohibitory 0.51, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.43, BBox: (1098, 525, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 474)\n",
      "Class: prohibitory 0.50, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.43, BBox: (1098, 525, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 394, 803, 474)\n",
      "Class: prohibitory 0.51, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 474)\n",
      "Class: prohibitory 0.52, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.44, BBox: (1098, 526, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 474)\n",
      "Class: prohibitory 0.50, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.42, BBox: (1099, 518, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 474)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.43, BBox: (1099, 518, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.43, BBox: (1099, 520, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 802, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.44, BBox: (1099, 520, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 802, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.44, BBox: (1099, 520, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.44, BBox: (1099, 520, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 390, 803, 474)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.44, BBox: (1099, 520, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 803, 474)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.44, BBox: (1099, 520, 1145, 617)\n",
      "Class: prohibitory 0.92, BBox: (749, 391, 803, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.44, BBox: (1099, 520, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 803, 473)\n",
      "Class: prohibitory 0.53, BBox: (1099, 550, 1143, 617)\n",
      "Class: prohibitory 0.44, BBox: (1099, 520, 1145, 617)\n",
      "Class: prohibitory 0.92, BBox: (749, 390, 803, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 473)\n",
      "Class: prohibitory 0.41, BBox: (1098, 520, 1146, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 473)\n",
      "Class: prohibitory 0.42, BBox: (1099, 547, 1144, 617)\n",
      "Class: prohibitory 0.41, BBox: (1098, 520, 1146, 618)\n",
      "Class: prohibitory 0.92, BBox: (749, 392, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 474)\n",
      "Class: prohibitory 0.45, BBox: (1098, 547, 1144, 617)\n",
      "Class: prohibitory 0.41, BBox: (1098, 519, 1146, 618)\n",
      "Class: prohibitory 0.92, BBox: (749, 390, 803, 474)\n",
      "Class: prohibitory 0.44, BBox: (1097, 545, 1145, 617)\n",
      "Class: prohibitory 0.44, BBox: (1097, 520, 1147, 618)\n",
      "Class: prohibitory 0.92, BBox: (749, 391, 803, 474)\n",
      "Class: prohibitory 0.45, BBox: (1097, 545, 1145, 617)\n",
      "Class: prohibitory 0.44, BBox: (1097, 519, 1147, 619)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 474)\n",
      "Class: prohibitory 0.43, BBox: (1097, 519, 1148, 618)\n",
      "Class: prohibitory 0.42, BBox: (1097, 545, 1145, 617)\n",
      "Class: prohibitory 0.92, BBox: (749, 391, 803, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 474)\n",
      "Class: prohibitory 0.42, BBox: (1097, 544, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 473)\n",
      "Class: prohibitory 0.43, BBox: (1097, 543, 1145, 617)\n",
      "Class: prohibitory 0.41, BBox: (1097, 520, 1148, 619)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 473)\n",
      "Class: prohibitory 0.42, BBox: (1097, 543, 1145, 617)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 394, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.92, BBox: (749, 391, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.92, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 394, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 394, 803, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 392, 804, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 392, 804, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 474)\n",
      "Class: prohibitory 0.94, BBox: (749, 392, 804, 474)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 804, 474)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 804, 474)\n",
      "Class: prohibitory 0.94, BBox: (749, 394, 804, 474)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 804, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 804, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 804, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 804, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 803, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 803, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 804, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 803, 474)\n",
      "Class: prohibitory 0.94, BBox: (749, 392, 803, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 803, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 390, 803, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 390, 803, 474)\n",
      "Class: prohibitory 0.94, BBox: (749, 390, 804, 474)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 804, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 391, 804, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 391, 804, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 392, 804, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 804, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 394, 802, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 394, 802, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 802, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 394, 802, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 394, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 393, 802, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 393, 803, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 392, 803, 473)\n",
      "Class: prohibitory 0.94, BBox: (749, 390, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 390, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 390, 803, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 391, 804, 473)\n",
      "Class: prohibitory 0.93, BBox: (749, 392, 803, 474)\n",
      "Class: prohibitory 0.91, BBox: (749, 392, 803, 475)\n",
      "Class: prohibitory 0.91, BBox: (749, 391, 803, 475)\n",
      "Class: prohibitory 0.91, BBox: (749, 393, 803, 475)\n",
      "Class: prohibitory 0.90, BBox: (749, 394, 803, 476)\n",
      "Class: prohibitory 0.88, BBox: (748, 400, 802, 478)\n",
      "Class: prohibitory 0.85, BBox: (749, 402, 803, 479)\n",
      "Class: prohibitory 0.86, BBox: (751, 402, 804, 479)\n",
      "Class: prohibitory 0.88, BBox: (751, 402, 805, 479)\n",
      "Class: prohibitory 0.92, BBox: (751, 403, 805, 480)\n",
      "Class: prohibitory 0.92, BBox: (752, 403, 806, 480)\n",
      "Class: prohibitory 0.95, BBox: (752, 403, 806, 480)\n",
      "Class: prohibitory 0.94, BBox: (753, 403, 809, 480)\n",
      "Class: prohibitory 0.93, BBox: (753, 403, 810, 480)\n",
      "Class: prohibitory 0.91, BBox: (755, 401, 813, 483)\n",
      "Class: prohibitory 0.92, BBox: (756, 401, 814, 483)\n",
      "Class: prohibitory 0.94, BBox: (757, 401, 815, 483)\n",
      "Class: prohibitory 0.94, BBox: (758, 401, 818, 484)\n",
      "Class: prohibitory 0.94, BBox: (759, 399, 820, 486)\n",
      "Class: prohibitory 0.41, BBox: (314, 338, 358, 525)\n",
      "Class: prohibitory 0.93, BBox: (767, 397, 823, 486)\n",
      "Class: prohibitory 0.41, BBox: (312, 338, 357, 525)\n",
      "Class: prohibitory 0.92, BBox: (767, 396, 826, 485)\n",
      "Class: prohibitory 0.92, BBox: (768, 395, 827, 485)\n",
      "Class: prohibitory 0.94, BBox: (769, 394, 830, 483)\n",
      "Class: prohibitory 0.94, BBox: (769, 394, 830, 483)\n",
      "Class: prohibitory 0.95, BBox: (770, 393, 832, 481)\n",
      "Class: prohibitory 0.94, BBox: (772, 386, 837, 481)\n",
      "Class: prohibitory 0.96, BBox: (775, 381, 840, 480)\n",
      "Class: prohibitory 0.97, BBox: (779, 382, 841, 478)\n",
      "Class: prohibitory 0.98, BBox: (779, 381, 844, 477)\n",
      "Class: prohibitory 0.99, BBox: (780, 379, 847, 475)\n",
      "Class: prohibitory 0.99, BBox: (782, 378, 850, 474)\n",
      "Class: prohibitory 0.99, BBox: (785, 377, 851, 472)\n",
      "Class: prohibitory 0.99, BBox: (788, 373, 853, 472)\n",
      "Class: prohibitory 0.99, BBox: (790, 371, 856, 471)\n",
      "Class: prohibitory 0.99, BBox: (790, 371, 857, 471)\n",
      "Class: prohibitory 0.98, BBox: (791, 370, 859, 471)\n",
      "Class: prohibitory 0.97, BBox: (792, 367, 864, 470)\n",
      "Class: prohibitory 0.97, BBox: (793, 367, 867, 470)\n",
      "Class: prohibitory 0.97, BBox: (798, 366, 869, 469)\n",
      "Class: prohibitory 0.98, BBox: (798, 365, 870, 469)\n",
      "Class: prohibitory 0.98, BBox: (799, 365, 870, 469)\n",
      "Class: prohibitory 0.98, BBox: (802, 363, 874, 469)\n",
      "Class: prohibitory 0.98, BBox: (804, 363, 877, 469)\n",
      "Class: prohibitory 0.98, BBox: (804, 363, 877, 469)\n",
      "Class: prohibitory 0.98, BBox: (808, 362, 880, 469)\n",
      "Class: prohibitory 0.97, BBox: (810, 362, 885, 469)\n",
      "Class: prohibitory 0.98, BBox: (811, 362, 887, 470)\n",
      "Class: prohibitory 0.98, BBox: (812, 362, 890, 470)\n",
      "Class: prohibitory 0.98, BBox: (813, 362, 890, 471)\n",
      "Class: prohibitory 0.99, BBox: (817, 361, 897, 472)\n",
      "Class: prohibitory 0.99, BBox: (818, 361, 897, 472)\n",
      "Class: prohibitory 0.99, BBox: (819, 361, 899, 472)\n",
      "Class: prohibitory 0.97, BBox: (827, 361, 903, 474)\n",
      "Class: prohibitory 0.96, BBox: (828, 361, 904, 474)\n",
      "Class: prohibitory 0.97, BBox: (830, 360, 912, 474)\n",
      "Class: prohibitory 0.98, BBox: (833, 360, 919, 477)\n",
      "Class: prohibitory 0.99, BBox: (834, 360, 919, 476)\n",
      "Class: prohibitory 0.98, BBox: (840, 358, 926, 478)\n",
      "Class: prohibitory 0.98, BBox: (849, 357, 935, 478)\n",
      "Class: prohibitory 0.99, BBox: (850, 356, 939, 478)\n",
      "Class: prohibitory 0.98, BBox: (852, 354, 940, 479)\n",
      "Class: prohibitory 0.98, BBox: (860, 351, 952, 479)\n",
      "Class: prohibitory 0.98, BBox: (866, 348, 959, 479)\n",
      "Class: prohibitory 0.97, BBox: (871, 342, 967, 480)\n",
      "Class: prohibitory 0.97, BBox: (877, 341, 978, 481)\n",
      "Class: prohibitory 0.99, BBox: (882, 339, 979, 481)\n",
      "Class: prohibitory 0.97, BBox: (890, 335, 993, 481)\n",
      "Class: prohibitory 0.98, BBox: (909, 325, 1019, 480)\n",
      "Class: prohibitory 0.97, BBox: (919, 319, 1040, 480)\n",
      "Class: prohibitory 0.95, BBox: (932, 316, 1060, 480)\n",
      "Class: prohibitory 0.95, BBox: (941, 310, 1065, 482)\n",
      "Class: prohibitory 0.97, BBox: (951, 303, 1080, 483)\n",
      "Class: prohibitory 0.96, BBox: (982, 290, 1122, 485)\n",
      "Class: prohibitory 0.97, BBox: (989, 283, 1140, 484)\n",
      "Class: prohibitory 0.99, BBox: (1000, 280, 1150, 483)\n",
      "Class: prohibitory 0.99, BBox: (1038, 260, 1200, 479)\n",
      "Class: prohibitory 0.98, BBox: (1056, 249, 1200, 472)\n",
      "Class: prohibitory 0.95, BBox: (1099, 231, 1200, 467)\n",
      "Class: prohibitory 0.70, BBox: (1130, 218, 1200, 470)\n",
      "Class: prohibitory 0.40, BBox: (713, 379, 728, 450)\n",
      "Class: prohibitory 0.44, BBox: (713, 378, 729, 449)\n",
      "Class: prohibitory 0.43, BBox: (714, 376, 732, 442)\n",
      "Class: Traffic light 0.41, BBox: (718, 373, 734, 440)\n",
      "Class: Traffic light 0.47, BBox: (719, 374, 735, 441)\n",
      "Class: prohibitory 0.43, BBox: (724, 364, 744, 438)\n",
      "Class: prohibitory 0.47, BBox: (725, 362, 744, 436)\n",
      "Class: Traffic light 0.44, BBox: (728, 361, 745, 434)\n",
      "Class: prohibitory 0.58, BBox: (732, 357, 749, 430)\n",
      "Class: prohibitory 0.63, BBox: (733, 356, 751, 431)\n",
      "Class: prohibitory 0.74, BBox: (736, 354, 754, 430)\n",
      "Class: Traffic light 0.45, BBox: (735, 354, 753, 428)\n",
      "Class: prohibitory 0.80, BBox: (736, 353, 755, 429)\n",
      "Class: Traffic light 0.43, BBox: (736, 354, 754, 428)\n",
      "Class: prohibitory 0.70, BBox: (739, 353, 759, 430)\n",
      "Class: prohibitory 0.74, BBox: (741, 351, 761, 431)\n",
      "Class: Traffic light 0.43, BBox: (741, 350, 763, 429)\n",
      "Class: prohibitory 0.76, BBox: (741, 350, 762, 431)\n",
      "Class: Traffic light 0.42, BBox: (741, 349, 763, 429)\n",
      "Class: prohibitory 0.70, BBox: (741, 349, 764, 432)\n",
      "Class: Traffic light 0.43, BBox: (741, 349, 765, 431)\n",
      "Class: prohibitory 0.69, BBox: (743, 348, 765, 431)\n",
      "Class: Traffic light 0.47, BBox: (744, 348, 766, 430)\n",
      "Class: prohibitory 0.68, BBox: (745, 347, 765, 431)\n",
      "Class: Traffic light 0.48, BBox: (745, 347, 766, 428)\n",
      "Class: prohibitory 0.63, BBox: (750, 345, 771, 431)\n",
      "Class: prohibitory 0.75, BBox: (750, 344, 773, 432)\n",
      "Class: prohibitory 0.79, BBox: (751, 344, 774, 431)\n",
      "Class: Traffic light 0.45, BBox: (751, 344, 772, 428)\n",
      "Class: prohibitory 0.67, BBox: (752, 343, 774, 431)\n",
      "Class: Traffic light 0.44, BBox: (751, 342, 773, 427)\n",
      "Class: prohibitory 0.75, BBox: (753, 342, 776, 431)\n",
      "Class: Traffic light 0.44, BBox: (753, 341, 776, 427)\n",
      "Class: prohibitory 0.76, BBox: (756, 338, 782, 431)\n",
      "Class: prohibitory 0.70, BBox: (759, 340, 783, 430)\n",
      "Class: prohibitory 0.72, BBox: (761, 339, 783, 430)\n",
      "Class: prohibitory 0.64, BBox: (762, 337, 785, 430)\n",
      "Class: Traffic light 0.46, BBox: (759, 333, 786, 428)\n",
      "Class: prohibitory 0.60, BBox: (763, 336, 785, 429)\n",
      "Class: Traffic light 0.44, BBox: (760, 333, 786, 428)\n",
      "Class: prohibitory 0.70, BBox: (764, 336, 788, 427)\n",
      "Class: Traffic light 0.44, BBox: (762, 333, 787, 426)\n",
      "Class: prohibitory 0.55, BBox: (768, 332, 793, 425)\n",
      "Class: prohibitory 0.48, BBox: (770, 332, 794, 425)\n",
      "Class: Traffic light 0.42, BBox: (770, 330, 792, 423)\n",
      "Class: prohibitory 0.61, BBox: (771, 332, 795, 424)\n",
      "Class: Traffic light 0.48, BBox: (770, 329, 793, 424)\n",
      "Class: prohibitory 0.70, BBox: (771, 331, 795, 423)\n",
      "Class: Traffic light 0.44, BBox: (771, 329, 794, 424)\n",
      "Class: prohibitory 0.68, BBox: (773, 326, 799, 423)\n",
      "Class: Traffic light 0.42, BBox: (773, 326, 798, 422)\n",
      "Class: prohibitory 0.62, BBox: (778, 322, 804, 422)\n",
      "Class: Traffic light 0.48, BBox: (778, 322, 804, 423)\n",
      "Class: prohibitory 0.58, BBox: (779, 323, 804, 423)\n",
      "Class: Traffic light 0.51, BBox: (779, 321, 805, 424)\n",
      "Class: prohibitory 0.59, BBox: (779, 323, 804, 423)\n",
      "Class: Traffic light 0.51, BBox: (779, 321, 805, 423)\n",
      "Class: prohibitory 0.70, BBox: (780, 323, 807, 424)\n",
      "Class: Traffic light 0.59, BBox: (781, 319, 807, 425)\n",
      "Class: prohibitory 0.75, BBox: (782, 323, 813, 423)\n",
      "Class: Traffic light 0.48, BBox: (782, 319, 811, 425)\n",
      "Class: prohibitory 0.52, BBox: (788, 320, 814, 426)\n",
      "Class: prohibitory 0.50, BBox: (789, 320, 815, 427)\n",
      "Class: prohibitory 0.70, BBox: (790, 319, 815, 425)\n",
      "Class: Traffic light 0.43, BBox: (790, 320, 815, 425)\n",
      "Class: prohibitory 0.72, BBox: (793, 317, 824, 423)\n",
      "Class: Traffic light 0.58, BBox: (793, 316, 824, 426)\n",
      "Class: Traffic light 0.65, BBox: (796, 315, 826, 428)\n",
      "Class: prohibitory 0.57, BBox: (797, 317, 825, 426)\n",
      "Class: Traffic light 0.68, BBox: (799, 313, 827, 428)\n",
      "Class: prohibitory 0.61, BBox: (799, 316, 827, 425)\n",
      "Class: prohibitory 0.55, BBox: (808, 314, 834, 426)\n",
      "Class: Traffic light 0.50, BBox: (810, 313, 835, 428)\n",
      "Class: Traffic light 0.61, BBox: (811, 310, 842, 428)\n",
      "Class: prohibitory 0.51, BBox: (810, 312, 844, 426)\n",
      "Class: Traffic light 0.67, BBox: (811, 309, 844, 428)\n",
      "Class: prohibitory 0.43, BBox: (811, 312, 844, 425)\n",
      "Class: Traffic light 0.50, BBox: (819, 305, 852, 430)\n",
      "Class: prohibitory 0.42, BBox: (819, 309, 853, 428)\n",
      "Class: Traffic light 0.63, BBox: (830, 302, 862, 428)\n",
      "Class: Traffic light 0.72, BBox: (830, 300, 865, 428)\n",
      "Class: prohibitory 0.58, BBox: (840, 292, 878, 427)\n",
      "Class: Traffic light 0.71, BBox: (849, 284, 885, 422)\n",
      "Class: prohibitory 0.63, BBox: (854, 279, 892, 420)\n",
      "Class: Traffic light 0.54, BBox: (853, 280, 891, 420)\n",
      "Class: prohibitory 0.64, BBox: (854, 279, 892, 420)\n",
      "Class: Traffic light 0.54, BBox: (853, 280, 891, 420)\n",
      "Class: prohibitory 0.59, BBox: (870, 264, 911, 416)\n",
      "Class: Traffic light 0.56, BBox: (870, 264, 911, 413)\n",
      "Class: prohibitory 0.69, BBox: (883, 256, 926, 414)\n",
      "Class: Traffic light 0.66, BBox: (884, 256, 926, 413)\n",
      "Class: prohibitory 0.68, BBox: (888, 253, 930, 415)\n",
      "Class: Traffic light 0.59, BBox: (888, 255, 930, 412)\n",
      "Class: prohibitory 0.75, BBox: (899, 243, 945, 412)\n",
      "Class: prohibitory 0.85, BBox: (923, 224, 972, 410)\n",
      "Class: prohibitory 0.80, BBox: (929, 221, 977, 410)\n",
      "Class: prohibitory 0.82, BBox: (971, 186, 1026, 399)\n",
      "Class: prohibitory 0.81, BBox: (979, 183, 1036, 397)\n",
      "Class: prohibitory 0.81, BBox: (1027, 143, 1091, 378)\n",
      "Class: prohibitory 0.81, BBox: (1027, 143, 1091, 378)\n",
      "Class: prohibitory 0.69, BBox: (1098, 79, 1176, 359)\n",
      "Class: prohibitory 0.52, BBox: (1112, 67, 1200, 367)\n",
      "Class: prohibitory 0.84, BBox: (13, 437, 54, 497)\n",
      "Class: prohibitory 0.84, BBox: (34, 436, 80, 497)\n",
      "Class: prohibitory 0.58, BBox: (1067, 398, 1097, 434)\n",
      "Class: prohibitory 0.83, BBox: (63, 434, 110, 495)\n",
      "Class: prohibitory 0.80, BBox: (1103, 393, 1138, 429)\n",
      "Class: prohibitory 0.85, BBox: (102, 434, 144, 496)\n",
      "Class: prohibitory 0.80, BBox: (1163, 386, 1198, 426)\n",
      "Class: prohibitory 0.88, BBox: (153, 434, 196, 492)\n",
      "Class: prohibitory 0.93, BBox: (165, 433, 208, 492)\n",
      "Class: prohibitory 0.91, BBox: (190, 432, 233, 491)\n",
      "Class: prohibitory 0.93, BBox: (215, 431, 259, 489)\n",
      "Class: prohibitory 0.86, BBox: (243, 429, 283, 487)\n",
      "Class: prohibitory 0.82, BBox: (270, 424, 310, 486)\n",
      "Class: prohibitory 0.89, BBox: (318, 422, 359, 480)\n",
      "Class: prohibitory 0.84, BBox: (337, 415, 378, 476)\n",
      "Class: prohibitory 0.90, BBox: (359, 412, 398, 471)\n",
      "Class: prohibitory 0.89, BBox: (365, 406, 403, 468)\n",
      "Class: prohibitory 0.91, BBox: (366, 405, 404, 467)\n",
      "Class: prohibitory 0.95, BBox: (364, 403, 403, 466)\n",
      "Class: prohibitory 0.94, BBox: (362, 402, 403, 465)\n",
      "Class: prohibitory 0.95, BBox: (361, 400, 400, 464)\n",
      "Class: prohibitory 0.94, BBox: (359, 399, 400, 464)\n",
      "Class: prohibitory 0.95, BBox: (356, 399, 399, 463)\n",
      "Class: prohibitory 0.91, BBox: (352, 398, 394, 462)\n",
      "Class: prohibitory 0.91, BBox: (350, 396, 393, 464)\n",
      "Class: prohibitory 0.93, BBox: (347, 394, 388, 463)\n",
      "Class: prohibitory 0.92, BBox: (344, 394, 388, 464)\n",
      "Class: prohibitory 0.92, BBox: (344, 394, 389, 464)\n",
      "Class: prohibitory 0.93, BBox: (344, 394, 389, 464)\n",
      "Class: prohibitory 0.95, BBox: (347, 395, 392, 465)\n",
      "Class: prohibitory 0.95, BBox: (350, 394, 395, 466)\n",
      "Class: prohibitory 0.93, BBox: (350, 394, 395, 467)\n",
      "Class: prohibitory 0.95, BBox: (353, 394, 401, 466)\n",
      "Class: prohibitory 0.95, BBox: (354, 394, 402, 466)\n",
      "Class: prohibitory 0.97, BBox: (357, 395, 404, 467)\n",
      "Class: prohibitory 0.94, BBox: (363, 394, 413, 467)\n",
      "Class: prohibitory 0.92, BBox: (369, 394, 417, 468)\n",
      "Class: prohibitory 0.97, BBox: (375, 394, 424, 467)\n",
      "Class: prohibitory 0.92, BBox: (380, 394, 432, 468)\n",
      "Class: prohibitory 0.95, BBox: (384, 394, 434, 469)\n",
      "Class: prohibitory 0.93, BBox: (393, 394, 444, 470)\n",
      "Class: prohibitory 0.93, BBox: (399, 394, 451, 471)\n",
      "Class: prohibitory 0.89, BBox: (410, 393, 458, 472)\n",
      "Class: prohibitory 0.91, BBox: (429, 394, 479, 474)\n",
      "Class: prohibitory 0.92, BBox: (432, 393, 482, 474)\n",
      "Class: prohibitory 0.88, BBox: (448, 399, 499, 479)\n",
      "Class: prohibitory 0.92, BBox: (466, 399, 517, 479)\n",
      "Class: prohibitory 0.90, BBox: (477, 392, 530, 479)\n",
      "Class: prohibitory 0.88, BBox: (492, 391, 544, 479)\n",
      "Class: prohibitory 0.87, BBox: (535, 399, 584, 480)\n",
      "Class: prohibitory 0.76, BBox: (550, 391, 616, 481)\n",
      "Class: prohibitory 0.60, BBox: (550, 398, 599, 481)\n",
      "Class: prohibitory 0.82, BBox: (567, 397, 620, 482)\n",
      "Class: prohibitory 0.89, BBox: (591, 392, 649, 481)\n",
      "Class: prohibitory 0.86, BBox: (638, 392, 694, 480)\n",
      "Class: prohibitory 0.89, BBox: (665, 392, 725, 478)\n",
      "Class: prohibitory 0.89, BBox: (688, 386, 746, 479)\n",
      "Class: prohibitory 0.92, BBox: (719, 384, 776, 480)\n",
      "Class: prohibitory 0.92, BBox: (752, 381, 812, 479)\n",
      "Class: prohibitory 0.97, BBox: (779, 379, 840, 478)\n",
      "Class: prohibitory 0.96, BBox: (831, 373, 895, 477)\n",
      "Class: prohibitory 0.97, BBox: (846, 371, 910, 478)\n",
      "Class: prohibitory 0.97, BBox: (851, 369, 919, 480)\n",
      "Class: prohibitory 0.96, BBox: (871, 367, 940, 483)\n",
      "Class: prohibitory 0.97, BBox: (883, 367, 952, 485)\n",
      "Class: prohibitory 0.99, BBox: (900, 363, 973, 489)\n",
      "Class: prohibitory 0.98, BBox: (910, 361, 988, 489)\n",
      "Class: prohibitory 0.97, BBox: (925, 356, 1005, 489)\n",
      "Class: prohibitory 0.98, BBox: (950, 341, 1038, 490)\n",
      "Class: prohibitory 0.97, BBox: (957, 342, 1044, 490)\n",
      "Class: prohibitory 0.98, BBox: (990, 326, 1081, 490)\n",
      "Class: prohibitory 0.99, BBox: (1003, 322, 1098, 490)\n",
      "Class: prohibitory 0.98, BBox: (1038, 306, 1140, 491)\n",
      "Class: prohibitory 0.94, BBox: (1061, 298, 1166, 490)\n",
      "Class: Traffic light 0.41, BBox: (844, 361, 866, 434)\n",
      "Class: prohibitory 0.42, BBox: (851, 361, 872, 433)\n",
      "Class: prohibitory 0.49, BBox: (861, 354, 884, 430)\n",
      "Class: Traffic light 0.45, BBox: (861, 353, 885, 431)\n",
      "Class: prohibitory 0.55, BBox: (878, 346, 903, 430)\n",
      "Class: Traffic light 0.43, BBox: (878, 347, 903, 428)\n",
      "Class: Traffic light 0.41, BBox: (884, 343, 910, 427)\n",
      "Class: prohibitory 0.40, BBox: (883, 343, 912, 429)\n",
      "Class: Traffic light 0.46, BBox: (901, 335, 928, 424)\n",
      "Class: prohibitory 0.51, BBox: (911, 333, 936, 422)\n",
      "Class: Traffic light 0.43, BBox: (910, 331, 937, 422)\n",
      "Class: Traffic light 0.50, BBox: (921, 329, 948, 423)\n",
      "Class: prohibitory 0.43, BBox: (920, 329, 949, 422)\n",
      "Class: prohibitory 0.70, BBox: (939, 323, 970, 419)\n",
      "Class: Traffic light 0.54, BBox: (940, 320, 969, 421)\n",
      "Class: Traffic light 0.54, BBox: (955, 313, 986, 420)\n",
      "Class: prohibitory 0.54, BBox: (958, 315, 986, 419)\n",
      "Class: prohibitory 0.59, BBox: (959, 314, 992, 419)\n",
      "Class: Traffic light 0.51, BBox: (959, 313, 990, 420)\n",
      "Class: prohibitory 0.44, BBox: (969, 311, 996, 419)\n",
      "Class: Traffic light 0.61, BBox: (974, 308, 1006, 420)\n",
      "Class: prohibitory 0.46, BBox: (974, 310, 1005, 418)\n",
      "Class: Traffic light 0.49, BBox: (987, 303, 1018, 420)\n",
      "Class: prohibitory 0.48, BBox: (985, 305, 1020, 417)\n",
      "Class: Traffic light 0.66, BBox: (989, 298, 1025, 420)\n",
      "Class: prohibitory 0.60, BBox: (991, 292, 1031, 421)\n",
      "Class: Traffic light 0.53, BBox: (992, 291, 1031, 421)\n",
      "Class: prohibitory 0.66, BBox: (992, 278, 1034, 421)\n",
      "Class: prohibitory 0.65, BBox: (993, 274, 1034, 419)\n",
      "Class: prohibitory 0.59, BBox: (999, 260, 1040, 413)\n",
      "Class: prohibitory 0.84, BBox: (1019, 239, 1066, 408)\n",
      "Class: prohibitory 0.81, BBox: (1048, 214, 1099, 400)\n",
      "Class: prohibitory 0.81, BBox: (1071, 198, 1128, 394)\n",
      "Class: prohibitory 0.79, BBox: (1079, 190, 1140, 391)\n",
      "Class: prohibitory 0.83, BBox: (1111, 170, 1172, 387)\n",
      "Class: prohibitory 0.65, BBox: (323, 902, 488, 959)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17512\\2490420866.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Run YOLO prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Draw bounding boxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'set_prompts'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpersist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# merge list of Result into one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[1;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[1;31m# Preprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mprofilers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m                 \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim0s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[1;31m# Inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\predictor.py\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# BGR to RGB, BHWC to BCHW, (n, 3, h, w)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# contiguous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Class names for your model (match your `data.yaml`)\n",
    "CLASS_NAMES = ['prohibitory', 'danger', 'mandatory', 'other', 'Traffic light']\n",
    "\n",
    "# Load trained model\n",
    "model = YOLO(\"runs/detect/train14/weights/best.pt\")\n",
    "\n",
    "# Load video (replace with your file path or 0 for webcam)\n",
    "video_path = \"C:/osama/CUFE/5-Senior2/GP/ADAS-System-prototype/Traffic_Recognition/CarlaVideos/traffic signs test.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Optional: Save the output video\n",
    "out = cv2.VideoWriter(\"C:/Users/OSAMA/Desktop/output_detected.mp4\",\n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame\n",
    "    frame = cv2.resize(frame, (1200, 960))\n",
    "\n",
    "    # Run YOLO prediction\n",
    "    results = model.predict(source=frame, conf=0.4, verbose=False)\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    boxes = results[0].boxes\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        label = f\"{CLASS_NAMES[cls_id]} {conf:.2f}\"\n",
    "\n",
    "        print(f\"Class: {label}, BBox: {x1, y1, x2, y2}\")\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Traffic Sign Detection\", frame)\n",
    "\n",
    "    # Save the frame\n",
    "    out.write(frame)\n",
    "\n",
    "    # Break loop on 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e4069",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9cf8d",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9856c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cropped images saved for SVM training.\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION\n",
    "dataset_root = Path(\"car_yolo_format\")\n",
    "save_root = Path(\"svm_dataset\")\n",
    "save_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Class names\n",
    "class_names = [\n",
    "    'Green Light', 'Red Light', 'Speed Limit 10', 'Speed Limit 100', 'Speed Limit 110',\n",
    "    'Speed Limit 120', 'Speed Limit 20', 'Speed Limit 30', 'Speed Limit 40', 'Speed Limit 50',\n",
    "    'Speed Limit 60', 'Speed Limit 70', 'Speed Limit 80', 'Speed Limit 90', 'Stop'\n",
    "]\n",
    "\n",
    "# Loop over train/val/test\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    image_dir = dataset_root / split / \"images\"\n",
    "    label_dir = dataset_root / split / \"labels\"\n",
    "\n",
    "    for img_file in image_dir.glob(\"*.jpg\"):\n",
    "        image = cv2.imread(str(img_file))\n",
    "        if image is None:\n",
    "            continue\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        label_file = label_dir / (img_file.stem + \".txt\")\n",
    "        if not label_file.exists():\n",
    "            continue\n",
    "\n",
    "        with open(label_file, \"r\") as f:\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "\n",
    "                class_id, x_center, y_center, bw, bh = map(float, parts)\n",
    "                class_id = int(class_id)\n",
    "\n",
    "                # Convert from YOLO normalized format to pixel coordinates\n",
    "                x1 = int((x_center - bw / 2) * w)\n",
    "                y1 = int((y_center - bh / 2) * h)\n",
    "                x2 = int((x_center + bw / 2) * w)\n",
    "                y2 = int((y_center + bh / 2) * h)\n",
    "\n",
    "                x1 = max(0, x1)\n",
    "                y1 = max(0, y1)\n",
    "                x2 = min(w, x2)\n",
    "                y2 = min(h, y2)\n",
    "\n",
    "                crop = image[y1:y2, x1:x2]\n",
    "                if crop.size == 0:\n",
    "                    continue\n",
    "\n",
    "                crop = cv2.resize(crop, (64, 64))\n",
    "                out_dir = save_root / split / str(class_id)\n",
    "                out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                out_path = out_dir / f\"{img_file.stem}_{i}.jpg\"\n",
    "                cv2.imwrite(str(out_path), crop)\n",
    "\n",
    "print(\" preprocessed images saved for SVM training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfac9e7",
   "metadata": {},
   "source": [
    "### Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data_and_extract_features(root_dir):\n",
    "    X, y = [], []\n",
    "    for class_dir in Path(root_dir).glob(\"*\"):\n",
    "        if not class_dir.is_dir():\n",
    "            continue\n",
    "        label = int(class_dir.name)\n",
    "        for img_path in class_dir.glob(\"*.jpg\"):\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            # Resize for consistency\n",
    "            img = cv2.resize(img, (64, 64))\n",
    "\n",
    "            # --- HOG ---\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            hog_feat = hog(gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)\n",
    "\n",
    "            # --- HSV ---\n",
    "            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "            h, s, v = cv2.split(hsv)\n",
    "\n",
    "            # Mean HSV values\n",
    "            mean_h = np.mean(h)\n",
    "            mean_s = np.mean(s)\n",
    "            mean_v = np.mean(v)\n",
    "\n",
    "            # Histogram of hue (8 bins)\n",
    "            hue_hist = cv2.calcHist([h], [0], None, [8], [0, 180])\n",
    "            hue_hist = cv2.normalize(hue_hist, hue_hist).flatten()\n",
    "\n",
    "            # Combine features\n",
    "            full_feature = np.concatenate([hog_feat, [mean_h, mean_s, mean_v], hue_hist])\n",
    "            X.append(full_feature)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "X, y = load_data_and_extract_features(\"svm_dataset/train\")\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "clf = SVC(kernel='rbf', probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb6538",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23f4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.79      0.88      0.83       216\\n           1       0.70      0.69      0.70       128\\n           2       0.75      1.00      0.86         3\\n           3       1.00      0.92      0.96        53\\n           4       0.94      0.79      0.86        19\\n           5       0.85      0.95      0.90        42\\n           6       0.91      0.90      0.91        59\\n           7       0.94      0.92      0.93       111\\n           8       0.97      0.90      0.93        63\\n           9       0.91      0.89      0.90        44\\n          10       0.97      0.90      0.93       101\\n          11       0.89      0.89      0.89        71\\n          12       0.89      0.83      0.86        69\\n          13       0.85      0.87      0.86        79\\n          14       1.00      1.00      1.00        77\\n\\n    accuracy                           0.87      1135\\n   macro avg       0.89      0.89      0.89      1135\\nweighted avg       0.88      0.87      0.87      1135\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load crops from svm_dataset/train (cropped by class ID)\n",
    "def load_data(root_dir):\n",
    "    X, y = [], []\n",
    "    for class_dir in Path(root_dir).glob(\"*\"):\n",
    "        if not class_dir.is_dir():\n",
    "            continue\n",
    "        label = int(class_dir.name)\n",
    "        for img_path in class_dir.glob(\"*.jpg\"):\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                continue\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            # Resize to 64x64\n",
    "            gray = cv2.resize(gray, (64, 64))\n",
    "            # Extract HOG features\n",
    "            features = hog(gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load training data\n",
    "X, y = load_data(\"svm_dataset/train\")\n",
    "\n",
    "# Split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "clf = SVC(kernel='linear', probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Validate\n",
    "y_pred = clf.predict(X_val)\n",
    "report = classification_report(y_val, y_pred, output_dict=False)\n",
    "\n",
    "report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b0c34",
   "metadata": {},
   "source": [
    "### save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901610e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, \"svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcd78d",
   "metadata": {},
   "source": [
    "### Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1821f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 544x960 1 prohibitory, 152.6ms\n",
      "Speed: 2.4ms preprocess, 152.6ms inference, 5.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 146.4ms\n",
      "Speed: 9.1ms preprocess, 146.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 141.7ms\n",
      "Speed: 4.0ms preprocess, 141.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 138.3ms\n",
      "Speed: 5.1ms preprocess, 138.3ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 142.2ms\n",
      "Speed: 7.0ms preprocess, 142.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 145.3ms\n",
      "Speed: 6.7ms preprocess, 145.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 144.8ms\n",
      "Speed: 4.8ms preprocess, 144.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 142.3ms\n",
      "Speed: 3.6ms preprocess, 142.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 155.9ms\n",
      "Speed: 7.0ms preprocess, 155.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 143.8ms\n",
      "Speed: 6.0ms preprocess, 143.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 153.4ms\n",
      "Speed: 6.6ms preprocess, 153.4ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 139.6ms\n",
      "Speed: 6.8ms preprocess, 139.6ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.5ms\n",
      "Speed: 4.6ms preprocess, 101.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 111.0ms\n",
      "Speed: 10.9ms preprocess, 111.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 111.5ms\n",
      "Speed: 4.0ms preprocess, 111.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 100.9ms\n",
      "Speed: 4.3ms preprocess, 100.9ms inference, 6.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.7ms\n",
      "Speed: 3.3ms preprocess, 108.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.4ms\n",
      "Speed: 3.0ms preprocess, 102.4ms inference, 6.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.5ms\n",
      "Speed: 7.1ms preprocess, 105.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.4ms\n",
      "Speed: 3.3ms preprocess, 108.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.9ms\n",
      "Speed: 3.9ms preprocess, 101.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.8ms\n",
      "Speed: 4.0ms preprocess, 107.8ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.5ms\n",
      "Speed: 3.1ms preprocess, 108.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.5ms\n",
      "Speed: 3.4ms preprocess, 107.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.9ms\n",
      "Speed: 4.0ms preprocess, 103.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.8ms\n",
      "Speed: 4.6ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 112.9ms\n",
      "Speed: 4.2ms preprocess, 112.9ms inference, 5.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 114.3ms\n",
      "Speed: 3.9ms preprocess, 114.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 119.3ms\n",
      "Speed: 3.9ms preprocess, 119.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 117.6ms\n",
      "Speed: 4.3ms preprocess, 117.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 108.1ms\n",
      "Speed: 3.5ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 103.4ms\n",
      "Speed: 3.9ms preprocess, 103.4ms inference, 6.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 108.0ms\n",
      "Speed: 3.5ms preprocess, 108.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 108.2ms\n",
      "Speed: 3.9ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 107.6ms\n",
      "Speed: 3.2ms preprocess, 107.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 109.3ms\n",
      "Speed: 4.9ms preprocess, 109.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 107.6ms\n",
      "Speed: 4.4ms preprocess, 107.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 106.6ms\n",
      "Speed: 3.4ms preprocess, 106.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 107.4ms\n",
      "Speed: 3.7ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 105.9ms\n",
      "Speed: 4.3ms preprocess, 105.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 111.3ms\n",
      "Speed: 6.6ms preprocess, 111.3ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 119.4ms\n",
      "Speed: 3.9ms preprocess, 119.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 109.9ms\n",
      "Speed: 4.0ms preprocess, 109.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 112.1ms\n",
      "Speed: 6.2ms preprocess, 112.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 112.9ms\n",
      "Speed: 4.5ms preprocess, 112.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 106.8ms\n",
      "Speed: 5.1ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 108.1ms\n",
      "Speed: 3.9ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 108.3ms\n",
      "Speed: 4.0ms preprocess, 108.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.8ms\n",
      "Speed: 4.0ms preprocess, 103.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 108.9ms\n",
      "Speed: 3.5ms preprocess, 108.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 106.6ms\n",
      "Speed: 4.8ms preprocess, 106.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 125.4ms\n",
      "Speed: 4.1ms preprocess, 125.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 109.2ms\n",
      "Speed: 4.9ms preprocess, 109.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 107.4ms\n",
      "Speed: 3.5ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 106.2ms\n",
      "Speed: 5.2ms preprocess, 106.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.6ms\n",
      "Speed: 4.0ms preprocess, 105.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.8ms\n",
      "Speed: 3.2ms preprocess, 109.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 112.9ms\n",
      "Speed: 4.6ms preprocess, 112.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.0ms\n",
      "Speed: 3.1ms preprocess, 108.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.9ms\n",
      "Speed: 4.5ms preprocess, 106.9ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.9ms\n",
      "Speed: 3.9ms preprocess, 101.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.3ms\n",
      "Speed: 4.9ms preprocess, 107.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.1ms\n",
      "Speed: 3.4ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 113.0ms\n",
      "Speed: 3.9ms preprocess, 113.0ms inference, 0.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.0ms\n",
      "Speed: 7.2ms preprocess, 104.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.8ms\n",
      "Speed: 2.4ms preprocess, 108.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.2ms\n",
      "Speed: 4.8ms preprocess, 107.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.1ms\n",
      "Speed: 4.6ms preprocess, 106.1ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.3ms\n",
      "Speed: 4.1ms preprocess, 105.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.3ms\n",
      "Speed: 4.1ms preprocess, 101.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.5ms\n",
      "Speed: 4.0ms preprocess, 109.5ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.1ms\n",
      "Speed: 4.0ms preprocess, 102.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 110.0ms\n",
      "Speed: 2.0ms preprocess, 110.0ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 95.7ms\n",
      "Speed: 4.2ms preprocess, 95.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.6ms\n",
      "Speed: 4.5ms preprocess, 104.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.9ms\n",
      "Speed: 4.5ms preprocess, 101.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.4ms\n",
      "Speed: 4.3ms preprocess, 104.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.1ms\n",
      "Speed: 3.8ms preprocess, 108.1ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.3ms\n",
      "Speed: 3.6ms preprocess, 102.3ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.7ms\n",
      "Speed: 3.1ms preprocess, 102.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.3ms\n",
      "Speed: 4.6ms preprocess, 105.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.8ms\n",
      "Speed: 4.0ms preprocess, 107.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.2ms\n",
      "Speed: 3.8ms preprocess, 106.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 112.2ms\n",
      "Speed: 4.6ms preprocess, 112.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.1ms\n",
      "Speed: 4.9ms preprocess, 107.1ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.2ms\n",
      "Speed: 3.4ms preprocess, 104.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.9ms\n",
      "Speed: 4.6ms preprocess, 106.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.8ms\n",
      "Speed: 6.9ms preprocess, 105.8ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 120.2ms\n",
      "Speed: 3.7ms preprocess, 120.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.9ms\n",
      "Speed: 4.3ms preprocess, 108.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.8ms\n",
      "Speed: 4.0ms preprocess, 108.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.1ms\n",
      "Speed: 3.5ms preprocess, 108.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 113.7ms\n",
      "Speed: 3.1ms preprocess, 113.7ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.4ms\n",
      "Speed: 2.3ms preprocess, 108.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.1ms\n",
      "Speed: 2.7ms preprocess, 109.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.2ms\n",
      "Speed: 5.2ms preprocess, 108.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.1ms\n",
      "Speed: 3.9ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.0ms\n",
      "Speed: 3.4ms preprocess, 107.0ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.1ms\n",
      "Speed: 3.4ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.8ms\n",
      "Speed: 4.3ms preprocess, 103.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.6ms\n",
      "Speed: 4.1ms preprocess, 105.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.1ms\n",
      "Speed: 4.5ms preprocess, 106.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.2ms\n",
      "Speed: 3.8ms preprocess, 104.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.8ms\n",
      "Speed: 4.2ms preprocess, 106.8ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 111.9ms\n",
      "Speed: 5.4ms preprocess, 111.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.3ms\n",
      "Speed: 3.2ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.8ms\n",
      "Speed: 6.9ms preprocess, 107.8ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 113.3ms\n",
      "Speed: 0.8ms preprocess, 113.3ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.4ms\n",
      "Speed: 3.0ms preprocess, 103.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 110.8ms\n",
      "Speed: 3.9ms preprocess, 110.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.3ms\n",
      "Speed: 3.1ms preprocess, 103.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 95.0ms\n",
      "Speed: 4.9ms preprocess, 95.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.2ms\n",
      "Speed: 3.5ms preprocess, 102.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.8ms\n",
      "Speed: 3.9ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.7ms\n",
      "Speed: 3.6ms preprocess, 107.7ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.1ms\n",
      "Speed: 3.5ms preprocess, 107.1ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.3ms\n",
      "Speed: 2.9ms preprocess, 109.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 112.7ms\n",
      "Speed: 4.7ms preprocess, 112.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 94.7ms\n",
      "Speed: 4.3ms preprocess, 94.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.9ms\n",
      "Speed: 4.3ms preprocess, 105.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.2ms\n",
      "Speed: 3.9ms preprocess, 109.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.8ms\n",
      "Speed: 4.0ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.0ms\n",
      "Speed: 3.9ms preprocess, 107.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.8ms\n",
      "Speed: 3.9ms preprocess, 107.8ms inference, 0.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.8ms\n",
      "Speed: 4.0ms preprocess, 104.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.1ms\n",
      "Speed: 4.7ms preprocess, 106.1ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.8ms\n",
      "Speed: 2.9ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.0ms\n",
      "Speed: 4.7ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 110.2ms\n",
      "Speed: 3.7ms preprocess, 110.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.9ms\n",
      "Speed: 4.2ms preprocess, 101.9ms inference, 5.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 111.9ms\n",
      "Speed: 6.3ms preprocess, 111.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.2ms\n",
      "Speed: 4.9ms preprocess, 109.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 120.1ms\n",
      "Speed: 3.9ms preprocess, 120.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 110.8ms\n",
      "Speed: 4.1ms preprocess, 110.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 108.1ms\n",
      "Speed: 4.7ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 106.4ms\n",
      "Speed: 4.4ms preprocess, 106.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 100.8ms\n",
      "Speed: 4.0ms preprocess, 100.8ms inference, 5.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 105.0ms\n",
      "Speed: 5.2ms preprocess, 105.0ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 105.0ms\n",
      "Speed: 3.9ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 108.9ms\n",
      "Speed: 3.0ms preprocess, 108.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 108.3ms\n",
      "Speed: 6.4ms preprocess, 108.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 110.1ms\n",
      "Speed: 2.6ms preprocess, 110.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.6ms\n",
      "Speed: 3.9ms preprocess, 103.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.3ms\n",
      "Speed: 3.8ms preprocess, 107.3ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.1ms\n",
      "Speed: 3.5ms preprocess, 108.1ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.5ms\n",
      "Speed: 3.4ms preprocess, 107.5ms inference, 0.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.0ms\n",
      "Speed: 3.2ms preprocess, 108.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.5ms\n",
      "Speed: 4.7ms preprocess, 108.5ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.4ms\n",
      "Speed: 4.5ms preprocess, 107.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.0ms\n",
      "Speed: 3.7ms preprocess, 106.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.1ms\n",
      "Speed: 3.0ms preprocess, 109.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.3ms\n",
      "Speed: 3.2ms preprocess, 103.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 110.7ms\n",
      "Speed: 4.8ms preprocess, 110.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 100.8ms\n",
      "Speed: 4.6ms preprocess, 100.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 110.9ms\n",
      "Speed: 5.0ms preprocess, 110.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.0ms\n",
      "Speed: 3.6ms preprocess, 107.0ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.1ms\n",
      "Speed: 2.2ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.2ms\n",
      "Speed: 4.7ms preprocess, 107.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 110.0ms\n",
      "Speed: 4.5ms preprocess, 110.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.8ms\n",
      "Speed: 3.4ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.4ms\n",
      "Speed: 3.4ms preprocess, 101.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 100.6ms\n",
      "Speed: 2.0ms preprocess, 100.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.7ms\n",
      "Speed: 4.2ms preprocess, 106.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.5ms\n",
      "Speed: 3.9ms preprocess, 105.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.6ms\n",
      "Speed: 4.3ms preprocess, 107.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.1ms\n",
      "Speed: 3.9ms preprocess, 108.1ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.7ms\n",
      "Speed: 4.6ms preprocess, 101.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.8ms\n",
      "Speed: 4.6ms preprocess, 105.8ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.7ms\n",
      "Speed: 3.9ms preprocess, 101.7ms inference, 3.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.9ms\n",
      "Speed: 3.4ms preprocess, 104.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.9ms\n",
      "Speed: 4.1ms preprocess, 106.9ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.1ms\n",
      "Speed: 2.9ms preprocess, 102.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.5ms\n",
      "Speed: 4.7ms preprocess, 105.5ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 131.9ms\n",
      "Speed: 5.4ms preprocess, 131.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.0ms\n",
      "Speed: 6.9ms preprocess, 103.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.1ms\n",
      "Speed: 2.7ms preprocess, 105.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 105.7ms\n",
      "Speed: 3.5ms preprocess, 105.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.2ms\n",
      "Speed: 2.0ms preprocess, 102.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.1ms\n",
      "Speed: 3.8ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.9ms\n",
      "Speed: 4.3ms preprocess, 105.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 116.5ms\n",
      "Speed: 0.0ms preprocess, 116.5ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.2ms\n",
      "Speed: 3.1ms preprocess, 107.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.2ms\n",
      "Speed: 6.6ms preprocess, 106.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.6ms\n",
      "Speed: 5.0ms preprocess, 106.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.1ms\n",
      "Speed: 3.2ms preprocess, 106.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.1ms\n",
      "Speed: 4.7ms preprocess, 106.1ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.2ms\n",
      "Speed: 3.6ms preprocess, 102.2ms inference, 5.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 96.9ms\n",
      "Speed: 3.8ms preprocess, 96.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 98.2ms\n",
      "Speed: 4.2ms preprocess, 98.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.5ms\n",
      "Speed: 3.6ms preprocess, 110.5ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.7ms\n",
      "Speed: 3.9ms preprocess, 101.7ms inference, 6.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.9ms\n",
      "Speed: 3.7ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.9ms\n",
      "Speed: 3.3ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.3ms\n",
      "Speed: 3.2ms preprocess, 106.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.1ms\n",
      "Speed: 3.7ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.3ms\n",
      "Speed: 4.5ms preprocess, 101.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 100.9ms\n",
      "Speed: 3.7ms preprocess, 100.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.8ms\n",
      "Speed: 3.4ms preprocess, 109.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.4ms\n",
      "Speed: 3.9ms preprocess, 106.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.7ms\n",
      "Speed: 4.7ms preprocess, 106.7ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.7ms\n",
      "Speed: 7.8ms preprocess, 104.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.4ms\n",
      "Speed: 6.9ms preprocess, 106.4ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.1ms\n",
      "Speed: 7.1ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.5ms\n",
      "Speed: 7.5ms preprocess, 101.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.1ms\n",
      "Speed: 3.5ms preprocess, 103.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.9ms\n",
      "Speed: 2.8ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.1ms\n",
      "Speed: 3.7ms preprocess, 105.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.8ms\n",
      "Speed: 3.1ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.1ms\n",
      "Speed: 6.6ms preprocess, 106.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.8ms\n",
      "Speed: 4.3ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.6ms\n",
      "Speed: 3.8ms preprocess, 101.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.8ms\n",
      "Speed: 4.0ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.2ms\n",
      "Speed: 6.6ms preprocess, 102.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 95.3ms\n",
      "Speed: 4.1ms preprocess, 95.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.1ms\n",
      "Speed: 3.2ms preprocess, 107.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.3ms\n",
      "Speed: 7.0ms preprocess, 102.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.1ms\n",
      "Speed: 4.1ms preprocess, 101.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.1ms\n",
      "Speed: 4.5ms preprocess, 102.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.9ms\n",
      "Speed: 4.7ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.8ms\n",
      "Speed: 3.5ms preprocess, 102.8ms inference, 6.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 96.8ms\n",
      "Speed: 2.7ms preprocess, 96.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.8ms\n",
      "Speed: 3.7ms preprocess, 110.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.7ms\n",
      "Speed: 3.4ms preprocess, 101.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.1ms\n",
      "Speed: 3.5ms preprocess, 102.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 112.0ms\n",
      "Speed: 3.8ms preprocess, 112.0ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.3ms\n",
      "Speed: 4.3ms preprocess, 105.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.8ms\n",
      "Speed: 3.9ms preprocess, 104.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 104.7ms\n",
      "Speed: 4.1ms preprocess, 104.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 103.4ms\n",
      "Speed: 4.1ms preprocess, 103.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.6ms\n",
      "Speed: 3.7ms preprocess, 105.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 96.3ms\n",
      "Speed: 3.3ms preprocess, 96.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.7ms\n",
      "Speed: 4.1ms preprocess, 106.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 112.4ms\n",
      "Speed: 0.0ms preprocess, 112.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.0ms\n",
      "Speed: 3.9ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.3ms\n",
      "Speed: 4.3ms preprocess, 109.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.6ms\n",
      "Speed: 4.4ms preprocess, 106.6ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.5ms\n",
      "Speed: 3.2ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.1ms\n",
      "Speed: 7.8ms preprocess, 103.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 115.9ms\n",
      "Speed: 4.2ms preprocess, 115.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.3ms\n",
      "Speed: 3.3ms preprocess, 101.3ms inference, 6.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.6ms\n",
      "Speed: 5.8ms preprocess, 107.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.9ms\n",
      "Speed: 3.8ms preprocess, 106.9ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.4ms\n",
      "Speed: 3.9ms preprocess, 109.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.5ms\n",
      "Speed: 3.5ms preprocess, 106.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.4ms\n",
      "Speed: 3.8ms preprocess, 106.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 105.0ms\n",
      "Speed: 4.7ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 95.9ms\n",
      "Speed: 3.8ms preprocess, 95.9ms inference, 12.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.0ms\n",
      "Speed: 3.4ms preprocess, 106.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.9ms\n",
      "Speed: 3.1ms preprocess, 106.9ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.5ms\n",
      "Speed: 4.2ms preprocess, 101.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 102.8ms\n",
      "Speed: 3.8ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 108.0ms\n",
      "Speed: 4.0ms preprocess, 108.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 108.8ms\n",
      "Speed: 2.7ms preprocess, 108.8ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 110.0ms\n",
      "Speed: 2.9ms preprocess, 110.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.0ms\n",
      "Speed: 4.9ms preprocess, 107.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 104.7ms\n",
      "Speed: 3.8ms preprocess, 104.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.6ms\n",
      "Speed: 4.0ms preprocess, 107.6ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 104.6ms\n",
      "Speed: 4.5ms preprocess, 104.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.0ms\n",
      "Speed: 4.9ms preprocess, 107.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 109.8ms\n",
      "Speed: 7.4ms preprocess, 109.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.9ms\n",
      "Speed: 3.9ms preprocess, 101.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.8ms\n",
      "Speed: 3.1ms preprocess, 101.8ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.2ms\n",
      "Speed: 3.9ms preprocess, 103.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.9ms\n",
      "Speed: 4.0ms preprocess, 101.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.4ms\n",
      "Speed: 4.3ms preprocess, 104.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.2ms\n",
      "Speed: 5.7ms preprocess, 102.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 100.2ms\n",
      "Speed: 0.0ms preprocess, 100.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 102.9ms\n",
      "Speed: 3.2ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.6ms\n",
      "Speed: 3.3ms preprocess, 105.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.2ms\n",
      "Speed: 3.8ms preprocess, 104.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.4ms\n",
      "Speed: 4.1ms preprocess, 101.4ms inference, 6.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.8ms\n",
      "Speed: 6.2ms preprocess, 104.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.1ms\n",
      "Speed: 4.2ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.0ms\n",
      "Speed: 3.1ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.6ms\n",
      "Speed: 7.6ms preprocess, 105.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.2ms\n",
      "Speed: 4.0ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.3ms\n",
      "Speed: 4.9ms preprocess, 108.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.2ms\n",
      "Speed: 4.3ms preprocess, 101.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.7ms\n",
      "Speed: 3.6ms preprocess, 108.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.9ms\n",
      "Speed: 3.7ms preprocess, 106.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.8ms\n",
      "Speed: 7.6ms preprocess, 103.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 100.2ms\n",
      "Speed: 0.5ms preprocess, 100.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.8ms\n",
      "Speed: 4.0ms preprocess, 107.8ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.2ms\n",
      "Speed: 6.6ms preprocess, 103.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.2ms\n",
      "Speed: 2.6ms preprocess, 106.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.0ms\n",
      "Speed: 6.7ms preprocess, 106.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.8ms\n",
      "Speed: 6.6ms preprocess, 110.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.6ms\n",
      "Speed: 3.2ms preprocess, 107.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.6ms\n",
      "Speed: 7.2ms preprocess, 101.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.3ms\n",
      "Speed: 3.8ms preprocess, 106.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.0ms\n",
      "Speed: 7.3ms preprocess, 107.0ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.8ms\n",
      "Speed: 7.6ms preprocess, 106.8ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.8ms\n",
      "Speed: 3.6ms preprocess, 107.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.8ms\n",
      "Speed: 7.9ms preprocess, 107.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 113.0ms\n",
      "Speed: 4.1ms preprocess, 113.0ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.4ms\n",
      "Speed: 3.4ms preprocess, 104.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.7ms\n",
      "Speed: 7.6ms preprocess, 108.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.5ms\n",
      "Speed: 4.1ms preprocess, 101.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 111.7ms\n",
      "Speed: 3.8ms preprocess, 111.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.0ms\n",
      "Speed: 6.1ms preprocess, 104.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.1ms\n",
      "Speed: 6.6ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.9ms\n",
      "Speed: 7.2ms preprocess, 105.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.6ms\n",
      "Speed: 4.2ms preprocess, 101.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.6ms\n",
      "Speed: 4.2ms preprocess, 103.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 105.1ms\n",
      "Speed: 4.5ms preprocess, 105.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.1ms\n",
      "Speed: 3.3ms preprocess, 106.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.8ms\n",
      "Speed: 4.2ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.8ms\n",
      "Speed: 4.0ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.9ms\n",
      "Speed: 3.0ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.6ms\n",
      "Speed: 5.8ms preprocess, 102.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.7ms\n",
      "Speed: 3.9ms preprocess, 103.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.5ms\n",
      "Speed: 7.2ms preprocess, 106.5ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 115.3ms\n",
      "Speed: 0.0ms preprocess, 115.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.7ms\n",
      "Speed: 4.4ms preprocess, 102.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 126.1ms\n",
      "Speed: 3.5ms preprocess, 126.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.8ms\n",
      "Speed: 3.8ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.2ms\n",
      "Speed: 0.0ms preprocess, 109.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 114.7ms\n",
      "Speed: 6.5ms preprocess, 114.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.5ms\n",
      "Speed: 4.1ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.4ms\n",
      "Speed: 3.4ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 103.9ms\n",
      "Speed: 4.3ms preprocess, 103.9ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.0ms\n",
      "Speed: 6.4ms preprocess, 106.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.2ms\n",
      "Speed: 7.3ms preprocess, 105.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 109.2ms\n",
      "Speed: 4.2ms preprocess, 109.2ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 108.0ms\n",
      "Speed: 3.7ms preprocess, 108.0ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.7ms\n",
      "Speed: 8.1ms preprocess, 106.7ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 100.6ms\n",
      "Speed: 5.0ms preprocess, 100.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.7ms\n",
      "Speed: 3.7ms preprocess, 101.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 113.7ms\n",
      "Speed: 4.3ms preprocess, 113.7ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.9ms\n",
      "Speed: 6.4ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.7ms\n",
      "Speed: 3.8ms preprocess, 101.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.2ms\n",
      "Speed: 4.4ms preprocess, 101.2ms inference, 6.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.6ms\n",
      "Speed: 3.9ms preprocess, 106.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.1ms\n",
      "Speed: 7.6ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.9ms\n",
      "Speed: 3.3ms preprocess, 108.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.9ms\n",
      "Speed: 4.7ms preprocess, 102.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 106.8ms\n",
      "Speed: 4.3ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 112.6ms\n",
      "Speed: 3.4ms preprocess, 112.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.2ms\n",
      "Speed: 3.9ms preprocess, 107.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.1ms\n",
      "Speed: 3.4ms preprocess, 105.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.4ms\n",
      "Speed: 4.3ms preprocess, 101.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.4ms\n",
      "Speed: 3.3ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.9ms\n",
      "Speed: 3.6ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.0ms\n",
      "Speed: 4.5ms preprocess, 109.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.8ms\n",
      "Speed: 6.2ms preprocess, 105.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.4ms\n",
      "Speed: 7.0ms preprocess, 110.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.8ms\n",
      "Speed: 3.8ms preprocess, 108.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.6ms\n",
      "Speed: 4.4ms preprocess, 107.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.2ms\n",
      "Speed: 4.1ms preprocess, 105.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 112.2ms\n",
      "Speed: 0.0ms preprocess, 112.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.1ms\n",
      "Speed: 5.2ms preprocess, 106.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 117.5ms\n",
      "Speed: 5.0ms preprocess, 117.5ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.3ms\n",
      "Speed: 4.1ms preprocess, 106.3ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.7ms\n",
      "Speed: 4.3ms preprocess, 104.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.6ms\n",
      "Speed: 5.1ms preprocess, 106.6ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.3ms\n",
      "Speed: 2.8ms preprocess, 108.3ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.5ms\n",
      "Speed: 4.3ms preprocess, 101.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.6ms\n",
      "Speed: 3.9ms preprocess, 101.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.7ms\n",
      "Speed: 4.0ms preprocess, 104.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.2ms\n",
      "Speed: 3.0ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.6ms\n",
      "Speed: 4.2ms preprocess, 103.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.9ms\n",
      "Speed: 8.5ms preprocess, 105.9ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.9ms\n",
      "Speed: 3.4ms preprocess, 105.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.9ms\n",
      "Speed: 4.0ms preprocess, 108.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.0ms\n",
      "Speed: 6.6ms preprocess, 110.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 113.2ms\n",
      "Speed: 0.0ms preprocess, 113.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.0ms\n",
      "Speed: 7.6ms preprocess, 103.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.0ms\n",
      "Speed: 7.4ms preprocess, 107.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.7ms\n",
      "Speed: 7.8ms preprocess, 101.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 113.0ms\n",
      "Speed: 0.0ms preprocess, 113.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.9ms\n",
      "Speed: 3.6ms preprocess, 101.9ms inference, 6.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.0ms\n",
      "Speed: 3.8ms preprocess, 105.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.2ms\n",
      "Speed: 3.0ms preprocess, 103.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.5ms\n",
      "Speed: 5.8ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 99.6ms\n",
      "Speed: 3.2ms preprocess, 99.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.0ms\n",
      "Speed: 2.9ms preprocess, 109.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.1ms\n",
      "Speed: 2.6ms preprocess, 106.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.4ms\n",
      "Speed: 5.4ms preprocess, 103.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.8ms\n",
      "Speed: 2.7ms preprocess, 102.8ms inference, 6.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.6ms\n",
      "Speed: 2.7ms preprocess, 110.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.7ms\n",
      "Speed: 6.8ms preprocess, 104.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.8ms\n",
      "Speed: 5.8ms preprocess, 101.8ms inference, 5.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.2ms\n",
      "Speed: 6.6ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.8ms\n",
      "Speed: 4.1ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.8ms\n",
      "Speed: 5.2ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.2ms\n",
      "Speed: 3.0ms preprocess, 107.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 102.6ms\n",
      "Speed: 7.0ms preprocess, 102.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 94.9ms\n",
      "Speed: 4.4ms preprocess, 94.9ms inference, 12.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.4ms\n",
      "Speed: 2.5ms preprocess, 106.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.6ms\n",
      "Speed: 3.5ms preprocess, 106.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.2ms\n",
      "Speed: 4.0ms preprocess, 103.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.9ms\n",
      "Speed: 3.3ms preprocess, 106.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.0ms\n",
      "Speed: 3.9ms preprocess, 107.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.4ms\n",
      "Speed: 3.3ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.7ms\n",
      "Speed: 4.3ms preprocess, 102.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.3ms\n",
      "Speed: 2.8ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.0ms\n",
      "Speed: 3.9ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.6ms\n",
      "Speed: 4.1ms preprocess, 106.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.4ms\n",
      "Speed: 4.2ms preprocess, 101.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.8ms\n",
      "Speed: 4.1ms preprocess, 108.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.9ms\n",
      "Speed: 4.9ms preprocess, 106.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 111.3ms\n",
      "Speed: 0.0ms preprocess, 111.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 106.2ms\n",
      "Speed: 5.0ms preprocess, 106.2ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 104.9ms\n",
      "Speed: 7.5ms preprocess, 104.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 103.3ms\n",
      "Speed: 6.3ms preprocess, 103.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.9ms\n",
      "Speed: 8.0ms preprocess, 101.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.8ms\n",
      "Speed: 4.4ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.9ms\n",
      "Speed: 3.9ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 112.9ms\n",
      "Speed: 0.0ms preprocess, 112.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.1ms\n",
      "Speed: 3.7ms preprocess, 106.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 103.4ms\n",
      "Speed: 4.3ms preprocess, 103.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.9ms\n",
      "Speed: 3.4ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 108.8ms\n",
      "Speed: 3.2ms preprocess, 108.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 106.3ms\n",
      "Speed: 5.7ms preprocess, 106.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 102.4ms\n",
      "Speed: 3.0ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 105.9ms\n",
      "Speed: 3.7ms preprocess, 105.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 109.1ms\n",
      "Speed: 3.4ms preprocess, 109.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 116.4ms\n",
      "Speed: 0.0ms preprocess, 116.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.3ms\n",
      "Speed: 7.5ms preprocess, 105.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.7ms\n",
      "Speed: 4.3ms preprocess, 103.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.1ms\n",
      "Speed: 4.3ms preprocess, 103.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.1ms\n",
      "Speed: 4.8ms preprocess, 108.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.1ms\n",
      "Speed: 4.3ms preprocess, 103.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.4ms\n",
      "Speed: 4.5ms preprocess, 105.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.8ms\n",
      "Speed: 4.7ms preprocess, 107.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.7ms\n",
      "Speed: 4.0ms preprocess, 101.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.1ms\n",
      "Speed: 3.2ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.7ms\n",
      "Speed: 4.4ms preprocess, 103.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.6ms\n",
      "Speed: 6.7ms preprocess, 103.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.3ms\n",
      "Speed: 3.4ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.8ms\n",
      "Speed: 3.9ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.9ms\n",
      "Speed: 2.8ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.5ms\n",
      "Speed: 3.8ms preprocess, 109.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 116.7ms\n",
      "Speed: 6.9ms preprocess, 116.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 96.6ms\n",
      "Speed: 3.4ms preprocess, 96.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.2ms\n",
      "Speed: 4.6ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.1ms\n",
      "Speed: 7.6ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.0ms\n",
      "Speed: 4.9ms preprocess, 106.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 113.4ms\n",
      "Speed: 0.0ms preprocess, 113.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.2ms\n",
      "Speed: 8.0ms preprocess, 104.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.9ms\n",
      "Speed: 4.3ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 116.3ms\n",
      "Speed: 3.9ms preprocess, 116.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 125.2ms\n",
      "Speed: 6.5ms preprocess, 125.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 113.3ms\n",
      "Speed: 7.8ms preprocess, 113.3ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.2ms\n",
      "Speed: 6.1ms preprocess, 108.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.0ms\n",
      "Speed: 8.2ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.0ms\n",
      "Speed: 7.6ms preprocess, 103.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.6ms\n",
      "Speed: 7.2ms preprocess, 101.6ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.7ms\n",
      "Speed: 3.4ms preprocess, 108.7ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.1ms\n",
      "Speed: 4.8ms preprocess, 106.1ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 102.4ms\n",
      "Speed: 3.4ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 108.2ms\n",
      "Speed: 4.4ms preprocess, 108.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 113.0ms\n",
      "Speed: 4.1ms preprocess, 113.0ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 111.2ms\n",
      "Speed: 4.2ms preprocess, 111.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.1ms\n",
      "Speed: 6.7ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.8ms\n",
      "Speed: 7.3ms preprocess, 110.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.5ms\n",
      "Speed: 9.0ms preprocess, 105.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.0ms\n",
      "Speed: 4.7ms preprocess, 101.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.2ms\n",
      "Speed: 4.5ms preprocess, 101.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.6ms\n",
      "Speed: 3.0ms preprocess, 104.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.0ms\n",
      "Speed: 5.4ms preprocess, 103.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 96.6ms\n",
      "Speed: 3.9ms preprocess, 96.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 96.9ms\n",
      "Speed: 3.6ms preprocess, 96.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.9ms\n",
      "Speed: 3.9ms preprocess, 104.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 129.2ms\n",
      "Speed: 8.3ms preprocess, 129.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 102.8ms\n",
      "Speed: 3.4ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 107.1ms\n",
      "Speed: 4.4ms preprocess, 107.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 100.9ms\n",
      "Speed: 4.0ms preprocess, 100.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.9ms\n",
      "Speed: 3.8ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 115.0ms\n",
      "Speed: 2.8ms preprocess, 115.0ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.1ms\n",
      "Speed: 4.2ms preprocess, 106.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.1ms\n",
      "Speed: 4.6ms preprocess, 105.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.8ms\n",
      "Speed: 4.1ms preprocess, 107.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 96.9ms\n",
      "Speed: 3.8ms preprocess, 96.9ms inference, 12.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.6ms\n",
      "Speed: 7.3ms preprocess, 107.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.2ms\n",
      "Speed: 4.1ms preprocess, 107.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.8ms\n",
      "Speed: 4.3ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 106.9ms\n",
      "Speed: 4.8ms preprocess, 106.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 105.9ms\n",
      "Speed: 3.7ms preprocess, 105.9ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.8ms\n",
      "Speed: 4.1ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.2ms\n",
      "Speed: 7.5ms preprocess, 107.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 114.3ms\n",
      "Speed: 2.3ms preprocess, 114.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.2ms\n",
      "Speed: 4.1ms preprocess, 101.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.9ms\n",
      "Speed: 4.2ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.3ms\n",
      "Speed: 2.8ms preprocess, 108.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 111.6ms\n",
      "Speed: 0.0ms preprocess, 111.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 112.0ms\n",
      "Speed: 5.5ms preprocess, 112.0ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.1ms\n",
      "Speed: 4.1ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 110.9ms\n",
      "Speed: 4.0ms preprocess, 110.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.6ms\n",
      "Speed: 4.0ms preprocess, 101.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 110.2ms\n",
      "Speed: 5.0ms preprocess, 110.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 110.2ms\n",
      "Speed: 4.9ms preprocess, 110.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 111.1ms\n",
      "Speed: 2.1ms preprocess, 111.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 106.8ms\n",
      "Speed: 2.6ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 109.8ms\n",
      "Speed: 1.3ms preprocess, 109.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 111.2ms\n",
      "Speed: 1.6ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.2ms\n",
      "Speed: 3.4ms preprocess, 107.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.0ms\n",
      "Speed: 3.0ms preprocess, 107.0ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 108.0ms\n",
      "Speed: 3.7ms preprocess, 108.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 108.1ms\n",
      "Speed: 2.5ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 104.9ms\n",
      "Speed: 4.1ms preprocess, 104.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 103.6ms\n",
      "Speed: 4.9ms preprocess, 103.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 108.8ms\n",
      "Speed: 3.8ms preprocess, 108.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 106.5ms\n",
      "Speed: 4.3ms preprocess, 106.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.0ms\n",
      "Speed: 6.9ms preprocess, 107.0ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.6ms\n",
      "Speed: 3.2ms preprocess, 107.6ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.0ms\n",
      "Speed: 4.0ms preprocess, 103.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 105.2ms\n",
      "Speed: 8.5ms preprocess, 105.2ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 106.0ms\n",
      "Speed: 3.6ms preprocess, 106.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.9ms\n",
      "Speed: 4.7ms preprocess, 101.9ms inference, 6.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 104.5ms\n",
      "Speed: 6.2ms preprocess, 104.5ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 106.2ms\n",
      "Speed: 4.6ms preprocess, 106.2ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 104.2ms\n",
      "Speed: 3.1ms preprocess, 104.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.9ms\n",
      "Speed: 3.7ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 108.2ms\n",
      "Speed: 4.0ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.7ms\n",
      "Speed: 4.1ms preprocess, 107.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.9ms\n",
      "Speed: 4.0ms preprocess, 107.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 108.2ms\n",
      "Speed: 3.3ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 110.2ms\n",
      "Speed: 3.5ms preprocess, 110.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 109.9ms\n",
      "Speed: 4.1ms preprocess, 109.9ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 111.3ms\n",
      "Speed: 3.8ms preprocess, 111.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.3ms\n",
      "Speed: 5.0ms preprocess, 108.3ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.0ms\n",
      "Speed: 4.7ms preprocess, 101.0ms inference, 6.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 96.9ms\n",
      "Speed: 3.6ms preprocess, 96.9ms inference, 12.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 108.6ms\n",
      "Speed: 6.4ms preprocess, 108.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 95.8ms\n",
      "Speed: 4.4ms preprocess, 95.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.1ms\n",
      "Speed: 4.1ms preprocess, 109.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 105.5ms\n",
      "Speed: 6.7ms preprocess, 105.5ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.8ms\n",
      "Speed: 3.9ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 100.4ms\n",
      "Speed: 4.2ms preprocess, 100.4ms inference, 6.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.8ms\n",
      "Speed: 6.3ms preprocess, 107.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.7ms\n",
      "Speed: 5.6ms preprocess, 108.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 102.4ms\n",
      "Speed: 4.3ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.3ms\n",
      "Speed: 4.9ms preprocess, 106.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 98.4ms\n",
      "Speed: 4.6ms preprocess, 98.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.4ms\n",
      "Speed: 7.9ms preprocess, 104.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.3ms\n",
      "Speed: 4.3ms preprocess, 107.3ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.9ms\n",
      "Speed: 4.0ms preprocess, 103.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.3ms\n",
      "Speed: 3.9ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.3ms\n",
      "Speed: 5.5ms preprocess, 103.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 110.0ms\n",
      "Speed: 2.8ms preprocess, 110.0ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.1ms\n",
      "Speed: 2.7ms preprocess, 107.1ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.0ms\n",
      "Speed: 3.7ms preprocess, 109.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.0ms\n",
      "Speed: 3.5ms preprocess, 109.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.8ms\n",
      "Speed: 4.2ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.0ms\n",
      "Speed: 3.1ms preprocess, 107.0ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.7ms\n",
      "Speed: 4.0ms preprocess, 107.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 112.9ms\n",
      "Speed: 4.0ms preprocess, 112.9ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.4ms\n",
      "Speed: 3.2ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.0ms\n",
      "Speed: 8.3ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.8ms\n",
      "Speed: 4.1ms preprocess, 105.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.2ms\n",
      "Speed: 4.5ms preprocess, 107.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.0ms\n",
      "Speed: 4.1ms preprocess, 108.0ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.1ms\n",
      "Speed: 4.1ms preprocess, 109.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 112.2ms\n",
      "Speed: 3.1ms preprocess, 112.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 111.4ms\n",
      "Speed: 3.7ms preprocess, 111.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.6ms\n",
      "Speed: 2.9ms preprocess, 105.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.3ms\n",
      "Speed: 4.7ms preprocess, 105.3ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.3ms\n",
      "Speed: 3.9ms preprocess, 108.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.3ms\n",
      "Speed: 4.5ms preprocess, 102.3ms inference, 5.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.5ms\n",
      "Speed: 4.0ms preprocess, 108.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 107.0ms\n",
      "Speed: 2.5ms preprocess, 107.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.0ms\n",
      "Speed: 4.5ms preprocess, 103.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.7ms\n",
      "Speed: 5.1ms preprocess, 105.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 104.9ms\n",
      "Speed: 3.3ms preprocess, 104.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 3 traffic lights, 104.8ms\n",
      "Speed: 4.1ms preprocess, 104.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 111.5ms\n",
      "Speed: 5.8ms preprocess, 111.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 2 traffic lights, 105.2ms\n",
      "Speed: 4.0ms preprocess, 105.2ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 111.3ms\n",
      "Speed: 2.1ms preprocess, 111.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 107.6ms\n",
      "Speed: 4.9ms preprocess, 107.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 108.9ms\n",
      "Speed: 2.5ms preprocess, 108.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 131.9ms\n",
      "Speed: 4.6ms preprocess, 131.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 104.8ms\n",
      "Speed: 3.0ms preprocess, 104.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 104.3ms\n",
      "Speed: 3.8ms preprocess, 104.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 2 traffic lights, 96.5ms\n",
      "Speed: 7.6ms preprocess, 96.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 105.8ms\n",
      "Speed: 3.1ms preprocess, 105.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.4ms\n",
      "Speed: 4.6ms preprocess, 106.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.3ms\n",
      "Speed: 7.5ms preprocess, 105.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.8ms\n",
      "Speed: 3.4ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.1ms\n",
      "Speed: 3.6ms preprocess, 105.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.1ms\n",
      "Speed: 4.1ms preprocess, 106.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.2ms\n",
      "Speed: 6.5ms preprocess, 107.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.0ms\n",
      "Speed: 7.5ms preprocess, 105.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 111.4ms\n",
      "Speed: 5.9ms preprocess, 111.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.4ms\n",
      "Speed: 3.6ms preprocess, 105.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.2ms\n",
      "Speed: 4.2ms preprocess, 105.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.6ms\n",
      "Speed: 0.0ms preprocess, 110.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 114.8ms\n",
      "Speed: 0.0ms preprocess, 114.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.2ms\n",
      "Speed: 4.8ms preprocess, 101.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.9ms\n",
      "Speed: 3.2ms preprocess, 106.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.1ms\n",
      "Speed: 2.9ms preprocess, 109.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.2ms\n",
      "Speed: 6.2ms preprocess, 105.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.8ms\n",
      "Speed: 7.5ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.0ms\n",
      "Speed: 4.7ms preprocess, 105.0ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.9ms\n",
      "Speed: 4.1ms preprocess, 105.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.4ms\n",
      "Speed: 3.9ms preprocess, 105.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.5ms\n",
      "Speed: 4.3ms preprocess, 107.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.0ms\n",
      "Speed: 4.1ms preprocess, 103.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.9ms\n",
      "Speed: 4.1ms preprocess, 108.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.8ms\n",
      "Speed: 3.6ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.7ms\n",
      "Speed: 3.7ms preprocess, 105.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.2ms\n",
      "Speed: 6.0ms preprocess, 103.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 111.2ms\n",
      "Speed: 2.9ms preprocess, 111.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 105.4ms\n",
      "Speed: 4.4ms preprocess, 105.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 116.5ms\n",
      "Speed: 4.6ms preprocess, 116.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.9ms\n",
      "Speed: 5.6ms preprocess, 109.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.5ms\n",
      "Speed: 3.3ms preprocess, 104.5ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 112.6ms\n",
      "Speed: 4.6ms preprocess, 112.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.2ms\n",
      "Speed: 3.0ms preprocess, 109.2ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 105.9ms\n",
      "Speed: 4.7ms preprocess, 105.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 105.7ms\n",
      "Speed: 4.4ms preprocess, 105.7ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 104.9ms\n",
      "Speed: 4.0ms preprocess, 104.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 prohibitorys, 108.0ms\n",
      "Speed: 5.1ms preprocess, 108.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.5ms\n",
      "Speed: 3.6ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.0ms\n",
      "Speed: 3.5ms preprocess, 105.0ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 103.7ms\n",
      "Speed: 4.0ms preprocess, 103.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 100.4ms\n",
      "Speed: 4.1ms preprocess, 100.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.2ms\n",
      "Speed: 6.8ms preprocess, 105.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.3ms\n",
      "Speed: 4.0ms preprocess, 107.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.2ms\n",
      "Speed: 2.7ms preprocess, 101.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.1ms\n",
      "Speed: 4.6ms preprocess, 101.1ms inference, 5.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 143.3ms\n",
      "Speed: 4.8ms preprocess, 143.3ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 116.7ms\n",
      "Speed: 6.3ms preprocess, 116.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 104.5ms\n",
      "Speed: 3.1ms preprocess, 104.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 102.9ms\n",
      "Speed: 5.3ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 102.9ms\n",
      "Speed: 4.0ms preprocess, 102.9ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.8ms\n",
      "Speed: 3.1ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.5ms\n",
      "Speed: 3.8ms preprocess, 102.5ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.3ms\n",
      "Speed: 7.2ms preprocess, 103.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.6ms\n",
      "Speed: 4.2ms preprocess, 108.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.1ms\n",
      "Speed: 3.9ms preprocess, 105.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.0ms\n",
      "Speed: 3.4ms preprocess, 102.0ms inference, 0.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.0ms\n",
      "Speed: 4.1ms preprocess, 105.0ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.2ms\n",
      "Speed: 3.8ms preprocess, 105.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.3ms\n",
      "Speed: 4.0ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.4ms\n",
      "Speed: 4.4ms preprocess, 108.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.8ms\n",
      "Speed: 4.3ms preprocess, 107.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.3ms\n",
      "Speed: 7.8ms preprocess, 105.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 113.0ms\n",
      "Speed: 2.7ms preprocess, 113.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.7ms\n",
      "Speed: 2.8ms preprocess, 102.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 103.9ms\n",
      "Speed: 5.0ms preprocess, 103.9ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 110.2ms\n",
      "Speed: 2.7ms preprocess, 110.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.6ms\n",
      "Speed: 3.0ms preprocess, 104.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.1ms\n",
      "Speed: 4.8ms preprocess, 108.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.9ms\n",
      "Speed: 0.0ms preprocess, 108.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.9ms\n",
      "Speed: 3.2ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.9ms\n",
      "Speed: 7.3ms preprocess, 105.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.5ms\n",
      "Speed: 4.2ms preprocess, 106.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 133.2ms\n",
      "Speed: 4.1ms preprocess, 133.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 100.8ms\n",
      "Speed: 3.9ms preprocess, 100.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 110.7ms\n",
      "Speed: 4.0ms preprocess, 110.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.9ms\n",
      "Speed: 7.3ms preprocess, 106.9ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.1ms\n",
      "Speed: 3.8ms preprocess, 107.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.1ms\n",
      "Speed: 1.8ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.9ms\n",
      "Speed: 4.1ms preprocess, 107.9ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.6ms\n",
      "Speed: 3.6ms preprocess, 101.6ms inference, 6.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.7ms\n",
      "Speed: 3.8ms preprocess, 101.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.1ms\n",
      "Speed: 4.3ms preprocess, 105.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.7ms\n",
      "Speed: 2.0ms preprocess, 106.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.0ms\n",
      "Speed: 4.4ms preprocess, 106.0ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 107.7ms\n",
      "Speed: 6.9ms preprocess, 107.7ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.0ms\n",
      "Speed: 3.6ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.8ms\n",
      "Speed: 2.9ms preprocess, 106.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.4ms\n",
      "Speed: 6.6ms preprocess, 105.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 108.9ms\n",
      "Speed: 4.4ms preprocess, 108.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.7ms\n",
      "Speed: 3.4ms preprocess, 101.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.0ms\n",
      "Speed: 5.9ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 106.5ms\n",
      "Speed: 4.0ms preprocess, 106.5ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 117.1ms\n",
      "Speed: 4.1ms preprocess, 117.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.3ms\n",
      "Speed: 3.9ms preprocess, 109.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 115.4ms\n",
      "Speed: 4.0ms preprocess, 115.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 113.0ms\n",
      "Speed: 4.0ms preprocess, 113.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 104.3ms\n",
      "Speed: 4.1ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 109.3ms\n",
      "Speed: 4.1ms preprocess, 109.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.8ms\n",
      "Speed: 3.7ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.0ms\n",
      "Speed: 6.8ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.6ms\n",
      "Speed: 3.7ms preprocess, 101.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 105.2ms\n",
      "Speed: 3.9ms preprocess, 105.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 102.1ms\n",
      "Speed: 6.3ms preprocess, 102.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 101.9ms\n",
      "Speed: 4.9ms preprocess, 101.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.2ms\n",
      "Speed: 7.8ms preprocess, 109.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.7ms\n",
      "Speed: 4.0ms preprocess, 106.7ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.5ms\n",
      "Speed: 3.6ms preprocess, 105.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.6ms\n",
      "Speed: 4.5ms preprocess, 101.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.2ms\n",
      "Speed: 4.0ms preprocess, 101.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.3ms\n",
      "Speed: 3.1ms preprocess, 107.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.0ms\n",
      "Speed: 4.4ms preprocess, 109.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 113.1ms\n",
      "Speed: 0.0ms preprocess, 113.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.6ms\n",
      "Speed: 8.0ms preprocess, 108.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.8ms\n",
      "Speed: 3.5ms preprocess, 103.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 1 traffic light, 102.8ms\n",
      "Speed: 5.8ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.1ms\n",
      "Speed: 5.4ms preprocess, 106.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.1ms\n",
      "Speed: 3.8ms preprocess, 106.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 100.8ms\n",
      "Speed: 3.9ms preprocess, 100.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.9ms\n",
      "Speed: 0.5ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.2ms\n",
      "Speed: 3.4ms preprocess, 104.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.3ms\n",
      "Speed: 3.8ms preprocess, 104.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.5ms\n",
      "Speed: 4.3ms preprocess, 104.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.0ms\n",
      "Speed: 4.4ms preprocess, 101.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 102.4ms\n",
      "Speed: 7.6ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.1ms\n",
      "Speed: 5.1ms preprocess, 110.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.4ms\n",
      "Speed: 7.6ms preprocess, 105.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.8ms\n",
      "Speed: 4.1ms preprocess, 107.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.0ms\n",
      "Speed: 6.5ms preprocess, 102.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.5ms\n",
      "Speed: 3.9ms preprocess, 105.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.6ms\n",
      "Speed: 3.9ms preprocess, 110.6ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 103.4ms\n",
      "Speed: 4.1ms preprocess, 103.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.1ms\n",
      "Speed: 3.1ms preprocess, 107.1ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 108.4ms\n",
      "Speed: 3.6ms preprocess, 108.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 111.8ms\n",
      "Speed: 2.2ms preprocess, 111.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 106.2ms\n",
      "Speed: 7.3ms preprocess, 106.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.1ms\n",
      "Speed: 3.1ms preprocess, 107.1ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.3ms\n",
      "Speed: 3.5ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 105.1ms\n",
      "Speed: 7.4ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 104.8ms\n",
      "Speed: 3.5ms preprocess, 104.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.0ms\n",
      "Speed: 4.0ms preprocess, 101.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 96.7ms\n",
      "Speed: 3.7ms preprocess, 96.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 104.7ms\n",
      "Speed: 8.2ms preprocess, 104.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 105.9ms\n",
      "Speed: 4.8ms preprocess, 105.9ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 112.2ms\n",
      "Speed: 5.3ms preprocess, 112.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 3 traffic lights, 107.0ms\n",
      "Speed: 4.7ms preprocess, 107.0ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 104.6ms\n",
      "Speed: 4.7ms preprocess, 104.6ms inference, 5.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.4ms\n",
      "Speed: 3.9ms preprocess, 109.4ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.7ms\n",
      "Speed: 7.3ms preprocess, 107.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 109.1ms\n",
      "Speed: 7.0ms preprocess, 109.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.6ms\n",
      "Speed: 4.0ms preprocess, 101.6ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 3 traffic lights, 100.9ms\n",
      "Speed: 3.9ms preprocess, 100.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 3 traffic lights, 107.8ms\n",
      "Speed: 4.0ms preprocess, 107.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 3 traffic lights, 99.9ms\n",
      "Speed: 4.6ms preprocess, 99.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 104.0ms\n",
      "Speed: 3.9ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 3 traffic lights, 111.9ms\n",
      "Speed: 3.5ms preprocess, 111.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 105.9ms\n",
      "Speed: 3.8ms preprocess, 105.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 107.7ms\n",
      "Speed: 5.0ms preprocess, 107.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 102.6ms\n",
      "Speed: 4.2ms preprocess, 102.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 106.8ms\n",
      "Speed: 4.1ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 108.0ms\n",
      "Speed: 7.2ms preprocess, 108.0ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 108.3ms\n",
      "Speed: 3.8ms preprocess, 108.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 104.9ms\n",
      "Speed: 2.9ms preprocess, 104.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 103.4ms\n",
      "Speed: 4.3ms preprocess, 103.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 101.3ms\n",
      "Speed: 3.7ms preprocess, 101.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 104.1ms\n",
      "Speed: 4.1ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 3 traffic lights, 104.4ms\n",
      "Speed: 4.0ms preprocess, 104.4ms inference, 6.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 4 traffic lights, 105.8ms\n",
      "Speed: 4.3ms preprocess, 105.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 4 traffic lights, 108.9ms\n",
      "Speed: 6.1ms preprocess, 108.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 4 traffic lights, 118.0ms\n",
      "Speed: 4.4ms preprocess, 118.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 4 traffic lights, 102.5ms\n",
      "Speed: 3.8ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 3 traffic lights, 104.3ms\n",
      "Speed: 3.4ms preprocess, 104.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 101.0ms\n",
      "Speed: 3.6ms preprocess, 101.0ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 3 traffic lights, 94.5ms\n",
      "Speed: 4.6ms preprocess, 94.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 99.6ms\n",
      "Speed: 3.7ms preprocess, 99.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 131.3ms\n",
      "Speed: 5.7ms preprocess, 131.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 107.9ms\n",
      "Speed: 2.3ms preprocess, 107.9ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 3 traffic lights, 107.1ms\n",
      "Speed: 4.0ms preprocess, 107.1ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 105.7ms\n",
      "Speed: 3.3ms preprocess, 105.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 112.5ms\n",
      "Speed: 4.0ms preprocess, 112.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 108.1ms\n",
      "Speed: 3.1ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.7ms\n",
      "Speed: 2.1ms preprocess, 105.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.9ms\n",
      "Speed: 6.3ms preprocess, 110.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.8ms\n",
      "Speed: 8.6ms preprocess, 105.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 103.0ms\n",
      "Speed: 6.6ms preprocess, 103.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.1ms\n",
      "Speed: 6.3ms preprocess, 105.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 106.7ms\n",
      "Speed: 1.7ms preprocess, 106.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 99.3ms\n",
      "Speed: 3.4ms preprocess, 99.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 106.8ms\n",
      "Speed: 6.1ms preprocess, 106.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 105.9ms\n",
      "Speed: 6.4ms preprocess, 105.9ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 105.2ms\n",
      "Speed: 6.9ms preprocess, 105.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 101.7ms\n",
      "Speed: 3.2ms preprocess, 101.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.8ms\n",
      "Speed: 3.7ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.7ms\n",
      "Speed: 3.7ms preprocess, 110.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.6ms\n",
      "Speed: 4.4ms preprocess, 108.6ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 105.5ms\n",
      "Speed: 4.3ms preprocess, 105.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.3ms\n",
      "Speed: 4.3ms preprocess, 110.3ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.6ms\n",
      "Speed: 3.7ms preprocess, 108.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.1ms\n",
      "Speed: 6.5ms preprocess, 104.1ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 104.4ms\n",
      "Speed: 2.5ms preprocess, 104.4ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 107.4ms\n",
      "Speed: 3.3ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 108.1ms\n",
      "Speed: 3.9ms preprocess, 108.1ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 106.5ms\n",
      "Speed: 3.2ms preprocess, 106.5ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 96.2ms\n",
      "Speed: 3.2ms preprocess, 96.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 99.1ms\n",
      "Speed: 2.1ms preprocess, 99.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 106.8ms\n",
      "Speed: 4.0ms preprocess, 106.8ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 101.4ms\n",
      "Speed: 4.3ms preprocess, 101.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 103.4ms\n",
      "Speed: 7.1ms preprocess, 103.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.7ms\n",
      "Speed: 4.3ms preprocess, 109.7ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 107.4ms\n",
      "Speed: 5.8ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 117.6ms\n",
      "Speed: 4.7ms preprocess, 117.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 110.9ms\n",
      "Speed: 6.8ms preprocess, 110.9ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 109.4ms\n",
      "Speed: 3.4ms preprocess, 109.4ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 120.4ms\n",
      "Speed: 0.0ms preprocess, 120.4ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 2 traffic lights, 104.3ms\n",
      "Speed: 4.6ms preprocess, 104.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 109.6ms\n",
      "Speed: 4.2ms preprocess, 109.6ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 113.8ms\n",
      "Speed: 5.5ms preprocess, 113.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 112.7ms\n",
      "Speed: 7.7ms preprocess, 112.7ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 112.4ms\n",
      "Speed: 3.5ms preprocess, 112.4ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 3 traffic lights, 103.9ms\n",
      "Speed: 3.9ms preprocess, 103.9ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 190.3ms\n",
      "Speed: 6.0ms preprocess, 190.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 132.9ms\n",
      "Speed: 6.5ms preprocess, 132.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 121.1ms\n",
      "Speed: 2.7ms preprocess, 121.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 117.9ms\n",
      "Speed: 2.8ms preprocess, 117.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 124.2ms\n",
      "Speed: 9.4ms preprocess, 124.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 prohibitory, 2 traffic lights, 109.6ms\n",
      "Speed: 3.7ms preprocess, 109.6ms inference, 14.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 (no detections), 102.2ms\n",
      "Speed: 4.0ms preprocess, 102.2ms inference, 16.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 158.8ms\n",
      "Speed: 8.5ms preprocess, 158.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 155.8ms\n",
      "Speed: 5.0ms preprocess, 155.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 114.2ms\n",
      "Speed: 5.9ms preprocess, 114.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 113.3ms\n",
      "Speed: 3.1ms preprocess, 113.3ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 1 traffic light, 111.9ms\n",
      "Speed: 8.2ms preprocess, 111.9ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22556\\854296390.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Run YOLO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myolo_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'set_prompts'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpersist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# merge list of Result into one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[1;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[1;31m# Inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mprofilers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m                 \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[1;31m# Postprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\predictor.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m         visualize = increment_path(self.save_dir / Path(self.batch[0][0]).stem,\n\u001b[0;32m    137\u001b[0m                                    mkdir=True) if self.args.visualize and (not self.source_type.tensor) else False\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, im, augment, visualize)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpt\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# PyTorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maugment\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mvisualize\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# TorchScript\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\tasks.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# for cases of training and validating while training.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\tasks.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_augment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\tasks.py\u001b[0m in \u001b[0;36m_predict_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_profile_one_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# save output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;34m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;34m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;34m\"\"\"'forward()' applies the YOLOv5 FPN to input data.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_fuse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;34m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 460\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load YOLO model\n",
    "yolo_model = YOLO(\"runs/detect/train12/weights/best.pt\")\n",
    "\n",
    "# Load SVM model\n",
    "svm_model = joblib.load(\"svm_model.pkl\")\n",
    "\n",
    "# Class names (must match your 15-class SVM dataset)\n",
    "CLASS_NAMES = [\n",
    "    'Green Light', 'Red Light', 'Speed Limit 10', 'Speed Limit 100', 'Speed Limit 110',\n",
    "    'Speed Limit 120', 'Speed Limit 20', 'Speed Limit 30', 'Speed Limit 40', 'Speed Limit 50',\n",
    "    'Speed Limit 60', 'Speed Limit 70', 'Speed Limit 80', 'Speed Limit 90', 'Stop'\n",
    "]\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(\"C:/osama/CUFE/5-Senior2/GP/ADAS-System-prototype/Traffic_Recognition/CarlaVideos/traffic signs test.mp4\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO\n",
    "    results = yolo_model.predict(frame, conf=0.2)\n",
    "    boxes = results[0].boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        cropped = frame[y1:y2, x1:x2]\n",
    "\n",
    "        # Check for valid crop\n",
    "        if cropped.size == 0 or x2 - x1 < 10 or y2 - y1 < 10:\n",
    "            continue\n",
    "\n",
    "        # Resize and convert to gray\n",
    "        resized = cv2.resize(cropped, (64, 64))\n",
    "        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Extract HOG features\n",
    "        features = hog(gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True).reshape(1, -1)\n",
    "\n",
    "        # Predict using SVM\n",
    "        prediction = svm_model.predict(features)[0]\n",
    "        confidence = max(svm_model.predict_proba(features)[0])\n",
    "\n",
    "        # Filter out weak detections\n",
    "        if confidence < 0.9:\n",
    "            continue\n",
    "\n",
    "        label = f\"{CLASS_NAMES[prediction]} ({confidence:.2f})\"\n",
    "\n",
    "        # Draw result\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"YOLO + SVM Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

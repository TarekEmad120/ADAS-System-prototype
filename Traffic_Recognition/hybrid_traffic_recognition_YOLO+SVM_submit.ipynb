{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf1d3d1",
   "metadata": {},
   "source": [
    "# Hybrid Approach\n",
    "- Deep learning to localize the sign using YOLO\n",
    "- classical ML to recognize the traffic sign using SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e096851",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa4db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog\n",
    "import joblib\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0107f",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b8b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO dataset paths\n",
    "orginal_yolo_dataset_root = \"YOLODataset\"\n",
    "structured_yolo_dataset_root = \"YOLO_dataset_structured\"\n",
    "\n",
    "# SVM dataset paths\n",
    "original_svm_dataset_root = \"svm_dataset/train\"\n",
    "\n",
    "# models\n",
    "yolo_model_path = \"saved_models/yolo_best.pt\"\n",
    "svm_model_path = \"saved_models/svm_model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88970573",
   "metadata": {},
   "source": [
    "# Dataset Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3185652",
   "metadata": {},
   "source": [
    "### dataset to YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996b440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO dataset prepared at: YOLO_dataset_structured\n"
     ]
    }
   ],
   "source": [
    "def copy_split_files(list_file, source_folder, img_output, lbl_output):\n",
    "    \"\"\"\n",
    "    Service Name      : copy_split_files\n",
    "    Sync/Async        : Synchronous\n",
    "    Reentrancy        : Reentrant\n",
    "    Parameters (in)   : list_file (str) - Path to .txt file containing relative image paths.\n",
    "                        source_folder (str) - Directory containing the original images and label files.\n",
    "                        img_output (str) - Destination directory for image files.\n",
    "                        lbl_output (str) - Destination directory for label files.\n",
    "    Parameters (inout): None\n",
    "    Parameters (out)  : None\n",
    "    Return value      : None\n",
    "    Description       : Reads a list of image filenames and copies both the image and its corresponding label\n",
    "                        (if available) to the specified destination folders.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(list_file, \"r\") as file:\n",
    "        image_list = file.readlines()\n",
    "\n",
    "    for entry in image_list:\n",
    "        entry = entry.strip()\n",
    "        filename = os.path.basename(entry)\n",
    "        name_without_ext = os.path.splitext(filename)[0]\n",
    "        label_filename = f\"{name_without_ext}.txt\"\n",
    "\n",
    "        src_img_path = os.path.join(source_folder, filename)\n",
    "        src_lbl_path = os.path.join(source_folder, label_filename)\n",
    "\n",
    "        shutil.copy(src_img_path, os.path.join(img_output, filename))\n",
    "        shutil.copy(src_lbl_path, os.path.join(lbl_output, label_filename))\n",
    "\n",
    "base_yolo_path = orginal_yolo_dataset_root\n",
    "source_data_dir = os.path.join(base_yolo_path, \"ts\")\n",
    "train_file_list = os.path.join(base_yolo_path, \"train.txt\")\n",
    "val_file_list = os.path.join(base_yolo_path, \"test.txt\")\n",
    "\n",
    "output_root = structured_yolo_dataset_root\n",
    "train_img_dir = os.path.join(output_root, \"images/train\")\n",
    "val_img_dir = os.path.join(output_root, \"images/val\")\n",
    "train_lbl_dir = os.path.join(output_root, \"labels/train\")\n",
    "val_lbl_dir = os.path.join(output_root, \"labels/val\")\n",
    "\n",
    "for path in [train_img_dir, val_img_dir, train_lbl_dir, val_lbl_dir]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "copy_split_files(train_file_list, source_data_dir, train_img_dir, train_lbl_dir)\n",
    "copy_split_files(val_file_list, source_data_dir, val_img_dir, val_lbl_dir)\n",
    "\n",
    "yaml_output_path = os.path.join(output_root, \"data.yaml\")\n",
    "with open(yaml_output_path, \"w\") as yaml_file:\n",
    "    yaml_file.write(\n",
    "        f\"train: {os.path.abspath(train_img_dir)}\\n\"\n",
    "        f\"val: {os.path.abspath(val_img_dir)}\\n\\n\"\n",
    "        f\"nc: 4\\n\"\n",
    "        f\"names: ['prohibitory', 'danger', 'mandatory', 'other']\\n\"\n",
    "    )\n",
    "\n",
    "print(\"YOLO dataset prepared at:\", output_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130e006",
   "metadata": {},
   "source": [
    "# Train YOLO detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678bfbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n.yaml\").load(\"yolov8n.pt\")  # build from YAML and transfer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.133 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.9 torch-1.13.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=YOLO_dataset/data.yaml, epochs=5, patience=50, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train8\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3011628 parameters, 3011612 gradients\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train8', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\labels\\train.cache... 630 images, 0 backgrounds, 0 corrupt: 100%|██████████| 630/630 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\images\\train\\00340.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\labels\\val.cache... 111 images, 0 backgrounds, 0 corrupt: 100%|██████████| 111/111 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train8\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 960 train, 960 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train8\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/5         0G      1.057      3.641     0.8755         12        960: 100%|██████████| 40/40 [08:27<00:00, 12.68s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:24<00:00,  6.02s/it]\n",
      "                   all        111        179    0.00197      0.318      0.231      0.196\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/5         0G     0.9083      2.398     0.8652          9        960: 100%|██████████| 40/40 [08:11<00:00, 12.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:23<00:00,  5.75s/it]\n",
      "                   all        111        179          1      0.155      0.717       0.57\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/5         0G     0.8593      1.977     0.8532         16        960: 100%|██████████| 40/40 [08:09<00:00, 12.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.72s/it]\n",
      "                   all        111        179      0.929      0.817      0.903      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        4/5         0G     0.8247      1.748      0.844         16        960: 100%|██████████| 40/40 [08:09<00:00, 12.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.74s/it]\n",
      "                   all        111        179      0.915      0.897       0.94      0.769\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        5/5         0G     0.8066      1.548     0.8434          9        960: 100%|██████████| 40/40 [08:07<00:00, 12.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:23<00:00,  5.79s/it]\n",
      "                   all        111        179      0.903      0.896      0.959      0.794\n",
      "\n",
      "5 epochs completed in 0.718 hours.\n",
      "Optimizer stripped from runs\\detect\\train8\\weights\\last.pt, 6.3MB\n",
      "Optimizer stripped from runs\\detect\\train8\\weights\\best.pt, 6.3MB\n",
      "\n",
      "Validating runs\\detect\\train8\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.9 torch-1.13.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "YOLOv8n summary (fused): 168 layers, 3006428 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:20<00:00,  5.02s/it]\n",
      "                   all        111        179      0.936      0.872      0.959      0.794\n",
      "           prohibitory        111         71      0.958      0.972      0.989      0.809\n",
      "                danger        111         36      0.928          1      0.995      0.857\n",
      "             mandatory        111         23      0.957      0.739       0.94      0.795\n",
      "                 other        111         49      0.901      0.776      0.912      0.715\n",
      "Speed: 2.5ms preprocess, 140.5ms inference, 0.0ms loss, 10.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model.train(data=\"structured_yolo_dataset_root/data.yaml\", epochs=5, imgsz=960, batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.133 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.9 torch-1.13.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=runs/detect/train8/weights/best.pt, data=YOLO_dataset/data.yaml, epochs=2, patience=50, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=True, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=5, translate=0.1, scale=0.5, shear=2.0, perspective=0.0, flipud=0.5, fliplr=0.5, mosaic=1.0, mixup=0.1, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train12\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3011628 parameters, 3011612 gradients\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train12', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\labels\\train.cache... 630 images, 0 backgrounds, 0 corrupt: 100%|██████████| 630/630 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\images\\train\\00340.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\YOLO_dataset\\labels\\val.cache... 111 images, 0 backgrounds, 0 corrupt: 100%|██████████| 111/111 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train12\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 960 train, 960 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train12\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"runs/detect/train8/weights/best.pt\")\n",
    "model.train(data=\"structured_yolo_dataset_root/data.yaml\", epochs=2, imgsz=960, visualize=True, batch=16, degrees=5, translate=0.1, scale=0.5, shear=2.0, flipud=0.5, fliplr=0.5, mosaic=1.0, mixup=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce537a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.143 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.9 torch-1.13.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=runs/detect/train17/weights/best.pt, data=car_yolo_filtered/data.yaml, epochs=2, patience=50, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=True, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train18\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3011823 parameters, 3011807 gradients\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train18', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\train\\labels.cache... 6711 images, 0 backgrounds, 1 corrupt: 100%|██████████| 6712/6712 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\train\\images\\00340.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\train\\images\\outdoor-vertical-traffic-light-green-color-outdoor-vertical-traffic-light-blue-sky-trees-around-traffic-control-concept-221609452_jpg.rf.6a211b5c631bf58021d4596ca8c6a26c.jpg: ignoring corrupt image/label: [Errno 2] No such file or directory: 'C:\\\\osama\\\\CUFE\\\\5-Senior2\\\\GP\\\\ADAS-System-prototype\\\\Traffic_Recognition\\\\car_yolo_filtered\\\\train\\\\images\\\\outdoor-vertical-traffic-light-green-color-outdoor-vertical-traffic-light-blue-sky-trees-around-traffic-control-concept-221609452_jpg.rf.6a211b5c631bf58021d4596ca8c6a26c.jpg'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\test\\labels.cache... 1331 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1331/1331 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train18\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 960 train, 960 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train18\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/2         0G     0.8285     0.5572      1.162         11        960: 100%|██████████| 420/420 [1:10:40<00:00, 10.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 42/42 [05:06<00:00,  7.29s/it]\n",
      "                   all       1331       1692      0.925      0.806      0.903      0.675\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/2         0G     0.8224     0.5594       1.16         16        960: 100%|██████████| 420/420 [1:09:31<00:00,  9.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 42/42 [05:01<00:00,  7.18s/it]\n",
      "                   all       1331       1692      0.878      0.834        0.9      0.696\n",
      "\n",
      "2 epochs completed in 2.506 hours.\n",
      "Optimizer stripped from runs\\detect\\train18\\weights\\last.pt, 6.3MB\n",
      "Optimizer stripped from runs\\detect\\train18\\weights\\best.pt, 6.3MB\n",
      "\n",
      "Validating runs\\detect\\train18\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.9 torch-1.13.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "YOLOv8n summary (fused): 168 layers, 3006623 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 42/42 [04:11<00:00,  5.99s/it]\n",
      "                   all       1331       1692      0.877      0.833        0.9      0.696\n",
      "           prohibitory       1331        637      0.948      0.959      0.979       0.82\n",
      "                danger       1331         36      0.942      0.901      0.972      0.791\n",
      "             mandatory       1331         23      0.774      0.826      0.874      0.699\n",
      "                 other       1331         49      0.823      0.694      0.803       0.63\n",
      "         traffic light       1331        947      0.901      0.787      0.873       0.54\n",
      "Speed: 3.8ms preprocess, 177.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train18\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"runs/detect/train9/weights/best.pt\")\n",
    "model.train(data=\"car_yolo_filtered/data.yaml\", epochs=2, imgsz=960,visualize=True , batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fa8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.143 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.9 torch-1.13.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=runs/detect/train19/weights/best.pt, data=car_yolo_filtered/data.yaml, epochs=2, patience=50, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=True, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3011823 parameters, 3011807 gradients\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train20', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\train\\labels.cache... 6711 images, 0 backgrounds, 1 corrupt: 100%|██████████| 6712/6712 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\train\\images\\00340.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\train\\images\\outdoor-vertical-traffic-light-green-color-outdoor-vertical-traffic-light-blue-sky-trees-around-traffic-control-concept-221609452_jpg.rf.6a211b5c631bf58021d4596ca8c6a26c.jpg: ignoring corrupt image/label: [Errno 2] No such file or directory: 'C:\\\\osama\\\\CUFE\\\\5-Senior2\\\\GP\\\\ADAS-System-prototype\\\\Traffic_Recognition\\\\car_yolo_filtered\\\\train\\\\images\\\\outdoor-vertical-traffic-light-green-color-outdoor-vertical-traffic-light-blue-sky-trees-around-traffic-control-concept-221609452_jpg.rf.6a211b5c631bf58021d4596ca8c6a26c.jpg'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\osama\\CUFE\\5-Senior2\\GP\\ADAS-System-prototype\\Traffic_Recognition\\car_yolo_filtered\\test\\labels.cache... 1331 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1331/1331 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train20\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 960 train, 960 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train20\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/2         0G     0.7501     0.4764      1.111         11        960: 100%|██████████| 420/420 [1:07:03<00:00,  9.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 42/42 [04:49<00:00,  6.90s/it]\n",
      "                   all       1331       1692      0.934      0.813      0.904      0.688\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/2         0G     0.7404     0.4763      1.108         16        960: 100%|██████████| 420/420 [1:07:23<00:00,  9.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 42/42 [04:51<00:00,  6.95s/it]\n",
      "                   all       1331       1692      0.891      0.885      0.913      0.698\n",
      "\n",
      "2 epochs completed in 2.403 hours.\n",
      "Optimizer stripped from runs\\detect\\train20\\weights\\last.pt, 6.3MB\n",
      "Optimizer stripped from runs\\detect\\train20\\weights\\best.pt, 6.3MB\n",
      "\n",
      "Validating runs\\detect\\train20\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.9 torch-1.13.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "YOLOv8n summary (fused): 168 layers, 3006623 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 42/42 [04:01<00:00,  5.76s/it]\n",
      "                   all       1331       1692       0.89      0.886      0.913      0.698\n",
      "           prohibitory       1331        637      0.943      0.959      0.982      0.822\n",
      "                danger       1331         36      0.972       0.97      0.974      0.798\n",
      "             mandatory       1331         23      0.775      0.898      0.893      0.697\n",
      "                 other       1331         49       0.85      0.807       0.84      0.639\n",
      "         traffic light       1331        947      0.913      0.796      0.877      0.533\n",
      "Speed: 3.6ms preprocess, 170.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train20\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"runs/detect/train14/weights/best.pt\")\n",
    "model.train(data=\"car_yolo_filtered/data.yaml\", epochs=2, imgsz=960,visualize=True , batch=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3298b0",
   "metadata": {},
   "source": [
    "#### Predict YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f97d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 802, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 802, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 802, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 802, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 802, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 802, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 476]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 476]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 476]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 476]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.96 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 394, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n",
      "Detected: prohibitory 0.97 at [751, 395, 803, 475]\n"
     ]
    }
   ],
   "source": [
    "# Define the mapping of class indices to names\n",
    "category_labels = ['prohibitory', 'danger', 'mandatory', 'other', 'Traffic light']\n",
    "\n",
    "# Initialize the YOLO object detector with a pre-trained model\n",
    "detector = YOLO(yolo_model_path)\n",
    "\n",
    "# Video input file\n",
    "video_source = \"./CarlaVideos/traffic signs test.mp4\"\n",
    "capture = cv2.VideoCapture(video_source)\n",
    "\n",
    "if not capture.isOpened():\n",
    "    print(\"Error: Unable to open video.\")\n",
    "    exit()\n",
    "\n",
    "# Frame loop\n",
    "while True:\n",
    "    success, frame = capture.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Resize to match model training dimensions or display preferences\n",
    "    frame = cv2.resize(frame, (1200, 960))\n",
    "\n",
    "    # Run YOLO detection\n",
    "    detections = detector.predict(source=frame, conf=0.6, verbose=False)\n",
    "    prediction_boxes = detections[0].boxes\n",
    "\n",
    "    for box in prediction_boxes:\n",
    "        x_min, y_min, x_max, y_max = map(int, box.xyxy[0])\n",
    "        class_index = int(box.cls[0])\n",
    "        confidence = float(box.conf[0])\n",
    "        class_label = f\"{category_labels[class_index]} {confidence:.2f}\"\n",
    "\n",
    "        print(f\"Detected: {class_label} at [{x_min}, {y_min}, {x_max}, {y_max}]\")\n",
    "\n",
    "        # Annotate the image with bounding box and label\n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, class_label, (x_min, y_min - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"Traffic Detection\", frame)\n",
    "\n",
    "    # Exit loop if user presses 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e4069",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e6506",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0057aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_with_labels(data_dir, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Service Name      : load_images_with_labels\n",
    "    Sync/Async        : Synchronous\n",
    "    Reentrancy        : Reentrant\n",
    "    Parameters (in)   : data_dir (str) - Root path containing class-named subfolders.\n",
    "                        target_size (tuple) - Desired image dimensions as (width, height).\n",
    "    Parameters (inout): None\n",
    "    Parameters (out)  : None\n",
    "    Return value      : images (list) - Loaded and resized image arrays.\n",
    "                        labels (list) - Corresponding integer labels.\n",
    "    Description       : Loads and resizes images from directory, assigning labels based on folder names.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for folder_name in os.listdir(data_dir):\n",
    "        folder_path = os.path.join(data_dir, folder_name)\n",
    "\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        label = int(folder_name)\n",
    "\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if not file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            img = cv2.imread(file_path)\n",
    "\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            resized_img = cv2.resize(img, target_size)\n",
    "\n",
    "            images.append(resized_img)\n",
    "            labels.append(label)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfac9e7",
   "metadata": {},
   "source": [
    "#### Preprocessing & Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_extract_features(image_list, label_list, apply_augmentation=True):\n",
    "    \"\"\"\n",
    "    Service Name      : preprocess_and_extract_features\n",
    "    Sync/Async        : Synchronous\n",
    "    Reentrancy        : Reentrant\n",
    "    Parameters (in)   : image_list (list) - List of input images (as NumPy arrays)\n",
    "                        label_list (list) - Corresponding labels for each image\n",
    "                        apply_augmentation (bool) - Whether to apply simple image augmentations\n",
    "    Parameters (inout): None\n",
    "    Parameters (out)  : None\n",
    "    Return value      : X_features (ndarray) - Array of HOG feature vectors\n",
    "                        y_labels (ndarray) - Array of corresponding labels\n",
    "    Description       : Preprocesses a list of images and extracts HOG features with optional augmentation.\n",
    "    \"\"\"\n",
    "    feature_vectors = []\n",
    "    output_labels = []\n",
    "\n",
    "    for idx in range(len(image_list)):\n",
    "        image = image_list[idx]\n",
    "        label = label_list[idx]\n",
    "\n",
    "        resized = cv2.resize(image, (64, 64))\n",
    "        grayscale = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "        normalized = grayscale.astype(np.float32) / 255.0\n",
    "        variations = [normalized]\n",
    "\n",
    "        if apply_augmentation:\n",
    "            blurred = cv2.GaussianBlur(normalized, (5, 5), 0)\n",
    "            variations.append(blurred)\n",
    "\n",
    "            angle = random.choice([-10, 10])\n",
    "            center = (normalized.shape[1] // 2, normalized.shape[0] // 2)\n",
    "            rot_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            rotated = cv2.warpAffine(normalized, rot_matrix, (normalized.shape[1], normalized.shape[0]))\n",
    "            variations.append(rotated)\n",
    "\n",
    "        for version in variations:\n",
    "            features = hog(\n",
    "                version,\n",
    "                pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2),\n",
    "                feature_vector=True\n",
    "            )\n",
    "            feature_vectors.append(features)\n",
    "            output_labels.append(label)\n",
    "\n",
    "    return np.array(feature_vectors), np.array(output_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507517f9",
   "metadata": {},
   "source": [
    "#### Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94       677\n",
      "           1       0.89      0.97      0.93       691\n",
      "           2       1.00      0.73      0.84        11\n",
      "           3       0.99      0.95      0.97       149\n",
      "           4       0.94      0.90      0.92        68\n",
      "           5       0.97      0.95      0.96       155\n",
      "           6       0.97      0.96      0.96       170\n",
      "           7       0.97      0.97      0.97       365\n",
      "           8       1.00      0.99      1.00       134\n",
      "           9       0.95      0.91      0.93       160\n",
      "          10       0.97      0.94      0.96       305\n",
      "          11       0.98      0.93      0.95       210\n",
      "          12       0.94      0.90      0.92       191\n",
      "          13       0.96      0.89      0.93       215\n",
      "          14       1.00      1.00      1.00       784\n",
      "          15       1.00      1.00      1.00       385\n",
      "          16       1.00      1.00      1.00       255\n",
      "          17       1.00      1.00      1.00       780\n",
      "          18       1.00      1.00      1.00       704\n",
      "          19       1.00      1.00      1.00       121\n",
      "          20       1.00      1.00      1.00       213\n",
      "          21       1.00      1.00      1.00       208\n",
      "          22       1.00      1.00      1.00       224\n",
      "          23       1.00      1.00      1.00       306\n",
      "          24       1.00      1.00      1.00       166\n",
      "          25       1.00      1.00      1.00       897\n",
      "          26       1.00      1.00      1.00       349\n",
      "          27       1.00      1.00      1.00       265\n",
      "          28       1.00      1.00      1.00       360\n",
      "          29       1.00      1.00      1.00       170\n",
      "          30       1.00      1.00      1.00       282\n",
      "          31       1.00      1.00      1.00       447\n",
      "          32       1.00      1.00      1.00       159\n",
      "          33       1.00      1.00      1.00       408\n",
      "          34       1.00      1.00      1.00       216\n",
      "          35       1.00      1.00      1.00       767\n",
      "          36       1.00      1.00      1.00       219\n",
      "          37       1.00      1.00      1.00       109\n",
      "          38       1.00      1.00      1.00      1239\n",
      "          39       1.00      1.00      1.00       174\n",
      "          40       1.00      1.00      1.00       207\n",
      "          41       1.00      1.00      1.00       149\n",
      "          42       1.00      1.00      1.00       176\n",
      "\n",
      "    accuracy                           0.99     14240\n",
      "   macro avg       0.99      0.97      0.98     14240\n",
      "weighted avg       0.99      0.99      0.99     14240\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_model_augmented.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw images\n",
    "images, labels = load_images_with_labels(original_svm_dataset_root)\n",
    "\n",
    "# Extract features (with preprocessing and augmentation)\n",
    "X, y = preprocess_and_extract_features(images, labels, apply_augmentation=True)\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "clf = SVC(kernel='rbf', probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(clf, \"svm_model_augmented.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcd78d",
   "metadata": {},
   "source": [
    "# Final Prediction YOLO + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1078fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\base.py:338: UserWarning: Trying to unpickle estimator SVC from version 1.5.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "\n",
      "0: 768x960 1 prohibitory, 252.8ms\n",
      "Speed: 14.0ms preprocess, 252.8ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 278.5ms\n",
      "Speed: 8.1ms preprocess, 278.5ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 249.6ms\n",
      "Speed: 7.6ms preprocess, 249.6ms inference, 3.6ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 250.9ms\n",
      "Speed: 9.1ms preprocess, 250.9ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 239.1ms\n",
      "Speed: 12.5ms preprocess, 239.1ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 239.8ms\n",
      "Speed: 9.4ms preprocess, 239.8ms inference, 3.5ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 234.8ms\n",
      "Speed: 12.3ms preprocess, 234.8ms inference, 2.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 237.0ms\n",
      "Speed: 10.4ms preprocess, 237.0ms inference, 0.8ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 236.5ms\n",
      "Speed: 9.8ms preprocess, 236.5ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 261.5ms\n",
      "Speed: 9.6ms preprocess, 261.5ms inference, 3.3ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 253.7ms\n",
      "Speed: 12.5ms preprocess, 253.7ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 245.3ms\n",
      "Speed: 11.5ms preprocess, 245.3ms inference, 5.8ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 225.7ms\n",
      "Speed: 8.6ms preprocess, 225.7ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 224.7ms\n",
      "Speed: 11.9ms preprocess, 224.7ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 227.0ms\n",
      "Speed: 10.5ms preprocess, 227.0ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 249.9ms\n",
      "Speed: 9.1ms preprocess, 249.9ms inference, 3.8ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 248.6ms\n",
      "Speed: 5.3ms preprocess, 248.6ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 216.2ms\n",
      "Speed: 8.9ms preprocess, 216.2ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 219.3ms\n",
      "Speed: 7.9ms preprocess, 219.3ms inference, 0.9ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 217.4ms\n",
      "Speed: 6.0ms preprocess, 217.4ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 212.7ms\n",
      "Speed: 6.0ms preprocess, 212.7ms inference, 4.6ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 252.4ms\n",
      "Speed: 7.0ms preprocess, 252.4ms inference, 5.9ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 225.8ms\n",
      "Speed: 12.0ms preprocess, 225.8ms inference, 1.5ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 207.4ms\n",
      "Speed: 11.6ms preprocess, 207.4ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 275.2ms\n",
      "Speed: 8.7ms preprocess, 275.2ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 205.9ms\n",
      "Speed: 7.1ms preprocess, 205.9ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 215.5ms\n",
      "Speed: 7.2ms preprocess, 215.5ms inference, 2.3ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 238.6ms\n",
      "Speed: 2.2ms preprocess, 238.6ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 212.4ms\n",
      "Speed: 11.0ms preprocess, 212.4ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 211.2ms\n",
      "Speed: 9.0ms preprocess, 211.2ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 215.6ms\n",
      "Speed: 8.8ms preprocess, 215.6ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 216.1ms\n",
      "Speed: 7.7ms preprocess, 216.1ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 220.9ms\n",
      "Speed: 8.0ms preprocess, 220.9ms inference, 1.1ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 221.0ms\n",
      "Speed: 9.8ms preprocess, 221.0ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 215.0ms\n",
      "Speed: 7.7ms preprocess, 215.0ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 218.6ms\n",
      "Speed: 7.1ms preprocess, 218.6ms inference, 1.4ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 207.6ms\n",
      "Speed: 8.3ms preprocess, 207.6ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 219.9ms\n",
      "Speed: 8.0ms preprocess, 219.9ms inference, 4.5ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 210.1ms\n",
      "Speed: 8.4ms preprocess, 210.1ms inference, 2.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 232.7ms\n",
      "Speed: 7.6ms preprocess, 232.7ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 220.7ms\n",
      "Speed: 11.0ms preprocess, 220.7ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 216.2ms\n",
      "Speed: 7.3ms preprocess, 216.2ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 215.0ms\n",
      "Speed: 8.4ms preprocess, 215.0ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 226.1ms\n",
      "Speed: 8.2ms preprocess, 226.1ms inference, 2.9ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 218.7ms\n",
      "Speed: 6.7ms preprocess, 218.7ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 228.9ms\n",
      "Speed: 7.1ms preprocess, 228.9ms inference, 1.7ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 243.8ms\n",
      "Speed: 4.2ms preprocess, 243.8ms inference, 4.5ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 224.1ms\n",
      "Speed: 7.9ms preprocess, 224.1ms inference, 1.9ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 226.6ms\n",
      "Speed: 10.8ms preprocess, 226.6ms inference, 3.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 211.1ms\n",
      "Speed: 11.4ms preprocess, 211.1ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 211.2ms\n",
      "Speed: 10.0ms preprocess, 211.2ms inference, 1.9ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 212.8ms\n",
      "Speed: 10.2ms preprocess, 212.8ms inference, 1.1ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 214.9ms\n",
      "Speed: 10.9ms preprocess, 214.9ms inference, 1.3ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 215.6ms\n",
      "Speed: 8.5ms preprocess, 215.6ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 216.2ms\n",
      "Speed: 10.0ms preprocess, 216.2ms inference, 4.2ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 220.0ms\n",
      "Speed: 4.8ms preprocess, 220.0ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 216.7ms\n",
      "Speed: 14.7ms preprocess, 216.7ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 213.9ms\n",
      "Speed: 9.3ms preprocess, 213.9ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 210.7ms\n",
      "Speed: 6.2ms preprocess, 210.7ms inference, 1.2ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 207.5ms\n",
      "Speed: 8.4ms preprocess, 207.5ms inference, 1.5ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 207.2ms\n",
      "Speed: 8.3ms preprocess, 207.2ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 218.5ms\n",
      "Speed: 4.1ms preprocess, 218.5ms inference, 3.3ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 222.0ms\n",
      "Speed: 10.3ms preprocess, 222.0ms inference, 1.5ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 230.8ms\n",
      "Speed: 4.1ms preprocess, 230.8ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 229.3ms\n",
      "Speed: 8.6ms preprocess, 229.3ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 217.9ms\n",
      "Speed: 9.4ms preprocess, 217.9ms inference, 6.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 222.7ms\n",
      "Speed: 3.9ms preprocess, 222.7ms inference, 2.1ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 239.2ms\n",
      "Speed: 8.1ms preprocess, 239.2ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 254.0ms\n",
      "Speed: 8.3ms preprocess, 254.0ms inference, 1.2ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 213.1ms\n",
      "Speed: 10.5ms preprocess, 213.1ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 218.4ms\n",
      "Speed: 7.8ms preprocess, 218.4ms inference, 0.9ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 208.4ms\n",
      "Speed: 8.7ms preprocess, 208.4ms inference, 0.7ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 210.3ms\n",
      "Speed: 9.1ms preprocess, 210.3ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 212.1ms\n",
      "Speed: 8.1ms preprocess, 212.1ms inference, 2.9ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 223.8ms\n",
      "Speed: 7.4ms preprocess, 223.8ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 217.5ms\n",
      "Speed: 10.6ms preprocess, 217.5ms inference, 1.7ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 235.2ms\n",
      "Speed: 12.4ms preprocess, 235.2ms inference, 2.7ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 264.5ms\n",
      "Speed: 7.6ms preprocess, 264.5ms inference, 1.5ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 220.0ms\n",
      "Speed: 7.2ms preprocess, 220.0ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 210.7ms\n",
      "Speed: 8.7ms preprocess, 210.7ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 224.6ms\n",
      "Speed: 9.4ms preprocess, 224.6ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 210.6ms\n",
      "Speed: 7.0ms preprocess, 210.6ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 248.0ms\n",
      "Speed: 4.7ms preprocess, 248.0ms inference, 2.1ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 223.9ms\n",
      "Speed: 8.7ms preprocess, 223.9ms inference, 1.4ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 208.6ms\n",
      "Speed: 10.3ms preprocess, 208.6ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 216.8ms\n",
      "Speed: 3.4ms preprocess, 216.8ms inference, 1.4ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 244.4ms\n",
      "Speed: 7.0ms preprocess, 244.4ms inference, 0.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 282.1ms\n",
      "Speed: 10.1ms preprocess, 282.1ms inference, 2.6ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 241.3ms\n",
      "Speed: 7.8ms preprocess, 241.3ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n",
      "0: 768x960 1 prohibitory, 223.2ms\n",
      "Speed: 7.5ms preprocess, 223.2ms inference, 2.2ms postprocess per image at shape (1, 3, 768, 960)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28976\\1834692032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myolo_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'set_prompts'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpersist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# merge list of Result into one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[1;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[1;31m# Inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mprofilers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m                 \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[1;31m# Postprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\engine\\predictor.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m         visualize = increment_path(self.save_dir / Path(self.batch[0][0]).stem,\n\u001b[0;32m    137\u001b[0m                                    mkdir=True) if self.args.visualize and (not self.source_type.tensor) else False\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, im, augment, visualize)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpt\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# PyTorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maugment\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mvisualize\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# TorchScript\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\tasks.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# for cases of training and validating while training.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\tasks.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_augment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\tasks.py\u001b[0m in \u001b[0;36m_predict_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_profile_one_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# save output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;34m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;34m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;34m\"\"\"'forward()' applies the YOLOv5 FPN to input data.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_fuse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;34m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OSAMA\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 460\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load YOLO model\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "\n",
    "# Load trained SVM model\n",
    "svm_model = joblib.load(svm_model_path)\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'Green Light', 'Red Light', 'Speed Limit 10', 'Speed Limit 100', 'Speed Limit 110',\n",
    "    'Speed Limit 120', 'Speed Limit 20', 'Speed Limit 30', 'Speed Limit 40', 'Speed Limit 50',\n",
    "    'Speed Limit 60', 'Speed Limit 70', 'Speed Limit 80', 'Speed Limit 90', 'Stop',\n",
    "    'No vehicles', 'Veh > 3.5 tons prohibited', 'No entry', 'General caution',\n",
    "    'Dangerous curve left', 'Dangerous curve right', 'Double curve', 'Bumpy road',\n",
    "    'Slippery road', 'Road narrows on the right', 'Road work', 'Traffic signals',\n",
    "    'Pedestrians', 'Children crossing', 'Bicycles crossing', 'Beware of ice/snow',\n",
    "    'Wild animals crossing', 'End speed + passing limits', 'Turn right ahead', 'Turn left ahead',\n",
    "    'Ahead only', 'Go straight or right', 'Go straight or left', 'Keep right',\n",
    "    'Keep left', 'Roundabout mandatory', 'End of no passing', 'End no passing veh > 3.5 tons'\n",
    "]\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(\"./CarlaVideos/traffic signs test.mp4\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO on the frame\n",
    "    frame = cv2.resize(frame, (1200, 960))\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    results = yolo_model.predict(frame, conf=0.5)\n",
    "    boxes = results[0].boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        cropped = frame[y1:y2, x1:x2]\n",
    "\n",
    "        if cropped.size == 0 or x2 - x1 < 10 or y2 - y1 < 10:\n",
    "            continue\n",
    "\n",
    "        # Resize to 64x64\n",
    "        resized = cv2.resize(cropped, (64, 64))\n",
    "\n",
    "        # --- HOG Feature ---\n",
    "        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "        hog_feat = hog(gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)\n",
    "\n",
    "        # reshape features\n",
    "        features = (hog_feat).reshape(1, -1)\n",
    "\n",
    "        # Predict with SVM\n",
    "        prediction = svm_model.predict(features)[0]\n",
    "        confidence = max(svm_model.predict_proba(features)[0])\n",
    "\n",
    "        if confidence < 0.8:\n",
    "            continue\n",
    "\n",
    "        label = f\"{CLASS_NAMES[prediction]} ({confidence:.2f})\"\n",
    "\n",
    "        # Draw prediction\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Display result\n",
    "    cv2.imshow(\"YOLO + SVM\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
